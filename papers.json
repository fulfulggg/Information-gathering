[
  {
    "title": "HSR-KAN: Efficient Hyperspectral Image Super-Resolution via Kolmogorov-Arnold Networks",
    "summary": "arXiv:2409.06705v1 Announce Type: new \nAbstract: Hyperspectral images (HSIs) have great potential in various visual tasks due to their rich spectral information. However, obtaining high-resolution hyperspectral images remains challenging due to limitations of physical imaging. Inspired by Kolmogorov-Arnold Networks (KANs), we propose an efficient HSI super-resolution (HSI-SR) model to fuse a low-resolution HSI (LR-HSI) and a high-resolution multispectral image (HR-MSI), yielding a high-resolution HSI (HR-HSI). To achieve the effective integration of spatial information from HR-MSI, we design a fusion module based on KANs, called KAN-Fusion. Further inspired by the channel attention mechanism, we design a spectral channel attention module called KAN Channel Attention Block (KAN-CAB) for post-fusion feature extraction. As a channel attention module integrated with KANs, KAN-CAB not only enhances the fine-grained adjustment ability of deep networks, enabling networks to accurately simulate details of spectral sequences and spatial textures, but also effectively avoid Curse of Dimensionality (COD). Extensive experiments show that, compared to current state-of-the-art (SOTA) HSI-SR methods, proposed HSR-KAN achieves the best performance in terms of both qualitative and quantitative assessments. Our code is available at: https://github.com/Baisonm-Li/HSR-KAN.",
    "link": "https://arxiv.org/abs/2409.06705",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Baisonm-Li/HSR-KAN."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "gsplat: An Open-Source Library for Gaussian Splatting",
    "summary": "arXiv:2409.06765v1 Announce Type: new \nAbstract: gsplat is an open-source library designed for training and developing Gaussian Splatting methods. It features a front-end with Python bindings compatible with the PyTorch library and a back-end with highly optimized CUDA kernels. gsplat offers numerous features that enhance the optimization of Gaussian Splatting models, which include optimization improvements for speed, memory, and convergence times. Experimental results demonstrate that gsplat achieves up to 10% less training time and 4x less memory than the original implementation. Utilized in several research projects, gsplat is actively maintained on GitHub. Source code is available at https://github.com/nerfstudio-project/gsplat under Apache License 2.0. We welcome contributions from the open-source community.",
    "link": "https://arxiv.org/abs/2409.06765",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/nerfstudio-project/gsplat"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "DetailCLIP: Detail-Oriented CLIP for Fine-Grained Tasks",
    "summary": "arXiv:2409.06809v1 Announce Type: new \nAbstract: In this paper, we introduce DetailCLIP: A Detail-Oriented CLIP to address the limitations of contrastive learning-based vision-language models, particularly CLIP, in handling detail-oriented and fine-grained tasks like segmentation. While CLIP and its variants excel in the global alignment of image and text representations, they often struggle to capture the fine-grained details necessary for precise segmentation. To overcome these challenges, we propose a novel framework that employs patch-level comparison of self-distillation and pixel-level reconstruction losses, enhanced with an attention-based token removal mechanism. This approach selectively retains semantically relevant tokens, enabling the model to focus on the image's critical regions aligned with the specific functions of our model, including textual information processing, patch comparison, and image reconstruction, ensuring that the model learns high-level semantics and detailed visual features. Our experiments demonstrate that DetailCLIP surpasses existing CLIP-based and traditional self-supervised learning (SSL) models in segmentation accuracy and exhibits superior generalization across diverse datasets. DetailCLIP represents a significant advancement in vision-language modeling, offering a robust solution for tasks that demand high-level semantic understanding and detailed feature extraction. https://github.com/KishoreP1/DetailCLIP.",
    "link": "https://arxiv.org/abs/2409.06809",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/KishoreP1/DetailCLIP."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "LIME-M: Less Is More for Evaluation of MLLMs",
    "summary": "arXiv:2409.06851v1 Announce Type: new \nAbstract: With the remarkable success achieved by Multimodal Large Language Models (MLLMs), numerous benchmarks have been designed to assess MLLMs' ability to guide their development in image perception tasks (e.g., image captioning and visual question answering). However, the existence of numerous benchmarks results in a substantial computational burden when evaluating model performance across all of them. Moreover, these benchmarks contain many overly simple problems or challenging samples, which do not effectively differentiate the capabilities among various MLLMs. To address these challenges, we propose a pipeline to process the existing benchmarks, which consists of two modules: (1) Semi-Automated Screening Process and (2) Eliminating Answer Leakage. The Semi-Automated Screening Process filters out samples that cannot distinguish the model's capabilities by synthesizing various MLLMs and manually evaluating them. The Eliminate Answer Leakage module filters samples whose answers can be inferred without images. Finally, we curate the LIME-M: Less Is More for Evaluation of Multimodal LLMs, a lightweight Multimodal benchmark that can more effectively evaluate the performance of different models. Our experiments demonstrate that: LIME-M can better distinguish the performance of different MLLMs with fewer samples (24% of the original) and reduced time (23% of the original); LIME-M eliminates answer leakage, focusing mainly on the information within images; The current automatic metric (i.e., CIDEr) is insufficient for evaluating MLLMs' capabilities in captioning. Moreover, removing the caption task score when calculating the overall score provides a more accurate reflection of model performance differences. All our codes and data are released at https://github.com/kangreen0210/LIME-M.",
    "link": "https://arxiv.org/abs/2409.06851",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/kangreen0210/LIME-M."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning",
    "summary": "arXiv:2409.06928v1 Announce Type: new \nAbstract: The segmentation of the pubic symphysis and fetal head (PSFH) constitutes a pivotal step in monitoring labor progression and identifying potential delivery complications. Despite the advances in deep learning, the lack of annotated medical images hinders the training of segmentation. Traditional semi-supervised learning approaches primarily utilize a unified network model based on Convolutional Neural Networks (CNNs) and apply consistency regularization to mitigate the reliance on extensive annotated data. However, these methods often fall short in capturing the discriminative features of unlabeled data and in delineating the long-range dependencies inherent in the ambiguous boundaries of PSFH within ultrasound images. To address these limitations, we introduce a novel framework, the Dual-Student and Teacher Combining CNN and Transformer (DSTCT), which synergistically integrates the capabilities of CNNs and Transformers. Our framework comprises a Vision Transformer (ViT) as the teacher and two student mod ls one ViT and one CNN. This dual-student setup enables mutual supervision through the generation of both hard and soft pseudo-labels, with the consistency in their predictions being refined by minimizing the classifier determinacy discrepancy. The teacher model further reinforces learning within this architecture through the imposition of consistency regularization constraints. To augment the generalization abilities of our approach, we employ a blend of data and model perturbation techniques. Comprehensive evaluations on the benchmark dataset of the PSFH Segmentation Grand Challenge at MICCAI 2023 demonstrate our DSTCT framework outperformed ten contemporary semi-supervised segmentation methods. Code available at https://github.com/jjm1589/DSTCT.",
    "link": "https://arxiv.org/abs/2409.06928",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/jjm1589/DSTCT."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "1M-Deepfakes Detection Challenge",
    "summary": "arXiv:2409.06991v1 Announce Type: new \nAbstract: The detection and localization of deepfake content, particularly when small fake segments are seamlessly mixed with real videos, remains a significant challenge in the field of digital media security. Based on the recently released AV-Deepfake1M dataset, which contains more than 1 million manipulated videos across more than 2,000 subjects, we introduce the 1M-Deepfakes Detection Challenge. This challenge is designed to engage the research community in developing advanced methods for detecting and localizing deepfake manipulations within the large-scale high-realistic audio-visual dataset. The participants can access the AV-Deepfake1M dataset and are required to submit their inference results for evaluation across the metrics for detection or localization tasks. The methodologies developed through the challenge will contribute to the development of next-generation deepfake detection and localization systems. Evaluation scripts, baseline models, and accompanying code will be available on https://github.com/ControlNet/AV-Deepfake1M.",
    "link": "https://arxiv.org/abs/2409.06991",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ControlNet/AV-Deepfake1M."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Diff-VPS: Video Polyp Segmentation via a Multi-task Diffusion Network with Adversarial Temporal Reasoning",
    "summary": "arXiv:2409.07238v1 Announce Type: new \nAbstract: Diffusion Probabilistic Models have recently attracted significant attention in the community of computer vision due to their outstanding performance. However, while a substantial amount of diffusion-based research has focused on generative tasks, no work introduces diffusion models to advance the results of polyp segmentation in videos, which is frequently challenged by polyps' high camouflage and redundant temporal cues.In this paper, we present a novel diffusion-based network for video polyp segmentation task, dubbed as Diff-VPS. We incorporate multi-task supervision into diffusion models to promote the discrimination of diffusion models on pixel-by-pixel segmentation. This integrates the contextual high-level information achieved by the joint classification and detection tasks. To explore the temporal dependency, Temporal Reasoning Module (TRM) is devised via reasoning and reconstructing the target frame from the previous frames. We further equip TRM with a generative adversarial self-supervised strategy to produce more realistic frames and thus capture better dynamic cues. Extensive experiments are conducted on SUN-SEG, and the results indicate that our proposed Diff-VPS significantly achieves state-of-the-art performance. Code is available at https://github.com/lydia-yllu/Diff-VPS.",
    "link": "https://arxiv.org/abs/2409.07238",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/lydia-yllu/Diff-VPS."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Realistic and Efficient Face Swapping: A Unified Approach with Diffusion Models",
    "summary": "arXiv:2409.07269v1 Announce Type: new \nAbstract: Despite promising progress in face swapping task, realistic swapped images remain elusive, often marred by artifacts, particularly in scenarios involving high pose variation, color differences, and occlusion. To address these issues, we propose a novel approach that better harnesses diffusion models for face-swapping by making following core contributions. (a) We propose to re-frame the face-swapping task as a self-supervised, train-time inpainting problem, enhancing the identity transfer while blending with the target image. (b) We introduce a multi-step Denoising Diffusion Implicit Model (DDIM) sampling during training, reinforcing identity and perceptual similarities. (c) Third, we introduce CLIP feature disentanglement to extract pose, expression, and lighting information from the target image, improving fidelity. (d) Further, we introduce a mask shuffling technique during inpainting training, which allows us to create a so-called universal model for swapping, with an additional feature of head swapping. Ours can swap hair and even accessories, beyond traditional face swapping. Unlike prior works reliant on multiple off-the-shelf models, ours is a relatively unified approach and so it is resilient to errors in other off-the-shelf models. Extensive experiments on FFHQ and CelebA datasets validate the efficacy and robustness of our approach, showcasing high-fidelity, realistic face-swapping with minimal inference time. Our code is available at https://github.com/Sanoojan/REFace.",
    "link": "https://arxiv.org/abs/2409.07269",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Sanoojan/REFace."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "TLD-READY: Traffic Light Detection -- Relevance Estimation and Deployment Analysis",
    "summary": "arXiv:2409.07284v1 Announce Type: new \nAbstract: Effective traffic light detection is a critical component of the perception stack in autonomous vehicles. This work introduces a novel deep-learning detection system while addressing the challenges of previous work. Utilizing a comprehensive dataset amalgamation, including the Bosch Small Traffic Lights Dataset, LISA, the DriveU Traffic Light Dataset, and a proprietary dataset from Karlsruhe, we ensure a robust evaluation across varied scenarios. Furthermore, we propose a relevance estimation system that innovatively uses directional arrow markings on the road, eliminating the need for prior map creation. On the DriveU dataset, this approach results in 96% accuracy in relevance estimation. Finally, a real-world evaluation is performed to evaluate the deployment and generalizing abilities of these models. For reproducibility and to facilitate further research, we provide the model weights and code: https://github.com/KASTEL-MobilityLab/traffic-light-detection.",
    "link": "https://arxiv.org/abs/2409.07284",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/KASTEL-MobilityLab/traffic-light-detection."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks",
    "summary": "arXiv:2409.07353v1 Announce Type: new \nAbstract: Large Vision-Language Models (LVLMs), trained on multimodal big datasets, have significantly advanced AI by excelling in vision-language tasks. However, these models remain vulnerable to adversarial attacks, particularly jailbreak attacks, which bypass safety protocols and cause the model to generate misleading or harmful responses. This vulnerability stems from both the inherent susceptibilities of LLMs and the expanded attack surface introduced by the visual modality. We propose Sim-CLIP+, a novel defense mechanism that adversarially fine-tunes the CLIP vision encoder by leveraging a Siamese architecture. This approach maximizes cosine similarity between perturbed and clean samples, facilitating resilience against adversarial manipulations. Sim-CLIP+ offers a plug-and-play solution, allowing seamless integration into existing LVLM architectures as a robust vision encoder. Unlike previous defenses, our method requires no structural modifications to the LVLM and incurs minimal computational overhead. Sim-CLIP+ demonstrates effectiveness against both gradient-based adversarial attacks and various jailbreak techniques. We evaluate Sim-CLIP+ against three distinct jailbreak attack strategies and perform clean evaluations using standard downstream datasets, including COCO for image captioning and OKVQA for visual question answering. Extensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracy while substantially improving robustness against both gradient-based adversarial attacks and jailbreak techniques. Our code and robust vision encoders are available at https://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.",
    "link": "https://arxiv.org/abs/2409.07353",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Event-based Mosaicing Bundle Adjustment",
    "summary": "arXiv:2409.07365v1 Announce Type: new \nAbstract: We tackle the problem of mosaicing bundle adjustment (i.e., simultaneous refinement of camera orientations and scene map) for a purely rotating event camera. We formulate the problem as a regularized non-linear least squares optimization. The objective function is defined using the linearized event generation model in the camera orientations and the panoramic gradient map of the scene. We show that this BA optimization has an exploitable block-diagonal sparsity structure, so that the problem can be solved efficiently. To the best of our knowledge, this is the first work to leverage such sparsity to speed up the optimization in the context of event-based cameras, without the need to convert events into image-like representations. We evaluate our method, called EMBA, on both synthetic and real-world datasets to show its effectiveness (50% photometric error decrease), yielding results of unprecedented quality. In addition, we demonstrate EMBA using high spatial resolution event cameras, yielding delicate panoramas in the wild, even without an initial map. Project page: https://github.com/tub-rip/emba",
    "link": "https://arxiv.org/abs/2409.07365",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/tub-rip/emba"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models",
    "summary": "arXiv:2409.07452v1 Announce Type: new \nAbstract: Despite having tremendous progress in image-to-3D generation, existing methods still struggle to produce multi-view consistent images with high-resolution textures in detail, especially in the paradigm of 2D diffusion that lacks 3D awareness. In this work, we present High-resolution Image-to-3D model (Hi3D), a new video diffusion based paradigm that redefines a single image to multi-view images as 3D-aware sequential image generation (i.e., orbital video generation). This methodology delves into the underlying temporal consistency knowledge in video diffusion model that generalizes well to geometry consistency across multiple views in 3D generation. Technically, Hi3D first empowers the pre-trained video diffusion model with 3D-aware prior (camera pose condition), yielding multi-view images with low-resolution texture details. A 3D-aware video-to-video refiner is learnt to further scale up the multi-view images with high-resolution texture details. Such high-resolution multi-view images are further augmented with novel views through 3D Gaussian Splatting, which are finally leveraged to obtain high-fidelity meshes via 3D reconstruction. Extensive experiments on both novel view synthesis and single view reconstruction demonstrate that our Hi3D manages to produce superior multi-view consistency images with highly-detailed textures. Source code and data are available at \\url{https://github.com/yanghb22-fdu/Hi3D-Official}.",
    "link": "https://arxiv.org/abs/2409.07452",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/yanghb22-fdu/Hi3D-Official}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Attention Down-Sampling Transformer, Relative Ranking and Self-Consistency for Blind Image Quality Assessment",
    "summary": "arXiv:2409.07115v1 Announce Type: cross \nAbstract: The no-reference image quality assessment is a challenging domain that addresses estimating image quality without the original reference. We introduce an improved mechanism to extract local and non-local information from images via different transformer encoders and CNNs. The utilization of Transformer encoders aims to mitigate locality bias and generate a non-local representation by sequentially processing CNN features, which inherently capture local visual structures. Establishing a stronger connection between subjective and objective assessments is achieved through sorting within batches of images based on relative distance information. A self-consistency approach to self-supervision is presented, explicitly addressing the degradation of no-reference image quality assessment (NR-IQA) models under equivariant transformations. Our approach ensures model robustness by maintaining consistency between an image and its horizontally flipped equivalent. Through empirical evaluation of five popular image quality assessment datasets, the proposed model outperforms alternative algorithms in the context of no-reference image quality assessment datasets, especially on smaller datasets. Codes are available at \\href{https://github.com/mas94/ADTRS}{https://github.com/mas94/ADTRS}",
    "link": "https://arxiv.org/abs/2409.07115",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/mas94/ADTRS}{https://github.com/mas94/ADTRS}"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "3DGCQA: A Quality Assessment Database for 3D AI-Generated Contents",
    "summary": "arXiv:2409.07236v1 Announce Type: cross \nAbstract: Although 3D generated content (3DGC) offers advantages in reducing production costs and accelerating design timelines, its quality often falls short when compared to 3D professionally generated content. Common quality issues frequently affect 3DGC, highlighting the importance of timely and effective quality assessment. Such evaluations not only ensure a higher standard of 3DGCs for end-users but also provide critical insights for advancing generative technologies. To address existing gaps in this domain, this paper introduces a novel 3DGC quality assessment dataset, 3DGCQA, built using 7 representative Text-to-3D generation methods. During the dataset's construction, 50 fixed prompts are utilized to generate contents across all methods, resulting in the creation of 313 textured meshes that constitute the 3DGCQA dataset. The visualization intuitively reveals the presence of 6 common distortion categories in the generated 3DGCs. To further explore the quality of the 3DGCs, subjective quality assessment is conducted by evaluators, whose ratings reveal significant variation in quality across different generation methods. Additionally, several objective quality assessment algorithms are tested on the 3DGCQA dataset. The results expose limitations in the performance of existing algorithms and underscore the need for developing more specialized quality assessment methods. To provide a valuable resource for future research and development in 3D content generation and quality assessment, the dataset has been open-sourced in https://github.com/zyj-2000/3DGCQA.",
    "link": "https://arxiv.org/abs/2409.07236",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/zyj-2000/3DGCQA."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "A Unified Contrastive Loss for Self-Training",
    "summary": "arXiv:2409.07292v1 Announce Type: cross \nAbstract: Self-training methods have proven to be effective in exploiting abundant unlabeled data in semi-supervised learning, particularly when labeled data is scarce. While many of these approaches rely on a cross-entropy loss function (CE), recent advances have shown that the supervised contrastive loss function (SupCon) can be more effective. Additionally, unsupervised contrastive learning approaches have also been shown to capture high quality data representations in the unsupervised setting. To benefit from these advantages in a semi-supervised setting, we propose a general framework to enhance self-training methods, which replaces all instances of CE losses with a unique contrastive loss. By using class prototypes, which are a set of class-wise trainable parameters, we recover the probability distributions of the CE setting and show a theoretical equivalence with it. Our framework, when applied to popular self-training methods, results in significant performance improvements across three different datasets with a limited number of labeled data. Additionally, we demonstrate further improvements in convergence speed, transfer ability, and hyperparameter stability. The code is available at \\url{https://github.com/AurelienGauffre/semisupcon/}.",
    "link": "https://arxiv.org/abs/2409.07292",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/AurelienGauffre/semisupcon/}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Quantifying Knee Cartilage Shape and Lesion: From Image to Metrics",
    "summary": "arXiv:2409.07361v1 Announce Type: cross \nAbstract: Imaging features of knee articular cartilage have been shown to be potential imaging biomarkers for knee osteoarthritis. Despite recent methodological advancements in image analysis techniques like image segmentation, registration, and domain-specific image computing algorithms, only a few works focus on building fully automated pipelines for imaging feature extraction. In this study, we developed a deep-learning-based medical image analysis application for knee cartilage morphometrics, CartiMorph Toolbox (CMT). We proposed a 2-stage joint template learning and registration network, CMT-reg. We trained the model using the OAI-ZIB dataset and assessed its performance in template-to-image registration. The CMT-reg demonstrated competitive results compared to other state-of-the-art models. We integrated the proposed model into an automated pipeline for the quantification of cartilage shape and lesion (full-thickness cartilage loss, specifically). The toolbox provides a comprehensive, user-friendly solution for medical image analysis and data visualization. The software and models are available at https://github.com/YongchengYAO/CMT-AMAI24paper .",
    "link": "https://arxiv.org/abs/2409.07361",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/YongchengYAO/CMT-AMAI24paper"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Adaptive Adapter Routing for Long-Tailed Class-Incremental Learning",
    "summary": "arXiv:2409.07446v1 Announce Type: cross \nAbstract: In our ever-evolving world, new data exhibits a long-tailed distribution, such as e-commerce platform reviews. This necessitates continuous model learning imbalanced data without forgetting, addressing the challenge of long-tailed class-incremental learning (LTCIL). Existing methods often rely on retraining linear classifiers with former data, which is impractical in real-world settings. In this paper, we harness the potent representation capabilities of pre-trained models and introduce AdaPtive Adapter RouTing (APART) as an exemplar-free solution for LTCIL. To counteract forgetting, we train inserted adapters with frozen pre-trained weights for deeper adaptation and maintain a pool of adapters for selection during sequential model updates. Additionally, we present an auxiliary adapter pool designed for effective generalization, especially on minority classes. Adaptive instance routing across these pools captures crucial correlations, facilitating a comprehensive representation of all classes. Consequently, APART tackles the imbalance problem as well as catastrophic forgetting in a unified framework. Extensive benchmark experiments validate the effectiveness of APART. Code is available at: https://github.com/vita-qzh/APART",
    "link": "https://arxiv.org/abs/2409.07446",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/vita-qzh/APART"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "GenSelfDiff-HIS: Generative Self-Supervision Using Diffusion for Histopathological Image Segmentation",
    "summary": "arXiv:2309.01487v2 Announce Type: replace \nAbstract: Histopathological image segmentation is a laborious and time-intensive task, often requiring analysis from experienced pathologists for accurate examinations. To reduce this burden, supervised machine-learning approaches have been adopted using large-scale annotated datasets for histopathological image analysis. However, in several scenarios, the availability of large-scale annotated data is a bottleneck while training such models. Self-supervised learning (SSL) is an alternative paradigm that provides some respite by constructing models utilizing only the unannotated data which is often abundant. The basic idea of SSL is to train a network to perform one or many pseudo or pretext tasks on unannotated data and use it subsequently as the basis for a variety of downstream tasks. It is seen that the success of SSL depends critically on the considered pretext task. While there have been many efforts in designing pretext tasks for classification problems, there haven't been many attempts on SSL for histopathological segmentation. Motivated by this, we propose an SSL approach for segmenting histopathological images via generative diffusion models in this paper. Our method is based on the observation that diffusion models effectively solve an image-to-image translation task akin to a segmentation task. Hence, we propose generative diffusion as the pretext task for histopathological image segmentation. We also propose a multi-loss function-based fine-tuning for the downstream task. We validate our method using several metrics on two publically available datasets along with a newly proposed head and neck (HN) cancer dataset containing hematoxylin and eosin (H\\&amp;E) stained images along with annotations. Codes will be made public at https://github.com/suhas-srinath/GenSelfDiff-HIS.",
    "link": "https://arxiv.org/abs/2309.01487",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/suhas-srinath/GenSelfDiff-HIS."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "[Citation needed] Data usage and citation practices in medical imaging conferences",
    "summary": "arXiv:2402.03003v2 Announce Type: replace \nAbstract: Medical imaging papers often focus on methodology, but the quality of the algorithms and the validity of the conclusions are highly dependent on the datasets used. As creating datasets requires a lot of effort, researchers often use publicly available datasets, there is however no adopted standard for citing the datasets used in scientific papers, leading to difficulty in tracking dataset usage. In this work, we present two open-source tools we created that could help with the detection of dataset usage, a pipeline \\url{https://github.com/TheoSourget/Public_Medical_Datasets_References} using OpenAlex and full-text analysis, and a PDF annotation software \\url{https://github.com/TheoSourget/pdf_annotator} used in our study to manually label the presence of datasets. We applied both tools on a study of the usage of 20 publicly available medical datasets in papers from MICCAI and MIDL. We compute the proportion and the evolution between 2013 and 2023 of 3 types of presence in a paper: cited, mentioned in the full text, cited and mentioned. Our findings demonstrate the concentration of the usage of a limited set of datasets. We also highlight different citing practices, making the automation of tracking difficult.",
    "link": "https://arxiv.org/abs/2402.03003",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/TheoSourget/Public_Medical_Datasets_References}",
      "https://github.com/TheoSourget/pdf_annotator}"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and Defenses",
    "summary": "arXiv:2402.04013v2 Announce Type: replace \nAbstract: Deep Neural Networks (DNNs) have revolutionized various domains with their exceptional performance across numerous applications. However, Model Inversion (MI) attacks, which disclose private information about the training dataset by abusing access to the trained models, have emerged as a formidable privacy threat. Given a trained network, these attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training samples, posing significant privacy concerns. Despite the rapid advances in the field, we lack a comprehensive and systematic overview of existing MI attacks and defenses. To fill this gap, this paper thoroughly investigates this realm and presents a holistic survey. Firstly, our work briefly reviews early MI studies on traditional machine learning scenarios. We then elaborately analyze and compare numerous recent attacks and defenses on Deep Neural Networks (DNNs) across multiple modalities and learning tasks. By meticulously analyzing their distinctive features, we summarize and classify these methods into different categories and provide a novel taxonomy. Finally, this paper discusses promising research directions and presents potential solutions to open issues. To facilitate further study on MI attacks and defenses, we have implemented an open-source model inversion toolbox on GitHub (https://github.com/ffhibnese/Model-Inversion-Attack-ToolBox).",
    "link": "https://arxiv.org/abs/2402.04013",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ffhibnese/Model-Inversion-Attack-ToolBox)."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning",
    "summary": "arXiv:2402.12185v2 Announce Type: replace \nAbstract: Recently, many versatile Multi-modal Large Language Models (MLLMs) have emerged continuously. However, their capacity to query information depicted in visual charts and engage in reasoning based on the queried contents remains under-explored. In this paper, to comprehensively and rigorously benchmark the ability of the off-the-shelf MLLMs in the chart domain, we construct ChartX, a multi-modal evaluation set covering 18 chart types, 7 chart tasks, 22 disciplinary topics, and high-quality chart data. Besides, we develop ChartVLM to offer a new perspective on handling multi-modal tasks that strongly depend on interpretable patterns, such as reasoning tasks in the field of charts or geometric images. We evaluate the chart-related ability of mainstream MLLMs and our ChartVLM on the proposed ChartX evaluation set. Extensive experiments demonstrate that ChartVLM surpasses both versatile and chart-related large models, achieving results comparable to GPT-4V. We believe that our study can pave the way for further exploration in creating a more comprehensive chart evaluation set and developing more interpretable multi-modal models. Both ChartX and ChartVLM are available at: https://github.com/UniModal4Reasoning/ChartVLM",
    "link": "https://arxiv.org/abs/2402.12185",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/UniModal4Reasoning/ChartVLM"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "RetinaRegNet: A Zero-Shot Approach for Retinal Image Registration",
    "summary": "arXiv:2404.16017v3 Announce Type: replace \nAbstract: We introduce RetinaRegNet, a zero-shot image registration model designed to register retinal images with minimal overlap, large deformations, and varying image quality. RetinaRegNet addresses these challenges and achieves robust and accurate registration through the following steps. First, we extract features from the moving and fixed images using latent diffusion models. We then sample feature points from the fixed image using a combination of the SIFT algorithm and random point sampling. For each sampled point, we identify its corresponding point in the moving image using a 2D correlation map, which computes the cosine similarity between the diffusion feature vectors of the point in the fixed image and all pixels in the moving image. Second, we eliminate most incorrectly detected point correspondences (outliers) by enforcing an inverse consistency constraint, ensuring that correspondences are consistent in both forward and backward directions. We further remove outliers with large distances between corresponding points using a global transformation based outlier detector. Finally, we implement a two-stage registration framework to handle large deformations. The first stage estimates a homography transformation to achieve global alignment between the images, while the second stage uses a third-order polynomial transformation to estimate local deformations. We evaluated RetinaRegNet on three retinal image registration datasets: color fundus images, fluorescein angiography images, and laser speckle flowgraphy images. Our model consistently outperformed state-of-the-art methods across all datasets. The accurate registration achieved by RetinaRegNet enables the tracking of eye disease progression, enhances surgical planning, and facilitates the evaluation of treatment efficacy. Our code is publicly available at: https://github.com/mirthAI/RetinaRegNet.",
    "link": "https://arxiv.org/abs/2404.16017",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/mirthAI/RetinaRegNet."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event Stream",
    "summary": "arXiv:2407.02174v3 Announce Type: replace \nAbstract: Neural implicit representation of visual scenes has attracted a lot of attention in recent research of computer vision and graphics. Most prior methods focus on how to reconstruct 3D scene representation from a set of images. In this work, we demonstrate the possibility to recover the neural radiance fields (NeRF) from a single blurry image and its corresponding event stream. We model the camera motion with a cubic B-Spline in SE(3) space. Both the blurry image and the brightness change within a time interval, can then be synthesized from the 3D scene representation given the 6-DoF poses interpolated from the cubic B-Spline. Our method can jointly learn both the implicit neural scene representation and recover the camera motion by minimizing the differences between the synthesized data and the real measurements without pre-computed camera poses from COLMAP. We evaluate the proposed method with both synthetic and real datasets. The experimental results demonstrate that we are able to render view-consistent latent sharp images from the learned NeRF and bring a blurry image alive in high quality. Code and data are available at https://github.com/wu-cvgl/BeNeRF.",
    "link": "https://arxiv.org/abs/2407.02174",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/wu-cvgl/BeNeRF."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models",
    "summary": "arXiv:2407.11691v2 Announce Type: replace \nAbstract: We present VLMEvalKit: an open-source toolkit for evaluating large multi-modality models based on PyTorch. The toolkit aims to provide a user-friendly and comprehensive framework for researchers and developers to evaluate existing multi-modality models and publish reproducible evaluation results. In VLMEvalKit, we implement over 70 different large multi-modality models, including both proprietary APIs and open-source models, as well as more than 20 different multi-modal benchmarks. By implementing a single interface, new models can be easily added to the toolkit, while the toolkit automatically handles the remaining workloads, including data preparation, distributed inference, prediction post-processing, and metric calculation. Although the toolkit is currently mainly used for evaluating large vision-language models, its design is compatible with future updates that incorporate additional modalities, such as audio and video. Based on the evaluation results obtained with the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to track the progress of multi-modality learning research. The toolkit is released at https://github.com/open-compass/VLMEvalKit and is actively maintained.",
    "link": "https://arxiv.org/abs/2407.11691",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/open-compass/VLMEvalKit"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "RainMamba: Enhanced Locality Learning with State Space Models for Video Deraining",
    "summary": "arXiv:2407.21773v2 Announce Type: replace \nAbstract: The outdoor vision systems are frequently contaminated by rain streaks and raindrops, which significantly degenerate the performance of visual tasks and multimedia applications. The nature of videos exhibits redundant temporal cues for rain removal with higher stability. Traditional video deraining methods heavily rely on optical flow estimation and kernel-based manners, which have a limited receptive field. Yet, transformer architectures, while enabling long-term dependencies, bring about a significant increase in computational complexity. Recently, the linear-complexity operator of the state space models (SSMs) has contrarily facilitated efficient long-term temporal modeling, which is crucial for rain streaks and raindrops removal in videos. Unexpectedly, its uni-dimensional sequential process on videos destroys the local correlations across the spatio-temporal dimension by distancing adjacent pixels. To address this, we present an improved SSMs-based video deraining network (RainMamba) with a novel Hilbert scanning mechanism to better capture sequence-level local information. We also introduce a difference-guided dynamic contrastive locality learning strategy to enhance the patch-level self-similarity learning ability of the proposed network. Extensive experiments on four synthesized video deraining datasets and real-world rainy videos demonstrate the effectiveness and efficiency of our network in the removal of rain streaks and raindrops. Our code and results are available at https://github.com/TonyHongtaoWu/RainMamba.",
    "link": "https://arxiv.org/abs/2407.21773",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/TonyHongtaoWu/RainMamba."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
    "summary": "arXiv:2408.12528v3 Announce Type: replace \nAbstract: We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at https://github.com/showlab/Show-o.",
    "link": "https://arxiv.org/abs/2408.12528",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/showlab/Show-o."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Exploring Low-Dimensional Subspaces in Diffusion Models for Controllable Image Editing",
    "summary": "arXiv:2409.02374v2 Announce Type: replace \nAbstract: Recently, diffusion models have emerged as a powerful class of generative models. Despite their success, there is still limited understanding of their semantic spaces. This makes it challenging to achieve precise and disentangled image generation without additional training, especially in an unsupervised way. In this work, we improve the understanding of their semantic spaces from intriguing observations: among a certain range of noise levels, (1) the learned posterior mean predictor (PMP) in the diffusion model is locally linear, and (2) the singular vectors of its Jacobian lie in low-dimensional semantic subspaces. We provide a solid theoretical basis to justify the linearity and low-rankness in the PMP. These insights allow us to propose an unsupervised, single-step, training-free LOw-rank COntrollable image editing (LOCO Edit) method for precise local editing in diffusion models. LOCO Edit identified editing directions with nice properties: homogeneity, transferability, composability, and linearity. These properties of LOCO Edit benefit greatly from the low-dimensional semantic subspace. Our method can further be extended to unsupervised or text-supervised editing in various text-to-image diffusion models (T-LOCO Edit). Finally, extensive empirical experiments demonstrate the effectiveness and efficiency of LOCO Edit. The codes will be released at https://github.com/ChicyChen/LOCO-Edit.",
    "link": "https://arxiv.org/abs/2409.02374",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ChicyChen/LOCO-Edit."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "One-Shot Diffusion Mimicker for Handwritten Text Generation",
    "summary": "arXiv:2409.04004v2 Announce Type: replace \nAbstract: Existing handwritten text generation methods often require more than ten handwriting samples as style references. However, in practical applications, users tend to prefer a handwriting generation model that operates with just a single reference sample for its convenience and efficiency. This approach, known as \"one-shot generation\", significantly simplifies the process but poses a significant challenge due to the difficulty of accurately capturing a writer's style from a single sample, especially when extracting fine details from the characters' edges amidst sparse foreground and undesired background noise. To address this problem, we propose a One-shot Diffusion Mimicker (One-DM) to generate handwritten text that can mimic any calligraphic style with only one reference sample. Inspired by the fact that high-frequency information of the individual sample often contains distinct style patterns (e.g., character slant and letter joining), we develop a novel style-enhanced module to improve the style extraction by incorporating high-frequency components from a single sample. We then fuse the style features with the text content as a merged condition for guiding the diffusion model to produce high-quality handwritten text images. Extensive experiments demonstrate that our method can successfully generate handwriting scripts with just one sample reference in multiple languages, even outperforming previous methods using over ten samples. Our source code is available at https://github.com/dailenson/One-DM.",
    "link": "https://arxiv.org/abs/2409.04004",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/dailenson/One-DM."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "A Survey of Multimodal Composite Editing and Retrieval",
    "summary": "arXiv:2409.05405v2 Announce Type: replace \nAbstract: In the real world, where information is abundant and diverse across different modalities, understanding and utilizing various data types to improve retrieval systems is a key focus of research. Multimodal composite retrieval integrates diverse modalities such as text, image and audio, etc. to provide more accurate, personalized, and contextually relevant results. To facilitate a deeper understanding of this promising direction, this survey explores multimodal composite editing and retrieval in depth, covering image-text composite editing, image-text composite retrieval, and other multimodal composite retrieval. In this survey, we systematically organize the application scenarios, methods, benchmarks, experiments, and future directions. Multimodal learning is a hot topic in large model era, and have also witnessed some surveys in multimodal learning and vision-language models with transformers published in the PAMI journal. To the best of our knowledge, this survey is the first comprehensive review of the literature on multimodal composite retrieval, which is a timely complement of multimodal fusion to existing reviews. To help readers' quickly track this field, we build the project page for this survey, which can be found at https://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval.",
    "link": "https://arxiv.org/abs/2409.05405",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "AMNS: Attention-Weighted Selective Mask and Noise Label Suppression for Text-to-Image Person Retrieval",
    "summary": "arXiv:2409.06385v2 Announce Type: replace \nAbstract: Text-to-image person retrieval aims to retrieve images of person given textual descriptions, and most methods implicitly assume that the training image-text pairs are correctly aligned, but in practice, under-correlated and false-correlated problems arise for image-text pairs due to poor image quality and mislabeling. Meanwhile, the random masking augmentation strategy may incorrectly discard semantic content resulting in the problem of generating noisy pairings between image lexical elements and text descriptions. To solve these two problems, we propose a new noise label suppression method and alleviate the problem generated by random mask through an attention-weighted selective mask strategy. In the proposed noise label suppression method, the effect of noise labels is suppressed by preventing the model from being overconfident by considering the inverse KL scatter loss, which is combined with the weight adjustment focus loss to further improve the model's recognition ability on difficult samples. On the other hand, Attention-Weighted Selective Mask processes the raw image through the EMA version of the image encoder, retaining some of the tokens with strong semantic associations with the corresponding text descriptions in order to extract better features. Numerous experiments validate the effectiveness of our approach in terms of dealing with noisy problems. The code will be available soon at https://github.com/RunQing715/AMNS.git.",
    "link": "https://arxiv.org/abs/2409.06385",
    "published": "Thu, 12 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/RunQing715/AMNS.git."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  }
]