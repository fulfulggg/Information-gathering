[
  {
    "title": "Mamba Capsule Routing Towards Part-Whole Relational Camouflaged Object Detection",
    "summary": "arXiv:2410.03987v1 Announce Type: new \nAbstract: The part-whole relational property endowed by Capsule Networks (CapsNets) has been known successful for camouflaged object detection due to its segmentation integrity. However, the previous Expectation Maximization (EM) capsule routing algorithm with heavy computation and large parameters obstructs this trend. The primary attribution behind lies in the pixel-level capsule routing. Alternatively, in this paper, we propose a novel mamba capsule routing at the type level. Specifically, we first extract the implicit latent state in mamba as capsule vectors, which abstract type-level capsules from pixel-level versions. These type-level mamba capsules are fed into the EM routing algorithm to get the high-layer mamba capsules, which greatly reduce the computation and parameters caused by the pixel-level capsule routing for part-whole relationships exploration. On top of that, to retrieve the pixel-level capsule features for further camouflaged prediction, we achieve this on the basis of the low-layer pixel-level capsules with the guidance of the correlations from adjacent-layer type-level mamba capsules. Extensive experiments on three widely used COD benchmark datasets demonstrate that our method significantly outperforms state-of-the-arts. Code has been available on https://github.com/Liangbo-Cheng/mamba\\_capsule.",
    "link": "https://arxiv.org/abs/2410.03987",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Liangbo-Cheng/mamba\\_capsule."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Taming the Tail: Leveraging Asymmetric Loss and Pade Approximation to Overcome Medical Image Long-Tailed Class Imbalance",
    "summary": "arXiv:2410.04084v1 Announce Type: new \nAbstract: Long-tailed problems in healthcare emerge from data imbalance due to variability in the prevalence and representation of different medical conditions, warranting the requirement of precise and dependable classification methods. Traditional loss functions such as cross-entropy and binary cross-entropy are often inadequate due to their inability to address the imbalances between the classes with high representation and the classes with low representation found in medical image datasets. We introduce a novel polynomial loss function based on Pade approximation, designed specifically to overcome the challenges associated with long-tailed classification. This approach incorporates asymmetric sampling techniques to better classify under-represented classes. We conducted extensive evaluations on three publicly available medical datasets and a proprietary medical dataset. Our implementation of the proposed loss function is open-sourced in the public repository:https://github.com/ipankhi/ALPA.",
    "link": "https://arxiv.org/abs/2410.04084",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ipankhi/ALPA."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Cross Resolution Encoding-Decoding For Detection Transformers",
    "summary": "arXiv:2410.04088v1 Announce Type: new \nAbstract: Detection Transformers (DETR) are renowned object detection pipelines, however computationally efficient multiscale detection using DETR is still challenging. In this paper, we propose a Cross-Resolution Encoding-Decoding (CRED) mechanism that allows DETR to achieve the accuracy of high-resolution detection while having the speed of low-resolution detection. CRED is based on two modules; Cross Resolution Attention Module (CRAM) and One Step Multiscale Attention (OSMA). CRAM is designed to transfer the knowledge of low-resolution encoder output to a high-resolution feature. While OSMA is designed to fuse multiscale features in a single step and produce a feature map of a desired resolution enriched with multiscale information. When used in prominent DETR methods, CRED delivers accuracy similar to the high-resolution DETR counterpart in roughly 50% fewer FLOPs. Specifically, state-of-the-art DN-DETR, when used with CRED (calling CRED-DETR), becomes 76% faster, with ~50% reduced FLOPs than its high-resolution counterpart with 202 G FLOPs on MS-COCO benchmark. We plan to release pretrained CRED-DETRs for use by the community. Code: https://github.com/ashishkumar822/CRED-DETR",
    "link": "https://arxiv.org/abs/2410.04088",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ashishkumar822/CRED-DETR"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Designing Concise ConvNets with Columnar Stages",
    "summary": "arXiv:2410.04089v1 Announce Type: new \nAbstract: In the era of vision Transformers, the recent success of VanillaNet shows the huge potential of simple and concise convolutional neural networks (ConvNets). Where such models mainly focus on runtime, it is also crucial to simultaneously focus on other aspects, e.g., FLOPs, parameters, etc, to strengthen their utility further. To this end, we introduce a refreshing ConvNet macro design called Columnar Stage Network (CoSNet). CoSNet has a systematically developed simple and concise structure, smaller depth, low parameter count, low FLOPs, and attention-less operations, well suited for resource-constrained deployment. The key novelty of CoSNet is deploying parallel convolutions with fewer kernels fed by input replication, using columnar stacking of these convolutions, and minimizing the use of 1x1 convolution layers. Our comprehensive evaluations show that CoSNet rivals many renowned ConvNets and Transformer designs under resource-constrained scenarios. Code: https://github.com/ashishkumar822/CoSNet",
    "link": "https://arxiv.org/abs/2410.04089",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ashishkumar822/CoSNet"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "TUBench: Benchmarking Large Vision-Language Models on Trustworthiness with Unanswerable Questions",
    "summary": "arXiv:2410.04107v1 Announce Type: new \nAbstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress on visual perception and linguistic interpretation. Despite their impressive capabilities across various tasks, LVLMs still suffer from the issue of hallucination, which involves generating content that is incorrect or unfaithful to the visual or textual inputs. Traditional benchmarks, such as MME and POPE, evaluate hallucination in LVLMs within the scope of Visual Question Answering (VQA) using answerable questions. However, some questions are unanswerable due to insufficient information in the images, and the performance of LVLMs on such unanswerable questions remains underexplored. To bridge this research gap, we propose TUBench, a benchmark specifically designed to evaluate the reliability of LVLMs using unanswerable questions. TUBench comprises an extensive collection of high-quality, unanswerable questions that are meticulously crafted using ten distinct strategies. To thoroughly evaluate LVLMs, the unanswerable questions in TUBench are based on images from four diverse domains as visual contexts: screenshots of code snippets, natural images, geometry diagrams, and screenshots of statistical tables. These unanswerable questions are tailored to test LVLMs' trustworthiness in code reasoning, commonsense reasoning, geometric reasoning, and mathematical reasoning related to tables, respectively. We conducted a comprehensive quantitative evaluation of 28 leading foundational models on TUBench, with Gemini-1.5-Pro, the top-performing model, achieving an average accuracy of 69.2%, and GPT-4o, the third-ranked model, reaching 66.7% average accuracy, in determining whether questions are answerable. TUBench is available at https://github.com/NLPCode/TUBench.",
    "link": "https://arxiv.org/abs/2410.04107",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/NLPCode/TUBench."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Distillation-Free One-Step Diffusion for Real-World Image Super-Resolution",
    "summary": "arXiv:2410.04224v1 Announce Type: new \nAbstract: Diffusion models have been achieving excellent performance for real-world image super-resolution (Real-ISR) with considerable computational costs. Current approaches are trying to derive one-step diffusion models from multi-step counterparts through knowledge distillation. However, these methods incur substantial training costs and may constrain the performance of the student model by the teacher's limitations. To tackle these issues, we propose DFOSD, a Distillation-Free One-Step Diffusion model. Specifically, we propose a noise-aware discriminator (NAD) to participate in adversarial training, further enhancing the authenticity of the generated content. Additionally, we improve the perceptual loss with edge-aware DISTS (EA-DISTS) to enhance the model's ability to generate fine details. Our experiments demonstrate that, compared with previous diffusion-based methods requiring dozens or even hundreds of steps, our DFOSD attains comparable or even superior results in both quantitative metrics and qualitative evaluations. Our DFOSD also abtains higher performance and efficiency compared with other one-step diffusion methods. We will release code and models at \\url{https://github.com/JianzeLi-114/DFOSD}.",
    "link": "https://arxiv.org/abs/2410.04224",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/JianzeLi-114/DFOSD}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Test-Time Adaptation for Keypoint-Based Spacecraft Pose Estimation Based on Predicted-View Synthesis",
    "summary": "arXiv:2410.04298v1 Announce Type: new \nAbstract: Due to the difficulty of replicating the real conditions during training, supervised algorithms for spacecraft pose estimation experience a drop in performance when trained on synthetic data and applied to real operational data. To address this issue, we propose a test-time adaptation approach that leverages the temporal redundancy between images acquired during close proximity operations. Our approach involves extracting features from sequential spacecraft images, estimating their poses, and then using this information to synthesise a reconstructed view. We establish a self-supervised learning objective by comparing the synthesised view with the actual one. During training, we supervise both pose estimation and image synthesis, while at test-time, we optimise the self-supervised objective. Additionally, we introduce a regularisation loss to prevent solutions that are not consistent with the keypoint structure of the spacecraft. Our code is available at: https://github.com/JotaBravo/spacecraft-tta.",
    "link": "https://arxiv.org/abs/2410.04298",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/JotaBravo/spacecraft-tta."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Accelerating Inference of Networks in the Frequency Domain",
    "summary": "arXiv:2410.04342v1 Announce Type: new \nAbstract: It has been demonstrated that networks' parameters can be significantly reduced in the frequency domain with a very small decrease in accuracy. However, given the cost of frequency transforms, the computational complexity is not significantly decreased. In this work, we propose performing network inference in the frequency domain to speed up networks whose frequency parameters are sparse. In particular, we propose a frequency inference chain that is dual to the network inference in the spatial domain. In order to handle the non-linear layers, we make a compromise to apply non-linear operations on frequency data directly, which works effectively. Enabled by the frequency inference chain and the strategy for non-linear layers, the proposed approach completes the entire inference in the frequency domain. Unlike previous approaches which require extra frequency or inverse transforms for all layers, the proposed approach only needs the frequency transform and its inverse once at the beginning and once at the end of a network. Comparisons with state-of-the-art methods demonstrate that the proposed approach significantly improves accuracy in the case of a high speedup ratio (over 100x). The source code is available at \\url{https://github.com/guanfangdong/FreqNet-Infer}.",
    "link": "https://arxiv.org/abs/2410.04342",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/guanfangdong/FreqNet-Infer}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "MVP-Bench: Can Large Vision--Language Models Conduct Multi-level Visual Perception Like Humans?",
    "summary": "arXiv:2410.04345v1 Announce Type: new \nAbstract: Humans perform visual perception at multiple levels, including low-level object recognition and high-level semantic interpretation such as behavior understanding. Subtle differences in low-level details can lead to substantial changes in high-level perception. For example, substituting the shopping bag held by a person with a gun suggests violent behavior, implying criminal or violent activity. Despite significant advancements in various multimodal tasks, Large Visual-Language Models (LVLMs) remain unexplored in their capabilities to conduct such multi-level visual perceptions.\n  To investigate the perception gap between LVLMs and humans, we introduce MVP-Bench, the first visual-language benchmark systematically evaluating both low- and high-level visual perception of LVLMs. We construct MVP-Bench across natural and synthetic images to investigate how manipulated content influences model perception. Using MVP-Bench, we diagnose the visual perception of 10 open-source and 2 closed-source LVLMs, showing that high-level perception tasks significantly challenge existing LVLMs. The state-of-the-art GPT-4o only achieves an accuracy of $56\\%$ on Yes/No questions, compared with $74\\%$ in low-level scenarios. Furthermore, the performance gap between natural and manipulated images indicates that current LVLMs do not generalize in understanding the visual semantics of synthetic images as humans do. Our data and code are publicly available at https://github.com/GuanzhenLi/MVP-Bench.",
    "link": "https://arxiv.org/abs/2410.04345",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/GuanzhenLi/MVP-Bench."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided Stable Diffusion",
    "summary": "arXiv:2410.04372v1 Announce Type: new \nAbstract: The rapid progress of Deepfake technology has made face swapping highly realistic, raising concerns about the malicious use of fabricated facial content. Existing methods often struggle to generalize to unseen domains due to the diverse nature of facial manipulations. In this paper, we revisit the generation process and identify a universal principle: Deepfake images inherently contain information from both source and target identities, while genuine faces maintain a consistent identity. Building upon this insight, we introduce DiffusionFake, a novel plug-and-play framework that reverses the generative process of face forgeries to enhance the generalization of detection models. DiffusionFake achieves this by injecting the features extracted by the detection model into a frozen pre-trained Stable Diffusion model, compelling it to reconstruct the corresponding target and source images. This guided reconstruction process constrains the detection network to capture the source and target related features to facilitate the reconstruction, thereby learning rich and disentangled representations that are more resilient to unseen forgeries. Extensive experiments demonstrate that DiffusionFake significantly improves cross-domain generalization of various detector architectures without introducing additional parameters during inference. Our Codes are available in https://github.com/skJack/DiffusionFake.git.",
    "link": "https://arxiv.org/abs/2410.04372",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/skJack/DiffusionFake.git."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference",
    "summary": "arXiv:2410.04417v1 Announce Type: new \nAbstract: In vision-language models (VLMs), visual tokens usually consume a significant amount of computational overhead, despite their sparser information density compared to text tokens. To address this, most existing methods learn a network to prune redundant visual tokens and require additional training data. Differently, we propose an efficient training-free token optimization mechanism dubbed SparseVLM without extra parameters or fine-tuning costs. Concretely, given that visual tokens complement text tokens in VLMs for linguistic reasoning, we select visual-relevant text tokens to rate the significance of vision tokens within the self-attention matrix extracted from the VLMs. Then we progressively prune irrelevant tokens. To maximize sparsity while retaining essential information, we introduce a rank-based strategy to adaptively determine the sparsification ratio for each layer, alongside a token recycling method that compresses pruned tokens into more compact representations. Experimental results show that our SparseVLM improves the efficiency of various VLMs across a range of image and video understanding tasks. In particular, LLaVA equipped with SparseVLM reduces 61% to 67% FLOPs with a compression ratio of 78% while maintaining 93% of the accuracy. Our code is available at https://github.com/Gumpest/SparseVLMs.",
    "link": "https://arxiv.org/abs/2410.04417",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Gumpest/SparseVLMs."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation",
    "summary": "arXiv:2410.04433v1 Announce Type: new \nAbstract: Deep neural networks (DNNs) have made significant progress in recognizing visual elements and generating descriptive text in image-captioning tasks. However, their improved performance comes from increased computational burden and inference latency. Early Exit (EE) strategies can be used to enhance their efficiency, but their adaptation presents challenges in image captioning as it requires varying levels of semantic information for accurate predictions. To overcome this, we introduce CAPEEN to improve the performance of EE strategies using knowledge distillation. Inference in CAPEEN is completed at intermediary layers if prediction confidence exceeds a predefined value learned from the training data. To account for real-world deployments, where target distributions could drift from that of training samples, we introduce a variant A-CAPEEN to adapt the thresholds on the fly using Multiarmed bandits framework. Experiments on the MS COCO and Flickr30k datasets show that CAPEEN gains speedup of 1.77x while maintaining competitive performance compared to the final layer, and A-CAPEEN additionally offers robustness against distortions. The source code is available at https://github.com/Div290/CapEEN",
    "link": "https://arxiv.org/abs/2410.04433",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Div290/CapEEN"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Optimising for the Unknown: Domain Alignment for Cephalometric Landmark Detection",
    "summary": "arXiv:2410.04445v1 Announce Type: new \nAbstract: Cephalometric Landmark Detection is the process of identifying key areas for cephalometry. Each landmark is a single GT point labelled by a clinician. A machine learning model predicts the probability locus of a landmark represented by a heatmap. This work, for the 2024 CL-Detection MICCAI Challenge, proposes a domain alignment strategy with a regional facial extraction module and an X-ray artefact augmentation procedure. The challenge ranks our method's results as the best in MRE of 1.186mm and third in the 2mm SDR of 82.04% on the online validation leaderboard. The code is available at https://github.com/Julian-Wyatt/OptimisingfortheUnknown.",
    "link": "https://arxiv.org/abs/2410.04445",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Julian-Wyatt/OptimisingfortheUnknown."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Enhancing 3D Human Pose Estimation Amidst Severe Occlusion with Dual Transformer Fusion",
    "summary": "arXiv:2410.04574v1 Announce Type: new \nAbstract: In the field of 3D Human Pose Estimation from monocular videos, the presence of diverse occlusion types presents a formidable challenge. Prior research has made progress by harnessing spatial and temporal cues to infer 3D poses from 2D joint observations. This paper introduces a Dual Transformer Fusion (DTF) algorithm, a novel approach to obtain a holistic 3D pose estimation, even in the presence of severe occlusions. Confronting the issue of occlusion-induced missing joint data, we propose a temporal interpolation-based occlusion guidance mechanism. To enable precise 3D Human Pose Estimation, our approach leverages the innovative DTF architecture, which first generates a pair of intermediate views. Each intermediate-view undergoes spatial refinement through a self-refinement schema. Subsequently, these intermediate-views are fused to yield the final 3D human pose estimation. The entire system is end-to-end trainable. Through extensive experiments conducted on the Human3.6M and MPI-INF-3DHP datasets, our method's performance is rigorously evaluated. Notably, our approach outperforms existing state-of-the-art methods on both datasets, yielding substantial improvements. The code is available here: https://github.com/MehwishG/DTF.",
    "link": "https://arxiv.org/abs/2410.04574",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/MehwishG/DTF."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "AdaptDiff: Cross-Modality Domain Adaptation via Weak Conditional Semantic Diffusion for Retinal Vessel Segmentation",
    "summary": "arXiv:2410.04648v1 Announce Type: new \nAbstract: Deep learning has shown remarkable performance in medical image segmentation. However, despite its promise, deep learning has many challenges in practice due to its inability to effectively transition to unseen domains, caused by the inherent data distribution shift and the lack of manual annotations to guide domain adaptation. To tackle this problem, we present an unsupervised domain adaptation (UDA) method named AdaptDiff that enables a retinal vessel segmentation network trained on fundus photography (FP) to produce satisfactory results on unseen modalities (e.g., OCT-A) without any manual labels. For all our target domains, we first adopt a segmentation model trained on the source domain to create pseudo-labels. With these pseudo-labels, we train a conditional semantic diffusion probabilistic model to represent the target domain distribution. Experimentally, we show that even with low quality pseudo-labels, the diffusion model can still capture the conditional semantic information. Subsequently, we sample on the target domain with binary vessel masks from the source domain to get paired data, i.e., target domain synthetic images conditioned on the binary vessel map. Finally, we fine-tune the pre-trained segmentation network using the synthetic paired data to mitigate the domain gap. We assess the effectiveness of AdaptDiff on seven publicly available datasets across three distinct modalities. Our results demonstrate a significant improvement in segmentation performance across all unseen datasets. Our code is publicly available at https://github.com/DeweiHu/AdaptDiff.",
    "link": "https://arxiv.org/abs/2410.04648",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/DeweiHu/AdaptDiff."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "PredFormer: Transformers Are Effective Spatial-Temporal Predictive Learners",
    "summary": "arXiv:2410.04733v1 Announce Type: new \nAbstract: Spatiotemporal predictive learning methods generally fall into two categories: recurrent-based approaches, which face challenges in parallelization and performance, and recurrent-free methods, which employ convolutional neural networks (CNNs) as encoder-decoder architectures. These methods benefit from strong inductive biases but often at the expense of scalability and generalization. This paper proposes PredFormer, a pure transformer-based framework for spatiotemporal predictive learning. Motivated by the Vision Transformers (ViT) design, PredFormer leverages carefully designed Gated Transformer blocks, following a comprehensive analysis of 3D attention mechanisms, including full-, factorized-, and interleaved- spatial-temporal attention. With its recurrent-free, transformer-based design, PredFormer is both simple and efficient, significantly outperforming previous methods by large margins. Extensive experiments on synthetic and real-world datasets demonstrate that PredFormer achieves state-of-the-art performance. On Moving MNIST, PredFormer achieves a 51.3% reduction in MSE relative to SimVP. For TaxiBJ, the model decreases MSE by 33.1% and boosts FPS from 533 to 2364. Additionally, on WeatherBench, it reduces MSE by 11.1% while enhancing FPS from 196 to 404. These performance gains in both accuracy and efficiency demonstrate PredFormer's potential for real-world applications. The source code will be released at https://github.com/yyyujintang/PredFormer.",
    "link": "https://arxiv.org/abs/2410.04733",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/yyyujintang/PredFormer."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality",
    "summary": "arXiv:2410.04780v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) have emerged as a central focus in both industry and academia, but often suffer from biases introduced by visual and language priors, which can lead to multimodal hallucination. These biases arise from the visual encoder and the Large Language Model (LLM) backbone, affecting the attention mechanism responsible for aligning multimodal inputs. Existing decoding-based mitigation methods focus on statistical correlations and overlook the causal relationships between attention mechanisms and model output, limiting their effectiveness in addressing these biases. To tackle this issue, we propose a causal inference framework termed CausalMM that applies structural causal modeling to MLLMs, treating modality priors as a confounder between attention mechanisms and output. Specifically, by employing backdoor adjustment and counterfactual reasoning at both the visual and language attention levels, our method mitigates the negative effects of modality priors and enhances the alignment of MLLM's inputs and outputs, with a maximum score improvement of 65.3% on 6 VLind-Bench indicators and 164 points on MME Benchmark compared to conventional methods. Extensive experiments validate the effectiveness of our approach while being a plug-and-play solution. Our code is available at: https://github.com/The-Martyr/CausalMM",
    "link": "https://arxiv.org/abs/2410.04780",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/The-Martyr/CausalMM"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Improved detection of discarded fish species through BoxAL active learning",
    "summary": "arXiv:2410.04880v1 Announce Type: new \nAbstract: In recent years, powerful data-driven deep-learning techniques have been developed and applied for automated catch registration. However, these methods are dependent on the labelled data, which is time-consuming, labour-intensive, expensive to collect and need expert knowledge. In this study, we present an active learning technique, named BoxAL, which includes estimation of epistemic certainty of the Faster R-CNN object-detection model. The method allows selecting the most uncertain training images from an unlabeled pool, which are then used to train the object-detection model. To evaluate the method, we used an open-source image dataset obtained with a dedicated image-acquisition system developed for commercial trawlers targeting demersal species. We demonstrated, that our approach allows reaching the same object-detection performance as with the random sampling using 400 fewer labelled images. Besides, mean AP score was significantly higher at the last training iteration with 1100 training images, specifically, 39.0&plusmn;1.6 and 34.8&plusmn;1.8 for certainty-based sampling and random sampling, respectively. Additionally, we showed that epistemic certainty is a suitable method to sample images that the current iteration of the model cannot deal with yet. Our study additionally showed that the sampled new data is more valuable for training than the remaining unlabeled data. Our software is available on https://github.com/pieterblok/boxal.",
    "link": "https://arxiv.org/abs/2410.04880",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/pieterblok/boxal."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "D-PoSE: Depth as an Intermediate Representation for 3D Human Pose and Shape Estimation",
    "summary": "arXiv:2410.04889v1 Announce Type: new \nAbstract: We present D-PoSE (Depth as an Intermediate Representation for 3D Human Pose and Shape Estimation), a one-stage method that estimates human pose and SMPL-X shape parameters from a single RGB image. Recent works use larger models with transformer backbones and decoders to improve the accuracy in human pose and shape (HPS) benchmarks. D-PoSE proposes a vision based approach that uses the estimated human depth-maps as an intermediate representation for HPS and leverages training with synthetic data and the ground-truth depth-maps provided with them for depth supervision during training. Although trained on synthetic datasets, D-PoSE achieves state-of-the-art performance on the real-world benchmark datasets, EMDB and 3DPW. Despite its simple lightweight design and the CNN backbone, it outperforms ViT-based models that have a number of parameters that is larger by almost an order of magnitude. D-PoSE code is available at: https://github.com/nvasilik/D-PoSE",
    "link": "https://arxiv.org/abs/2410.04889",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/nvasilik/D-PoSE"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "PRFusion: Toward Effective and Robust Multi-Modal Place Recognition with Image and Point Cloud Fusion",
    "summary": "arXiv:2410.04939v1 Announce Type: new \nAbstract: Place recognition plays a crucial role in the fields of robotics and computer vision, finding applications in areas such as autonomous driving, mapping, and localization. Place recognition identifies a place using query sensor data and a known database. One of the main challenges is to develop a model that can deliver accurate results while being robust to environmental variations. We propose two multi-modal place recognition models, namely PRFusion and PRFusion++. PRFusion utilizes global fusion with manifold metric attention, enabling effective interaction between features without requiring camera-LiDAR extrinsic calibrations. In contrast, PRFusion++ assumes the availability of extrinsic calibrations and leverages pixel-point correspondences to enhance feature learning on local windows. Additionally, both models incorporate neural diffusion layers, which enable reliable operation even in challenging environments. We verify the state-of-the-art performance of both models on three large-scale benchmarks. Notably, they outperform existing models by a substantial margin of +3.0 AR@1 on the demanding Boreas dataset. Furthermore, we conduct ablation studies to validate the effectiveness of our proposed methods. The codes are available at: https://github.com/sijieaaa/PRFusion",
    "link": "https://arxiv.org/abs/2410.04939",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/sijieaaa/PRFusion"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "RoWeeder: Unsupervised Weed Mapping through Crop-Row Detection",
    "summary": "arXiv:2410.04983v1 Announce Type: new \nAbstract: Precision agriculture relies heavily on effective weed management to ensure robust crop yields. This study presents RoWeeder, an innovative framework for unsupervised weed mapping that combines crop-row detection with a noise-resilient deep learning model. By leveraging crop-row information to create a pseudo-ground truth, our method trains a lightweight deep learning model capable of distinguishing between crops and weeds, even in the presence of noisy data. Evaluated on the WeedMap dataset, RoWeeder achieves an F1 score of 75.3, outperforming several baselines. Comprehensive ablation studies further validated the model's performance. By integrating RoWeeder with drone technology, farmers can conduct real-time aerial surveys, enabling precise weed management across large fields. The code is available at: \\url{https://github.com/pasqualedem/RoWeeder}.",
    "link": "https://arxiv.org/abs/2410.04983",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/pasqualedem/RoWeeder}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification",
    "summary": "arXiv:2410.05057v1 Announce Type: new \nAbstract: Data curation is the problem of how to collect and organize samples into a dataset that supports efficient learning. Despite the centrality of the task, little work has been devoted towards a large-scale, systematic comparison of various curation methods. In this work, we take steps towards a formal evaluation of data curation strategies and introduce SELECT, the first large-scale benchmark of curation strategies for image classification.\n  In order to generate baseline methods for the SELECT benchmark, we create a new dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K to date. Our dataset extends ImageNet with 5 new training-data shifts, each approximately the size of ImageNet-1K itself, and each assembled using a distinct curation strategy. We evaluate our data curation baselines in two ways: (i) using each training-data shift to train identical image classification models from scratch (ii) using the data itself to fit a pretrained self-supervised representation.\n  Our findings show interesting trends, particularly pertaining to recent methods for data curation such as synthetic data generation and lookup based on CLIP embeddings. We show that although these strategies are highly competitive for certain tasks, the curation strategy used to assemble the original ImageNet-1K dataset remains the gold standard. We anticipate that our benchmark can illuminate the path for new methods to further reduce the gap. We release our checkpoints, code, documentation, and a link to our dataset at https://github.com/jimmyxu123/SELECT.",
    "link": "https://arxiv.org/abs/2410.05057",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/jimmyxu123/SELECT."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "DreamSat: Towards a General 3D Model for Novel View Synthesis of Space Objects",
    "summary": "arXiv:2410.05097v1 Announce Type: new \nAbstract: Novel view synthesis (NVS) enables to generate new images of a scene or convert a set of 2D images into a comprehensive 3D model. In the context of Space Domain Awareness, since space is becoming increasingly congested, NVS can accurately map space objects and debris, improving the safety and efficiency of space operations. Similarly, in Rendezvous and Proximity Operations missions, 3D models can provide details about a target object's shape, size, and orientation, allowing for better planning and prediction of the target's behavior. In this work, we explore the generalization abilities of these reconstruction techniques, aiming to avoid the necessity of retraining for each new scene, by presenting a novel approach to 3D spacecraft reconstruction from single-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art single-view reconstruction model, on a high-quality dataset of 190 high-quality spacecraft models and integrating it into the DreamGaussian framework. We demonstrate consistent improvements in reconstruction quality across multiple metrics, including Contrastive Language-Image Pretraining (CLIP) score (+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity Index (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS) (+0.16%) on a test set of 30 previously unseen spacecraft images. Our method addresses the lack of domain-specific 3D reconstruction tools in the space industry by leveraging state-of-the-art diffusion models and 3D Gaussian splatting techniques. This approach maintains the efficiency of the DreamGaussian framework while enhancing the accuracy and detail of spacecraft reconstructions. The code for this work can be accessed on GitHub (https://github.com/ARCLab-MIT/space-nvs).",
    "link": "https://arxiv.org/abs/2410.05097",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ARCLab-MIT/space-nvs)."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality",
    "summary": "arXiv:2410.05210v1 Announce Type: new \nAbstract: In this paper, we propose a new method to enhance compositional understanding in pre-trained vision and language models (VLMs) without sacrificing performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches often improve compositional reasoning at the cost of degrading multi-modal capabilities, primarily due to the use of global hard negative (HN) loss, which contrasts global representations of images and texts. This global HN loss pushes HN texts that are highly similar to the original ones, damaging the model's multi-modal representations. To overcome this limitation, we propose Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard negative loss and selective calibrated regularization. These innovations provide fine-grained negative supervision while preserving the model's representational integrity. Our extensive evaluations across diverse benchmarks for both compositionality and multi-modal tasks show that FSC-CLIP not only achieves compositionality on par with state-of-the-art models but also retains strong multi-modal capabilities. Code is available at: https://github.com/ytaek-oh/fsc-clip.",
    "link": "https://arxiv.org/abs/2410.05210",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ytaek-oh/fsc-clip."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation Models",
    "summary": "arXiv:2410.05239v1 Announce Type: new \nAbstract: Vision-Language Models (VLMs) have shown impressive performance in vision tasks, but adapting them to new domains often requires expensive fine-tuning. Prompt tuning techniques, including textual, visual, and multimodal prompting, offer efficient alternatives by leveraging learnable prompts. However, their application to Vision-Language Segmentation Models (VLSMs) and evaluation under significant domain shifts remain unexplored. This work presents an open-source benchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal prompt tuning techniques into VLSMs, making prompt tuning usable for downstream segmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt tuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$ different combinations. We test various prompt tuning on $8$ diverse medical datasets, including $3$ radiology datasets (breast tumor, echocardiograph, chest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin cancer), and two natural domain segmentation datasets. Our study found that textual prompt tuning struggles under significant domain shifts, from natural-domain images to medical data. Furthermore, visual prompt tuning, with fewer hyperparameters than multimodal prompt tuning, often achieves performance competitive to multimodal approaches, making it a valuable first attempt. Our work advances the understanding and applicability of different prompt-tuning techniques for robust domain-specific segmentation. The source code is available at https://github.com/naamiinepal/tunevlseg.",
    "link": "https://arxiv.org/abs/2410.05239",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/naamiinepal/tunevlseg."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "SePPO: Semi-Policy Preference Optimization for Diffusion Alignment",
    "summary": "arXiv:2410.05255v1 Announce Type: new \nAbstract: Reinforcement learning from human feedback (RLHF) methods are emerging as a way to fine-tune diffusion models (DMs) for visual generation. However, commonly used on-policy strategies are limited by the generalization capability of the reward model, while off-policy approaches require large amounts of difficult-to-obtain paired human-annotated data, particularly in visual generation tasks. To address the limitations of both on- and off-policy RLHF, we propose a preference optimization method that aligns DMs with preferences without relying on reward models or paired human-annotated data. Specifically, we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO leverages previous checkpoints as reference models while using them to generate on-policy reference samples, which replace \"losing images\" in preference pairs. This approach allows us to optimize using only off-policy \"winning images.\" Furthermore, we design a strategy for reference model selection that expands the exploration in the policy space. Notably, we do not simply treat reference samples as negative examples for learning. Instead, we design an anchor-based criterion to assess whether the reference samples are likely to be winning or losing images, allowing the model to selectively learn from the generated reference samples. This approach mitigates performance degradation caused by the uncertainty in reference sample quality. We validate SePPO across both text-to-image and text-to-video benchmarks. SePPO surpasses all previous approaches on the text-to-image benchmarks and also demonstrates outstanding performance on the text-to-video benchmarks. Code will be released in https://github.com/DwanZhang-AI/SePPO.",
    "link": "https://arxiv.org/abs/2410.05255",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/DwanZhang-AI/SePPO."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Fine-Tuning CLIP's Last Visual Projector: A Few-Shot Cornucopia",
    "summary": "arXiv:2410.05270v1 Announce Type: new \nAbstract: We consider the problem of adapting a contrastively pretrained vision-language model like CLIP (Radford et al., 2021) for few-shot classification. The existing literature addresses this problem by learning a linear classifier of the frozen visual features, optimizing word embeddings, or learning external feature adapters. This paper introduces an alternative way for CLIP adaptation without adding 'external' parameters to optimize. We find that simply fine-tuning the last projection matrix of the vision encoder leads to strong performance compared to the existing baselines. Furthermore, we show that regularizing training with the distance between the fine-tuned and pretrained matrices adds reliability for adapting CLIP through this layer. Perhaps surprisingly, this approach, coined ProLIP, yields performances on par or better than state of the art on 11 few-shot classification benchmarks, few-shot domain generalization, cross-dataset transfer and test-time adaptation. Code will be made available at https://github.com/astra-vision/ProLIP .",
    "link": "https://arxiv.org/abs/2410.05270",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/astra-vision/ProLIP"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Clustering Alzheimer's Disease Subtypes via Similarity Learning and Graph Diffusion",
    "summary": "arXiv:2410.03937v1 Announce Type: cross \nAbstract: Alzheimer's disease (AD) is a complex neurodegenerative disorder that affects millions of people worldwide. Due to the heterogeneous nature of AD, its diagnosis and treatment pose critical challenges. Consequently, there is a growing research interest in identifying homogeneous AD subtypes that can assist in addressing these challenges in recent years. In this study, we aim to identify subtypes of AD that represent distinctive clinical features and underlying pathology by utilizing unsupervised clustering with graph diffusion and similarity learning. We adopted SIMLR, a multi-kernel similarity learning framework, and graph diffusion to perform clustering on a group of 829 patients with AD and mild cognitive impairment (MCI, a prodromal stage of AD) based on their cortical thickness measurements extracted from magnetic resonance imaging (MRI) scans. Although the clustering approach we utilized has not been explored for the task of AD subtyping before, it demonstrated significantly better performance than several commonly used clustering methods. Specifically, we showed the power of graph diffusion in reducing the effects of noise in the subtype detection. Our results revealed five subtypes that differed remarkably in their biomarkers, cognitive status, and some other clinical features. To evaluate the resultant subtypes further, a genetic association study was carried out and successfully identified potential genetic underpinnings of different AD subtypes. Our source code is available at: https://github.com/PennShenLab/AD-SIMLR.",
    "link": "https://arxiv.org/abs/2410.03937",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/PennShenLab/AD-SIMLR."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "SpecSAR-Former: A Lightweight Transformer-based Network for Global LULC Mapping Using Integrated Sentinel-1 and Sentinel-2",
    "summary": "arXiv:2410.03962v1 Announce Type: cross \nAbstract: Recent approaches in remote sensing have increasingly focused on multimodal data, driven by the growing availability of diverse earth observation datasets. Integrating complementary information from different modalities has shown substantial potential in enhancing semantic understanding. However, existing global multimodal datasets often lack the inclusion of Synthetic Aperture Radar (SAR) data, which excels at capturing texture and structural details. SAR, as a complementary perspective to other modalities, facilitates the utilization of spatial information for global land use and land cover (LULC). To address this gap, we introduce the Dynamic World+ dataset, expanding the current authoritative multispectral dataset, Dynamic World, with aligned SAR data. Additionally, to facilitate the combination of multispectral and SAR data, we propose a lightweight transformer architecture termed SpecSAR-Former. It incorporates two innovative modules, Dual Modal Enhancement Module (DMEM) and Mutual Modal Aggregation Module (MMAM), designed to exploit cross-information between the two modalities in a split-fusion manner. These modules enhance the model's ability to integrate spectral and spatial information, thereby improving the overall performance of global LULC semantic segmentation. Furthermore, we adopt an imbalanced parameter allocation strategy that assigns parameters to different modalities based on their importance and information density. Extensive experiments demonstrate that our network outperforms existing transformer and CNN-based models, achieving a mean Intersection over Union (mIoU) of 59.58%, an Overall Accuracy (OA) of 79.48%, and an F1 Score of 71.68% with only 26.70M parameters. The code will be available at https://github.com/Reagan1311/LULC_segmentation.",
    "link": "https://arxiv.org/abs/2410.03962",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Reagan1311/LULC_segmentation."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Optimizing Medical Image Segmentation with Advanced Decoder Design",
    "summary": "arXiv:2410.04128v1 Announce Type: cross \nAbstract: U-Net is widely used in medical image segmentation due to its simple and flexible architecture design. To address the challenges of scale and complexity in medical tasks, several variants of U-Net have been proposed. In particular, methods based on Vision Transformer (ViT), represented by Swin UNETR, have gained widespread attention in recent years. However, these improvements often focus on the encoder, overlooking the crucial role of the decoder in optimizing segmentation details. This design imbalance limits the potential for further enhancing segmentation performance. To address this issue, we analyze the roles of various decoder components, including upsampling method, skip connection, and feature extraction module, as well as the shortcomings of existing methods. Consequently, we propose Swin DER (i.e., Swin UNETR Decoder Enhanced and Refined) by specifically optimizing the design of these three components. Swin DER performs upsampling using learnable interpolation algorithm called offset coordinate neighborhood weighted up sampling (Onsampling) and replaces traditional skip connection with spatial-channel parallel attention gate (SCP AG). Additionally, Swin DER introduces deformable convolution along with attention mechanism in the feature extraction module of the decoder. Our model design achieves excellent results, surpassing other state-of-the-art methods on both the Synapse and the MSD brain tumor segmentation task.\n  Code is available at: https://github.com/WillBeanYang/Swin-DER",
    "link": "https://arxiv.org/abs/2410.04128",
    "published": "Tue, 08 Oct 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/WillBeanYang/Swin-DER"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  }
]