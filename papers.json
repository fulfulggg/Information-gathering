[
  {
    "title": "Transformer-Enhanced Iterative Feedback Mechanism for Polyp Segmentation",
    "summary": "arXiv:2409.05875v1 Announce Type: new \nAbstract: Colorectal cancer (CRC) is the third most common cause of cancer diagnosed in the United States and the second leading cause of cancer-related death among both genders. Notably, CRC is the leading cause of cancer in younger men less than 50 years old. Colonoscopy is considered the gold standard for the early diagnosis of CRC. Skills vary significantly among endoscopists, and a high miss rate is reported. Automated polyp segmentation can reduce the missed rates, and timely treatment is possible in the early stage. To address this challenge, we introduce \\textit{\\textbf{\\ac{FANetv2}}}, an advanced encoder-decoder network designed to accurately segment polyps from colonoscopy images. Leveraging an initial input mask generated by Otsu thresholding, FANetv2 iteratively refines its binary segmentation masks through a novel feedback attention mechanism informed by the mask predictions of previous epochs. Additionally, it employs a text-guided approach that integrates essential information about the number (one or many) and size (small, medium, large) of polyps to further enhance its feature representation capabilities. This dual-task approach facilitates accurate polyp segmentation and aids in the auxiliary classification of polyp attributes, significantly boosting the model's performance. Our comprehensive evaluations on the publicly available BKAI-IGH and CVC-ClinicDB datasets demonstrate the superior performance of FANetv2, evidenced by high dice similarity coefficients (DSC) of 0.9186 and 0.9481, along with low Hausdorff distances of 2.83 and 3.19, respectively. The source code for FANetv2 is available at https://github.com/xxxxx/FANetv2.",
    "link": "https://arxiv.org/abs/2409.05875",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/xxxxx/FANetv2."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "DiffusionPen: Towards Controlling the Style of Handwritten Text Generation",
    "summary": "arXiv:2409.06065v1 Announce Type: new \nAbstract: Handwritten Text Generation (HTG) conditioned on text and style is a challenging task due to the variability of inter-user characteristics and the unlimited combinations of characters that form new words unseen during training. Diffusion Models have recently shown promising results in HTG but still remain under-explored. We present DiffusionPen (DiffPen), a 5-shot style handwritten text generation approach based on Latent Diffusion Models. By utilizing a hybrid style extractor that combines metric learning and classification, our approach manages to capture both textual and stylistic characteristics of seen and unseen words and styles, generating realistic handwritten samples. Moreover, we explore several variation strategies of the data with multi-style mixtures and noisy embeddings, enhancing the robustness and diversity of the generated data. Extensive experiments using IAM offline handwriting database show that our method outperforms existing methods qualitatively and quantitatively, and its additional generated data can improve the performance of Handwriting Text Recognition (HTR) systems. The code is available at: https://github.com/koninik/DiffusionPen.",
    "link": "https://arxiv.org/abs/2409.06065",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/koninik/DiffusionPen."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "LSE-NeRF: Learning Sensor Modeling Errors for Deblured Neural Radiance Fields with RGB-Event Stereo",
    "summary": "arXiv:2409.06104v1 Announce Type: new \nAbstract: We present a method for reconstructing a clear Neural Radiance Field (NeRF) even with fast camera motions. To address blur artifacts, we leverage both (blurry) RGB images and event camera data captured in a binocular configuration. Importantly, when reconstructing our clear NeRF, we consider the camera modeling imperfections that arise from the simple pinhole camera model as learned embeddings for each camera measurement, and further learn a mapper that connects event camera measurements with RGB data. As no previous dataset exists for our binocular setting, we introduce an event camera dataset with captures from a 3D-printed stereo configuration between RGB and event cameras. Empirically, we evaluate our introduced dataset and EVIMOv2 and show that our method leads to improved reconstructions. Our code and dataset are available at https://github.com/ubc-vision/LSENeRF.",
    "link": "https://arxiv.org/abs/2409.06104",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ubc-vision/LSENeRF."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "UniLearn: Enhancing Dynamic Facial Expression Recognition through Unified Pre-Training and Fine-Tuning on Images and Videos",
    "summary": "arXiv:2409.06154v1 Announce Type: new \nAbstract: Dynamic facial expression recognition (DFER) is essential for understanding human emotions and behavior. However, conventional DFER methods, which primarily use dynamic facial data, often underutilize static expression images and their labels, limiting their performance and robustness. To overcome this, we introduce UniLearn, a novel unified learning paradigm that integrates static facial expression recognition (SFER) data to enhance DFER task. UniLearn employs a dual-modal self-supervised pre-training method, leveraging both facial expression images and videos to enhance a ViT model's spatiotemporal representation capability. Then, the pre-trained model is fine-tuned on both static and dynamic expression datasets using a joint fine-tuning strategy. To prevent negative transfer during joint fine-tuning, we introduce an innovative Mixture of Adapter Experts (MoAE) module that enables task-specific knowledge acquisition and effectively integrates information from both static and dynamic expression data. Extensive experiments demonstrate UniLearn's effectiveness in leveraging complementary information from static and dynamic facial data, leading to more accurate and robust DFER. UniLearn consistently achieves state-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with weighted average recall (WAR) of 53.65\\%, 58.44\\%, and 76.68\\%, respectively. The source code and model weights will be publicly available at \\url{https://github.com/MSA-LMC/UniLearn}.",
    "link": "https://arxiv.org/abs/2409.06154",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/MSA-LMC/UniLearn}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Loss Distillation via Gradient Matching for Point Cloud Completion with Weighted Chamfer Distance",
    "summary": "arXiv:2409.06171v1 Announce Type: new \nAbstract: 3D point clouds enhanced the robot's ability to perceive the geometrical information of the environments, making it possible for many downstream tasks such as grasp pose detection and scene understanding. The performance of these tasks, though, heavily relies on the quality of data input, as incomplete can lead to poor results and failure cases. Recent training loss functions designed for deep learning-based point cloud completion, such as Chamfer distance (CD) and its variants (\\eg HyperCD ), imply a good gradient weighting scheme can significantly boost performance. However, these CD-based loss functions usually require data-related parameter tuning, which can be time-consuming for data-extensive tasks. To address this issue, we aim to find a family of weighted training losses ({\\em weighted CD}) that requires no parameter tuning. To this end, we propose a search scheme, {\\em Loss Distillation via Gradient Matching}, to find good candidate loss functions by mimicking the learning behavior in backpropagation between HyperCD and weighted CD. Once this is done, we propose a novel bilevel optimization formula to train the backbone network based on the weighted CD loss. We observe that: (1) with proper weighted functions, the weighted CD can always achieve similar performance to HyperCD, and (2) the Landau weighted CD, namely {\\em Landau CD}, can outperform HyperCD for point cloud completion and lead to new state-of-the-art results on several benchmark datasets. {\\it Our demo code is available at \\url{https://github.com/Zhang-VISLab/IROS2024-LossDistillationWeightedCD}.}",
    "link": "https://arxiv.org/abs/2409.06171",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Zhang-VISLab/IROS2024-LossDistillationWeightedCD}.}"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "EDADepth: Enhanced Data Augmentation for Monocular Depth Estimation",
    "summary": "arXiv:2409.06183v1 Announce Type: new \nAbstract: Due to their text-to-image synthesis feature, diffusion models have recently seen a rise in visual perception tasks, such as depth estimation. The lack of good-quality datasets makes the extraction of a fine-grain semantic context challenging for the diffusion models. The semantic context with fewer details further worsens the process of creating effective text embeddings that will be used as input for diffusion models. In this paper, we propose a novel EDADepth, an enhanced data augmentation method to estimate monocular depth without using additional training data. We use Swin2SR, a super-resolution model, to enhance the quality of input images. We employ the BEiT pre-trained semantic segmentation model for better extraction of text embeddings. We introduce BLIP-2 tokenizer to generate tokens from these text embeddings. The novelty of our approach is the introduction of Swin2SR, the BEiT model, and the BLIP-2 tokenizer in the diffusion-based pipeline for the monocular depth estimation. Our model achieves state-of-the-art results (SOTA) on the {\\delta}3 metric on NYUv2 and KITTI datasets. It also achieves results comparable to those of the SOTA models in the RMSE and REL metrics. Finally, we also show improvements in the visualization of the estimated depth compared to the SOTA diffusion-based monocular depth estimation models. Code: https://github.com/edadepthmde/EDADepth_ICMLA.",
    "link": "https://arxiv.org/abs/2409.06183",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/edadepthmde/EDADepth_ICMLA."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Deep kernel representations of latent space features for low-dose PET-MR imaging robust to variable dose reduction",
    "summary": "arXiv:2409.06198v1 Announce Type: new \nAbstract: Low-dose positron emission tomography (PET) image reconstruction methods have potential to significantly improve PET as an imaging modality. Deep learning provides a promising means of incorporating prior information into the image reconstruction problem to produce quantitatively accurate images from compromised signal. Deep learning-based methods for low-dose PET are generally poorly conditioned and perform unreliably on images with features not present in the training distribution. We present a method which explicitly models deep latent space features using a robust kernel representation, providing robust performance on previously unseen dose reduction factors. Additional constraints on the information content of deep latent features allow for tuning in-distribution accuracy and generalisability. Tests with out-of-distribution dose reduction factors ranging from $\\times 10$ to $\\times 1000$ and with both paired and unpaired MR, demonstrate significantly improved performance relative to conventional deep-learning methods trained using the same data. Code:https://github.com/cameronPain",
    "link": "https://arxiv.org/abs/2409.06198",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/cameronPain"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online Surgical Phase Recognition",
    "summary": "arXiv:2409.06217v1 Announce Type: new \nAbstract: Surgical phase recognition has become a crucial requirement in laparoscopic surgery, enabling various clinical applications like surgical risk forecasting. Current methods typically identify the surgical phase using individual frame-wise embeddings as the fundamental unit for time modeling. However, this approach is overly sensitive to current observations, often resulting in discontinuous and erroneous predictions within a complete surgical phase. In this paper, we propose DACAT, a novel dual-stream model that adaptively learns clip-aware context information to enhance the temporal relationship. In one stream, DACAT pretrains a frame encoder, caching all historical frame-wise features. In the other stream, DACAT fine-tunes a new frame encoder to extract the frame-wise feature at the current moment. Additionally, a max clip-response read-out (Max-R) module is introduced to bridge the two streams by using the current frame-wise feature to adaptively fetch the most relevant past clip from the feature cache. The clip-aware context feature is then encoded via cross-attention between the current frame and its fetched adaptive clip, and further utilized to enhance the time modeling for accurate online surgical phase recognition. The benchmark results on three public datasets, i.e., Cholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed DACAT over existing state-of-the-art methods, with improvements in Jaccard scores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have been released at https://github.com/kk42yy/DACAT.",
    "link": "https://arxiv.org/abs/2409.06217",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/kk42yy/DACAT."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "MIP-GAF: A MLLM-annotated Benchmark for Most Important Person Localization and Group Context Understanding",
    "summary": "arXiv:2409.06224v1 Announce Type: new \nAbstract: Estimating the Most Important Person (MIP) in any social event setup is a challenging problem mainly due to contextual complexity and scarcity of labeled data. Moreover, the causality aspects of MIP estimation are quite subjective and diverse. To this end, we aim to address the problem by annotating a large-scale `in-the-wild' dataset for identifying human perceptions about the `Most Important Person (MIP)' in an image. The paper provides a thorough description of our proposed Multimodal Large Language Model (MLLM) based data annotation strategy, and a thorough data quality analysis. Further, we perform a comprehensive benchmarking of the proposed dataset utilizing state-of-the-art MIP localization methods, indicating a significant drop in performance compared to existing datasets. The performance drop shows that the existing MIP localization algorithms must be more robust with respect to `in-the-wild' situations. We believe the proposed dataset will play a vital role in building the next-generation social situation understanding methods. The code and data is available at https://github.com/surbhimadan92/MIP-GAF.",
    "link": "https://arxiv.org/abs/2409.06224",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/surbhimadan92/MIP-GAF."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "ALSS-YOLO: An Adaptive Lightweight Channel Split and Shuffling Network for TIR Wildlife Detection in UAV Imagery",
    "summary": "arXiv:2409.06259v1 Announce Type: new \nAbstract: Unmanned aerial vehicles (UAVs) equipped with thermal infrared (TIR) cameras play a crucial role in combating nocturnal wildlife poaching. However, TIR images often face challenges such as jitter, and wildlife overlap, necessitating UAVs to possess the capability to identify blurred and overlapping small targets. Current traditional lightweight networks deployed on UAVs struggle to extract features from blurry small targets. To address this issue, we developed ALSS-YOLO, an efficient and lightweight detector optimized for TIR aerial images. Firstly, we propose a novel Adaptive Lightweight Channel Split and Shuffling (ALSS) module. This module employs an adaptive channel split strategy to optimize feature extraction and integrates a channel shuffling mechanism to enhance information exchange between channels. This improves the extraction of blurry features, crucial for handling jitter-induced blur and overlapping targets. Secondly, we developed a Lightweight Coordinate Attention (LCA) module that employs adaptive pooling and grouped convolution to integrate feature information across dimensions. This module ensures lightweight operation while maintaining high detection precision and robustness against jitter and target overlap. Additionally, we developed a single-channel focus module to aggregate the width and height information of each channel into four-dimensional channel fusion, which improves the feature representation efficiency of infrared images. Finally, we modify the localization loss function to emphasize the loss value associated with small objects to improve localization accuracy. Extensive experiments on the BIRDSAI and ISOD TIR UAV wildlife datasets show that ALSS-YOLO achieves state-of-the-art performance, Our code is openly available at https://github.com/helloworlder8/computer_vision.",
    "link": "https://arxiv.org/abs/2409.06259",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/helloworlder8/computer_vision."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Mahalanobis k-NN: A Statistical Lens for Robust Point-Cloud Registrations",
    "summary": "arXiv:2409.06267v1 Announce Type: new \nAbstract: In this paper, we discuss Mahalanobis k-NN: a statistical lens designed to address the challenges of feature matching in learning-based point cloud registration when confronted with an arbitrary density of point clouds, either in the source or target point cloud. We tackle this by adopting Mahalanobis k-NN's inherent property to capture the distribution of the local neighborhood and surficial geometry. Our method can be seamlessly integrated into any local-graph-based point cloud analysis method. In this paper, we focus on two distinct methodologies: Deep Closest Point (DCP) and Deep Universal Manifold Embedding (DeepUME). Our extensive benchmarking on the ModelNet40 and Faust datasets highlights the efficacy of the proposed method in point cloud registration tasks. Moreover, we establish for the first time that the features acquired through point cloud registration inherently can possess discriminative capabilities. This is evident by a substantial improvement of about 20\\% in the average accuracy observed in the point cloud few-shot classification task benchmarked on ModelNet40 and ScanObjectNN. The code is publicly available at https://github.com/TejasAnvekar/Mahalanobis-k-NN",
    "link": "https://arxiv.org/abs/2409.06267",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/TejasAnvekar/Mahalanobis-k-NN"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Context Enhancement with Reconstruction as Sequence for Unified Unsupervised Anomaly Detection",
    "summary": "arXiv:2409.06285v1 Announce Type: new \nAbstract: Unsupervised anomaly detection (AD) aims to train robust detection models using only normal samples, while can generalize well to unseen anomalies. Recent research focuses on a unified unsupervised AD setting in which only one model is trained for all classes, i.e., n-class-one-model paradigm. Feature-reconstruction-based methods achieve state-of-the-art performance in this scenario. However, existing methods often suffer from a lack of sufficient contextual awareness, thereby compromising the quality of the reconstruction. To address this issue, we introduce a novel Reconstruction as Sequence (RAS) method, which enhances the contextual correspondence during feature reconstruction from a sequence modeling perspective. In particular, based on the transformer technique, we integrate a specialized RASFormer block into RAS. This block enables the capture of spatial relationships among different image regions and enhances sequential dependencies throughout the reconstruction process. By incorporating the RASFormer block, our RAS method achieves superior contextual awareness capabilities, leading to remarkable performance. Experimental results show that our RAS significantly outperforms competing methods, well demonstrating the effectiveness and superiority of our method. Our code is available at https://github.com/Nothingtolose9979/RAS.",
    "link": "https://arxiv.org/abs/2409.06285",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Nothingtolose9979/RAS."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "EntAugment: Entropy-Driven Adaptive Data Augmentation Framework for Image Classification",
    "summary": "arXiv:2409.06290v1 Announce Type: new \nAbstract: Data augmentation (DA) has been widely used to improve the generalization of deep neural networks. While existing DA methods have proven effective, they often rely on augmentation operations with random magnitudes to each sample. However, this approach can inadvertently introduce noise, induce distribution shifts, and increase the risk of overfitting. In this paper, we propose EntAugment, a tuning-free and adaptive DA framework. Unlike previous work, EntAugment dynamically assesses and adjusts the augmentation magnitudes for each sample during training, leveraging insights into both the inherent complexities of training samples and the evolving status of deep models. Specifically, in EntAugment, the magnitudes are determined by the information entropy derived from the probability distribution obtained by applying the softmax function to the model's output. In addition, to further enhance the efficacy of EntAugment, we introduce a novel entropy regularization term, EntLoss, which complements the EntAugment approach. Theoretical analysis further demonstrates that EntLoss, compared to traditional cross-entropy loss, achieves closer alignment between the model distributions and underlying dataset distributions. Moreover, EntAugment and EntLoss can be utilized separately or jointly. We conduct extensive experiments across multiple image classification tasks and network architectures with thorough comparisons of existing DA methods. Importantly, the proposed methods outperform others without introducing any auxiliary models or noticeable extra computational costs, highlighting both effectiveness and efficiency. Code is available at https://github.com/Jackbrocp/EntAugment.",
    "link": "https://arxiv.org/abs/2409.06290",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Jackbrocp/EntAugment."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "High-Performance Few-Shot Segmentation with Foundation Models: An Empirical Study",
    "summary": "arXiv:2409.06305v1 Announce Type: new \nAbstract: Existing few-shot segmentation (FSS) methods mainly focus on designing novel support-query matching and self-matching mechanisms to exploit implicit knowledge in pre-trained backbones. However, the performance of these methods is often constrained by models pre-trained on classification tasks. The exploration of what types of pre-trained models can provide more beneficial implicit knowledge for FSS remains limited. In this paper, inspired by the representation consistency of foundational computer vision models, we develop a FSS framework based on foundation models. To be specific, we propose a simple approach to extract implicit knowledge from foundation models to construct coarse correspondence and introduce a lightweight decoder to refine coarse correspondence for fine-grained segmentation. We systematically summarize the performance of various foundation models on FSS and discover that the implicit knowledge within some of these models is more beneficial for FSS than models pre-trained on classification tasks. Extensive experiments on two widely used datasets demonstrate the effectiveness of our approach in leveraging the implicit knowledge of foundation models. Notably, the combination of DINOv2 and DFN exceeds previous state-of-the-art methods by 17.5% on COCO-20i. Code is available at https://github.com/DUT-CSJ/FoundationFSS.",
    "link": "https://arxiv.org/abs/2409.06305",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/DUT-CSJ/FoundationFSS."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "AMNS: Attention-Weighted Selective Mask and Noise Label Suppression for Text-to-Image Person Retrieval",
    "summary": "arXiv:2409.06385v1 Announce Type: new \nAbstract: Text-to-image person retrieval aims to retrieve images of person given textual descriptions, and most methods implicitly assume that the training image-text pairs are correctly aligned, but in practice, under-correlated and false-correlated problems arise for image-text pairs due to poor image quality and mislabeling. Meanwhile, the random masking augmentation strategy may incorrectly discard semantic content resulting in the problem of generating noisy pairings between image lexical elements and text descriptions. To solve these two problems, we propose a new noise label suppression method and alleviate the problem generated by random mask through an attention-weighted selective mask strategy. In the proposed noise label suppression method, the effect of noise labels is suppressed by preventing the model from being overconfident by considering the inverse KL scatter loss, which is combined with the weight adjustment focus loss to further improve the model's recognition ability on difficult samples. On the other hand, Attention-Weighted Selective Mask processes the raw image through the EMA version of the image encoder, retaining some of the tokens with strong semantic associations with the corresponding text descriptions in order to extract better features. Numerous experiments validate the effectiveness of our approach in terms of dealing with noisy problems. The code will be available soon at https://github.com/RunQing715/AMNS.git.",
    "link": "https://arxiv.org/abs/2409.06385",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/RunQing715/AMNS.git."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Prompt2Fashion: An automatically generated fashion dataset",
    "summary": "arXiv:2409.06442v1 Announce Type: new \nAbstract: Despite the rapid evolution and increasing efficacy of language and vision generative models, there remains a lack of comprehensive datasets that bridge the gap between personalized fashion needs and AI-driven design, limiting the potential for truly inclusive and customized fashion solutions. In this work, we leverage generative models to automatically construct a fashion image dataset tailored to various occasions, styles, and body types as instructed by users. We use different Large Language Models (LLMs) and prompting strategies to offer personalized outfits of high aesthetic quality, detail, and relevance to both expert and non-expert users' requirements, as demonstrated by qualitative analysis. Up until now the evaluation of the generated outfits has been conducted by non-expert human subjects. Despite the provided fine-grained insights on the quality and relevance of generation, we extend the discussion on the importance of expert knowledge for the evaluation of artistic AI-generated datasets such as this one. Our dataset is publicly available on GitHub at https://github.com/georgiarg/Prompt2Fashion.",
    "link": "https://arxiv.org/abs/2409.06442",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/georgiarg/Prompt2Fashion."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Learning Generative Interactive Environments By Trained Agent Exploration",
    "summary": "arXiv:2409.06445v1 Announce Type: new \nAbstract: World models are increasingly pivotal in interpreting and simulating the rules and actions of complex environments. Genie, a recent model, excels at learning from visually diverse environments but relies on costly human-collected data. We observe that their alternative method of using random agents is too limited to explore the environment. We propose to improve the model by employing reinforcement learning based agents for data generation. This approach produces diverse datasets that enhance the model's ability to adapt and perform well across various scenarios and realistic actions within the environment. In this paper, we first release the model GenieRedux - an implementation based on Genie. Additionally, we introduce GenieRedux-G, a variant that uses the agent's readily available actions to factor out action prediction uncertainty during validation. Our evaluation, including a replication of the Coinrun case study, shows that GenieRedux-G achieves superior visual fidelity and controllability using the trained agent exploration. The proposed approach is reproducable, scalable and adaptable to new types of environments. Our codebase is available at https://github.com/insait-institute/GenieRedux .",
    "link": "https://arxiv.org/abs/2409.06445",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/insait-institute/GenieRedux"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Neural Laplacian Operator for 3D Point Clouds",
    "summary": "arXiv:2409.06506v1 Announce Type: new \nAbstract: The discrete Laplacian operator holds a crucial role in 3D geometry processing, yet it is still challenging to define it on point clouds. Previous works mainly focused on constructing a local triangulation around each point to approximate the underlying manifold for defining the Laplacian operator, which may not be robust or accurate. In contrast, we simply use the K-nearest neighbors (KNN) graph constructed from the input point cloud and learn the Laplacian operator on the KNN graph with graph neural networks (GNNs). However, the ground-truth Laplacian operator is defined on a manifold mesh with a different connectivity from the KNN graph and thus cannot be directly used for training. To train the GNN, we propose a novel training scheme by imitating the behavior of the ground-truth Laplacian operator on a set of probe functions so that the learned Laplacian operator behaves similarly to the ground-truth Laplacian operator. We train our network on a subset of ShapeNet and evaluate it across a variety of point clouds. Compared with previous methods, our method reduces the error by an order of magnitude and excels in handling sparse point clouds with thin structures or sharp features. Our method also demonstrates a strong generalization ability to unseen shapes. With our learned Laplacian operator, we further apply a series of Laplacian-based geometry processing algorithms directly to point clouds and achieve accurate results, enabling many exciting possibilities for geometry processing on point clouds. The code and trained models are available at https://github.com/IntelligentGeometry/NeLo.",
    "link": "https://arxiv.org/abs/2409.06506",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/IntelligentGeometry/NeLo."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "When to Extract ReID Features: A Selective Approach for Improved Multiple Object Tracking",
    "summary": "arXiv:2409.06617v1 Announce Type: new \nAbstract: Extracting and matching Re-Identification (ReID) features is used by many state-of-the-art (SOTA) Multiple Object Tracking (MOT) methods, particularly effective against frequent and long-term occlusions. While end-to-end object detection and tracking have been the main focus of recent research, they have yet to outperform traditional methods in benchmarks like MOT17 and MOT20. Thus, from an application standpoint, methods with separate detection and embedding remain the best option for accuracy, modularity, and ease of implementation, though they are impractical for edge devices due to the overhead involved. In this paper, we investigate a selective approach to minimize the overhead of feature extraction while preserving accuracy, modularity, and ease of implementation. This approach can be integrated into various SOTA methods. We demonstrate its effectiveness by applying it to StrongSORT and Deep OC-SORT. Experiments on MOT17, MOT20, and DanceTrack datasets show that our mechanism retains the advantages of feature extraction during occlusions while significantly reducing runtime. Additionally, it improves accuracy by preventing confusion in the feature-matching stage, particularly in cases of deformation and appearance similarity, which are common in DanceTrack. https://github.com/emirhanbayar/Fast-StrongSORT, https://github.com/emirhanbayar/Fast-Deep-OC-SORT",
    "link": "https://arxiv.org/abs/2409.06617",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/emirhanbayar/Fast-StrongSORT,",
      "https://github.com/emirhanbayar/Fast-Deep-OC-SORT"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "GeoCalib: Learning Single-image Calibration with Geometric Optimization",
    "summary": "arXiv:2409.06704v1 Announce Type: new \nAbstract: From a single image, visual cues can help deduce intrinsic and extrinsic camera parameters like the focal length and the gravity direction. This single-image calibration can benefit various downstream applications like image editing and 3D mapping. Current approaches to this problem are based on either classical geometry with lines and vanishing points or on deep neural networks trained end-to-end. The learned approaches are more robust but struggle to generalize to new environments and are less accurate than their classical counterparts. We hypothesize that they lack the constraints that 3D geometry provides. In this work, we introduce GeoCalib, a deep neural network that leverages universal rules of 3D geometry through an optimization process. GeoCalib is trained end-to-end to estimate camera parameters and learns to find useful visual cues from the data. Experiments on various benchmarks show that GeoCalib is more robust and more accurate than existing classical and learned approaches. Its internal optimization estimates uncertainties, which help flag failure cases and benefit downstream applications like visual localization. The code and trained models are publicly available at https://github.com/cvg/GeoCalib.",
    "link": "https://arxiv.org/abs/2409.06704",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/cvg/GeoCalib."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Memory-Optimized Once-For-All Network",
    "summary": "arXiv:2409.05900v1 Announce Type: cross \nAbstract: Deploying Deep Neural Networks (DNNs) on different hardware platforms is challenging due to varying resource constraints. Besides handcrafted approaches aiming at making deep models hardware-friendly, Neural Architectures Search is rising as a toolbox to craft more efficient DNNs without sacrificing performance. Among these, the Once-For-All (OFA) approach offers a solution by allowing the sampling of well-performing sub-networks from a single supernet -- this leads to evident advantages in terms of computation. However, OFA does not fully utilize the potential memory capacity of the target device, focusing instead on limiting maximum memory usage per layer. This leaves room for an unexploited potential in terms of model generalizability. In this paper, we introduce a Memory-Optimized OFA (MOOFA) supernet, designed to enhance DNN deployment on resource-limited devices by maximizing memory usage (and for instance, features diversity) across different configurations. Tested on ImageNet, our MOOFA supernet demonstrates improvements in memory exploitation and model accuracy compared to the original OFA supernet. Our code is available at https://github.com/MaximeGirard/memory-optimized-once-for-all.",
    "link": "https://arxiv.org/abs/2409.05900",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/MaximeGirard/memory-optimized-once-for-all."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Caterpillar: A Pure-MLP Architecture with Shifted-Pillars-Concatenation",
    "summary": "arXiv:2305.17644v3 Announce Type: replace \nAbstract: Modeling in Computer Vision has evolved to MLPs. Vision MLPs naturally lack local modeling capability, to which the simplest treatment is combined with convolutional layers. Convolution, famous for its sliding window scheme, also suffers from this scheme of redundancy and lower parallel computation. In this paper, we seek to dispense with the windowing scheme and introduce a more elaborate and parallelizable method to exploit locality. To this end, we propose a new MLP module, namely Shifted-Pillars-Concatenation (SPC), that consists of two steps of processes: (1) Pillars-Shift, which generates four neighboring maps by shifting the input image along four directions, and (2) Pillars-Concatenation, which applies linear transformations and concatenation on the maps to aggregate local features. SPC module offers superior local modeling power and performance gains, making it a promising alternative to the convolutional layer. Then, we build a pure-MLP architecture called Caterpillar by replacing the convolutional layer with the SPC module in a hybrid model of sMLPNet. Extensive experiments show Caterpillar's excellent performance on both small-scale and ImageNet-1k classification benchmarks, with remarkable scalability and transfer capability possessed as well. The code is available at https://github.com/sunjin19126/Caterpillar.",
    "link": "https://arxiv.org/abs/2305.17644",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/sunjin19126/Caterpillar."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "LiDAR-based 4D Occupancy Completion and Forecasting",
    "summary": "arXiv:2310.11239v2 Announce Type: replace \nAbstract: Scene completion and forecasting are two popular perception problems in research for mobile agents like autonomous vehicles. Existing approaches treat the two problems in isolation, resulting in a separate perception of the two aspects. In this paper, we introduce a novel LiDAR perception task of Occupancy Completion and Forecasting (OCF) in the context of autonomous driving to unify these aspects into a cohesive framework. This task requires new algorithms to address three challenges altogether: (1) sparse-to-dense reconstruction, (2) partial-to-complete hallucination, and (3) 3D-to-4D prediction. To enable supervision and evaluation, we curate a large-scale dataset termed OCFBench from public autonomous driving datasets. We analyze the performance of closely related existing baseline models and our own ones on our dataset. We envision that this research will inspire and call for further investigation in this evolving and crucial area of 4D perception. Our code for data curation and baseline implementation is available at https://github.com/ai4ce/Occ4cast.",
    "link": "https://arxiv.org/abs/2310.11239",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ai4ce/Occ4cast."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Adversarial Score Distillation: When score distillation meets GAN",
    "summary": "arXiv:2312.00739v2 Announce Type: replace \nAbstract: Existing score distillation methods are sensitive to classifier-free guidance (CFG) scale: manifested as over-smoothness or instability at small CFG scales, while over-saturation at large ones. To explain and analyze these issues, we revisit the derivation of Score Distillation Sampling (SDS) and decipher existing score distillation with the Wasserstein Generative Adversarial Network (WGAN) paradigm. With the WGAN paradigm, we find that existing score distillation either employs a fixed sub-optimal discriminator or conducts incomplete discriminator optimization, resulting in the scale-sensitive issue. We propose the Adversarial Score Distillation (ASD), which maintains an optimizable discriminator and updates it using the complete optimization objective. Experiments show that the proposed ASD performs favorably in 2D distillation and text-to-3D tasks against existing methods. Furthermore, to explore the generalization ability of our WGAN paradigm, we extend ASD to the image editing task, which achieves competitive results. The project page and code are at https://github.com/2y7c3/ASD.",
    "link": "https://arxiv.org/abs/2312.00739",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/2y7c3/ASD."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Make-A-Shape: a Ten-Million-scale 3D Shape Model",
    "summary": "arXiv:2401.11067v2 Announce Type: replace \nAbstract: Significant progress has been made in training large generative models for natural language and images. Yet, the advancement of 3D generative models is hindered by their substantial resource demands for training, along with inefficient, non-compact, and less expressive representations. This paper introduces Make-A-Shape, a new 3D generative model designed for efficient training on a vast scale, capable of utilizing 10 millions publicly-available shapes. Technical-wise, we first innovate a wavelet-tree representation to compactly encode shapes by formulating the subband coefficient filtering scheme to efficiently exploit coefficient relations. We then make the representation generatable by a diffusion model by devising the subband coefficients packing scheme to layout the representation in a low-resolution grid. Further, we derive the subband adaptive training strategy to train our model to effectively learn to generate coarse and detail wavelet coefficients. Last, we extend our framework to be controlled by additional input conditions to enable it to generate shapes from assorted modalities, e.g., single/multi-view images, point clouds, and low-resolution voxels. In our extensive set of experiments, we demonstrate various applications, such as unconditional generation, shape completion, and conditional generation on a wide range of modalities. Our approach not only surpasses the state of the art in delivering high-quality results but also efficiently generates shapes within a few seconds, often achieving this in just 2 seconds for most conditions. Our source code is available at https://github.com/AutodeskAILab/Make-a-Shape.",
    "link": "https://arxiv.org/abs/2401.11067",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/AutodeskAILab/Make-a-Shape."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Shedding More Light on Robust Classifiers under the lens of Energy-based Models",
    "summary": "arXiv:2407.06315v3 Announce Type: replace \nAbstract: By reinterpreting a robust discriminative classifier as Energy-based Model (EBM), we offer a new take on the dynamics of adversarial training (AT). Our analysis of the energy landscape during AT reveals that untargeted attacks generate adversarial images much more in-distribution (lower energy) than the original data from the point of view of the model. Conversely, we observe the opposite for targeted attacks. On the ground of our thorough analysis, we present new theoretical and practical results that show how interpreting AT energy dynamics unlocks a better understanding: (1) AT dynamic is governed by three phases and robust overfitting occurs in the third phase with a drastic divergence between natural and adversarial energies (2) by rewriting the loss of TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization (TRADES) in terms of energies, we show that TRADES implicitly alleviates overfitting by means of aligning the natural energy with the adversarial one (3) we empirically show that all recent state-of-the-art robust classifiers are smoothing the energy landscape and we reconcile a variety of studies about understanding AT and weighting the loss function under the umbrella of EBMs. Motivated by rigorous evidence, we propose Weighted Energy Adversarial Training (WEAT), a novel sample weighting scheme that yields robust accuracy matching the state-of-the-art on multiple benchmarks such as CIFAR-10 and SVHN and going beyond in CIFAR-100 and Tiny-ImageNet. We further show that robust classifiers vary in the intensity and quality of their generative capabilities, and offer a simple method to push this capability, reaching a remarkable Inception Score (IS) and FID using a robust classifier without training for generative modeling. The code to reproduce our results is available at http://github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/ .",
    "link": "https://arxiv.org/abs/2407.06315",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "http://github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "SUMix: Mixup with Semantic and Uncertain Information",
    "summary": "arXiv:2407.07805v4 Announce Type: replace \nAbstract: Mixup data augmentation approaches have been applied for various tasks of deep learning to improve the generalization ability of deep neural networks. Some existing approaches CutMix, SaliencyMix, etc. randomly replace a patch in one image with patches from another to generate the mixed image. Similarly, the corresponding labels are linearly combined by a fixed ratio $\\lambda$ by l. The objects in two images may be overlapped during the mixing process, so some semantic information is corrupted in the mixed samples. In this case, the mixed image does not match the mixed label information. Besides, such a label may mislead the deep learning model training, which results in poor performance. To solve this problem, we proposed a novel approach named SUMix to learn the mixing ratio as well as the uncertainty for the mixed samples during the training process. First, we design a learnable similarity function to compute an accurate mix ratio. Second, an approach is investigated as a regularized term to model the uncertainty of the mixed samples. We conduct experiments on five image benchmarks, and extensive experimental results imply that our method is capable of improving the performance of classifiers with different cutting-based mixup approaches. The source code is available at https://github.com/JinXins/SUMix.",
    "link": "https://arxiv.org/abs/2407.07805",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/JinXins/SUMix."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "TrackSSM: A General Motion Predictor by State-Space Model",
    "summary": "arXiv:2409.00487v2 Announce Type: replace \nAbstract: Temporal motion modeling has always been a key component in multiple object tracking (MOT) which can ensure smooth trajectory movement and provide accurate positional information to enhance association precision. However, current motion models struggle to be both efficient and effective across different application scenarios. To this end, we propose TrackSSM inspired by the recently popular state space models (SSM), a unified encoder-decoder motion framework that uses data-dependent state space model to perform temporal motion of trajectories. Specifically, we propose Flow-SSM, a module that utilizes the position and motion information from historical trajectories to guide the temporal state transition of object bounding boxes. Based on Flow-SSM, we design a flow decoder. It is composed of a cascaded motion decoding module employing Flow-SSM, which can use the encoded flow information to complete the temporal position prediction of trajectories. Additionally, we propose a Step-by-Step Linear (S$^2$L) training strategy. By performing linear interpolation between the positions of the object in the previous frame and the current frame, we construct the pseudo labels of step-by-step linear training, ensuring that the trajectory flow information can better guide the object bounding box in completing temporal transitions. TrackSSM utilizes a simple Mamba-Block to build a motion encoder for historical trajectories, forming a temporal motion model with an encoder-decoder structure in conjunction with the flow decoder. TrackSSM is applicable to various tracking scenarios and achieves excellent tracking performance across multiple benchmarks, further extending the potential of SSM-like temporal motion models in multi-object tracking tasks. Code and models are publicly available at \\url{https://github.com/Xavier-Lin/TrackSSM}.",
    "link": "https://arxiv.org/abs/2409.00487",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Xavier-Lin/TrackSSM}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "GET-UP: GEomeTric-aware Depth Estimation with Radar Points UPsampling",
    "summary": "arXiv:2409.02720v2 Announce Type: replace \nAbstract: Depth estimation plays a pivotal role in autonomous driving, facilitating a comprehensive understanding of the vehicle's 3D surroundings. Radar, with its robustness to adverse weather conditions and capability to measure distances, has drawn significant interest for radar-camera depth estimation. However, existing algorithms process the inherently noisy and sparse radar data by projecting 3D points onto the image plane for pixel-level feature extraction, overlooking the valuable geometric information contained within the radar point cloud. To address this gap, we propose GET-UP, leveraging attention-enhanced Graph Neural Networks (GNN) to exchange and aggregate both 2D and 3D information from radar data. This approach effectively enriches the feature representation by incorporating spatial relationships compared to traditional methods that rely only on 2D feature extraction. Furthermore, we incorporate a point cloud upsampling task to densify the radar point cloud, rectify point positions, and derive additional 3D features under the guidance of lidar data. Finally, we fuse radar and camera features during the decoding phase for depth estimation. We benchmark our proposed GET-UP on the nuScenes dataset, achieving state-of-the-art performance with a 15.3% and 14.7% improvement in MAE and RMSE over the previously best-performing model. Code: https://github.com/harborsarah/GET-UP",
    "link": "https://arxiv.org/abs/2409.02720",
    "published": "Wed, 11 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/harborsarah/GET-UP"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  }
]