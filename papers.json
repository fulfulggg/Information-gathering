[
  {
    "title": "ResEmoteNet: Bridging Accuracy and Loss Reduction in Facial Emotion Recognition",
    "summary": "arXiv:2409.10545v1 Announce Type: new \nAbstract: The human face is a silent communicator, expressing emotions and thoughts through its facial expressions. With the advancements in computer vision in recent years, facial emotion recognition technology has made significant strides, enabling machines to decode the intricacies of facial cues. In this work, we propose ResEmoteNet, a novel deep learning architecture for facial emotion recognition designed with the combination of Convolutional, Squeeze-Excitation (SE) and Residual Networks. The inclusion of SE block selectively focuses on the important features of the human face, enhances the feature representation and suppresses the less relevant ones. This helps in reducing the loss and enhancing the overall model performance. We also integrate the SE block with three residual blocks that help in learning more complex representation of the data through deeper layers. We evaluated ResEmoteNet on three open-source databases: FER2013, RAF-DB, and AffectNet, achieving accuracies of 79.79%, 94.76%, and 72.39%, respectively. The proposed network outperforms state-of-the-art models across all three databases. The source code for ResEmoteNet is available at https://github.com/ArnabKumarRoy02/ResEmoteNet.",
    "link": "https://arxiv.org/abs/2409.10545",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ArnabKumarRoy02/ResEmoteNet."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "SoccerNet 2024 Challenges Results",
    "summary": "arXiv:2409.10587v1 Announce Type: new \nAbstract: The SoccerNet 2024 challenges represent the fourth annual video understanding challenges organized by the SoccerNet team. These challenges aim to advance research across multiple themes in football, including broadcast video understanding, field understanding, and player understanding. This year, the challenges encompass four vision-based tasks. (1) Ball Action Spotting, focusing on precisely localizing when and which soccer actions related to the ball occur, (2) Dense Video Captioning, focusing on describing the broadcast with natural language and anchored timestamps, (3) Multi-View Foul Recognition, a novel task focusing on analyzing multiple viewpoints of a potential foul incident to classify whether a foul occurred and assess its severity, (4) Game State Reconstruction, another novel task focusing on reconstructing the game state from broadcast videos onto a 2D top-view map of the field. Detailed information about the tasks, challenges, and leaderboards can be found at https://www.soccer-net.org, with baselines and development kits available at https://github.com/SoccerNet.",
    "link": "https://arxiv.org/abs/2409.10587",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/SoccerNet."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph",
    "summary": "arXiv:2409.10921v1 Announce Type: new \nAbstract: Exploring the narratives conveyed by fine-art paintings is a challenge in image captioning, where the goal is to generate descriptions that not only precisely represent the visual content but also offer a in-depth interpretation of the artwork's meaning. The task is particularly complex for artwork images due to their diverse interpretations and varied aesthetic principles across different artistic schools and styles. In response to this, we present KALE Knowledge-Augmented vision-Language model for artwork Elaborations), a novel approach that enhances existing vision-language models by integrating artwork metadata as additional knowledge. KALE incorporates the metadata in two ways: firstly as direct textual input, and secondly through a multimodal heterogeneous knowledge graph. To optimize the learning of graph representations, we introduce a new cross-modal alignment loss that maximizes the similarity between the image and its corresponding metadata. Experimental results demonstrate that KALE achieves strong performance (when evaluated with CIDEr, in particular) over existing state-of-the-art work across several artwork datasets. Source code of the project is available at https://github.com/Yanbei-Jiang/Artwork-Interpretation.",
    "link": "https://arxiv.org/abs/2409.10921",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Yanbei-Jiang/Artwork-Interpretation."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Versatile Incremental Learning: Towards Class and Domain-Agnostic Incremental Learning",
    "summary": "arXiv:2409.10956v1 Announce Type: new \nAbstract: Incremental Learning (IL) aims to accumulate knowledge from sequential input tasks while overcoming catastrophic forgetting. Existing IL methods typically assume that an incoming task has only increments of classes or domains, referred to as Class IL (CIL) or Domain IL (DIL), respectively. In this work, we consider a more challenging and realistic but under-explored IL scenario, named Versatile Incremental Learning (VIL), in which a model has no prior of which of the classes or domains will increase in the next task. In the proposed VIL scenario, the model faces intra-class domain confusion and inter-domain class confusion, which makes the model fail to accumulate new knowledge without interference with learned knowledge. To address these issues, we propose a simple yet effective IL framework, named Incremental Classifier with Adaptation Shift cONtrol (ICON). Based on shifts of learnable modules, we design a novel regularization method called Cluster-based Adaptation Shift conTrol (CAST) to control the model to avoid confusion with the previously learned knowledge and thereby accumulate the new knowledge more effectively. Moreover, we introduce an Incremental Classifier (IC) which expands its output nodes to address the overwriting issue from different domains corresponding to a single class while maintaining the previous knowledge. We conducted extensive experiments on three benchmarks, showcasing the effectiveness of our method across all the scenarios, particularly in cases where the next task can be randomly altered. Our implementation code is available at https://github.com/KHU-AGI/VIL.",
    "link": "https://arxiv.org/abs/2409.10956",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/KHU-AGI/VIL."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "MM2Latent: Text-to-facial image generation and editing in GANs with multimodal assistance",
    "summary": "arXiv:2409.11010v1 Announce Type: new \nAbstract: Generating human portraits is a hot topic in the image generation area, e.g. mask-to-face generation and text-to-face generation. However, these unimodal generation methods lack controllability in image generation. Controllability can be enhanced by exploring the advantages and complementarities of various modalities. For instance, we can utilize the advantages of text in controlling diverse attributes and masks in controlling spatial locations. Current state-of-the-art methods in multimodal generation face limitations due to their reliance on extensive hyperparameters, manual operations during the inference stage, substantial computational demands during training and inference, or inability to edit real images. In this paper, we propose a practical framework - MM2Latent - for multimodal image generation and editing. We use StyleGAN2 as our image generator, FaRL for text encoding, and train an autoencoders for spatial modalities like mask, sketch and 3DMM. We propose a strategy that involves training a mapping network to map the multimodal input into the w latent space of StyleGAN. The proposed framework 1) eliminates hyperparameters and manual operations in the inference stage, 2) ensures fast inference speeds, and 3) enables the editing of real images. Extensive experiments demonstrate that our method exhibits superior performance in multimodal image generation, surpassing recent GAN- and diffusion-based methods. Also, it proves effective in multimodal image editing and is faster than GAN- and diffusion-based methods. We make the code publicly available at: https://github.com/Open-Debin/MM2Latent",
    "link": "https://arxiv.org/abs/2409.11010",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Open-Debin/MM2Latent"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Quantitative Evaluation of MILs' Reliability For WSIs Classification",
    "summary": "arXiv:2409.11110v1 Announce Type: new \nAbstract: Reliable models are dependable and provide predictions acceptable given basic domain knowledge. Therefore, it is critical to develop and deploy reliable models, especially for healthcare applications. However, Multiple Instance Learning (MIL) models designed for Whole Slide Images (WSIs) classification in computational pathology are not evaluated in terms of reliability. Hence, in this paper we compare the reliability of MIL models with three suggested metrics and use three region-wise annotated datasets. We find the mean pooling instance (MEAN-POOL-INS) model more reliable than other networks despite its naive architecture design and computation efficiency. The code to reproduce the results is accessible at https://github.com/tueimage/MILs'R .",
    "link": "https://arxiv.org/abs/2409.11110",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/tueimage/MILs'R"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "HS3-Bench: A Benchmark and Strong Baseline for Hyperspectral Semantic Segmentation in Driving Scenarios",
    "summary": "arXiv:2409.11205v1 Announce Type: new \nAbstract: Semantic segmentation is an essential step for many vision applications in order to understand a scene and the objects within. Recent progress in hyperspectral imaging technology enables the application in driving scenarios and the hope is that the devices perceptive abilities provide an advantage over RGB-cameras. Even though some datasets exist, there is no standard benchmark available to systematically measure progress on this task and evaluate the benefit of hyperspectral data. In this paper, we work towards closing this gap by providing the HyperSpectral Semantic Segmentation benchmark (HS3-Bench). It combines annotated hyperspectral images from three driving scenario datasets and provides standardized metrics, implementations, and evaluation protocols. We use the benchmark to derive two strong baseline models that surpass the previous state-of-the-art performances with and without pre-training on the individual datasets. Further, our results indicate that the existing learning-based methods benefit more from leveraging additional RGB training data than from leveraging the additional hyperspectral channels. This poses important questions for future research on hyperspectral imaging for semantic segmentation in driving scenarios. Code to run the benchmark and the strong baseline approaches are available under https://github.com/nickstheisen/hyperseg.",
    "link": "https://arxiv.org/abs/2409.11205",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/nickstheisen/hyperseg."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "High-Order Evolving Graphs for Enhanced Representation of Traffic Dynamics",
    "summary": "arXiv:2409.11206v1 Announce Type: new \nAbstract: We present an innovative framework for traffic dynamics analysis using High-Order Evolving Graphs, designed to improve spatio-temporal representations in autonomous driving contexts. Our approach constructs temporal bidirectional bipartite graphs that effectively model the complex interactions within traffic scenes in real-time. By integrating Graph Neural Networks (GNNs) with high-order multi-aggregation strategies, we significantly enhance the modeling of traffic scene dynamics, providing a more accurate and detailed analysis of these interactions. Additionally, we incorporate inductive learning techniques inspired by the GraphSAGE framework, enabling our model to adapt to new and unseen traffic scenarios without the need for retraining, thus ensuring robust generalization. Through extensive experiments on the ROAD and ROAD Waymo datasets, we establish a comprehensive baseline for further developments, demonstrating the potential of our method in accurately capturing traffic behavior. Our results emphasize the value of high-order statistical moments and feature-gated attention mechanisms in improving traffic behavior analysis, laying the groundwork for advancing autonomous driving technologies. Our source code is available at: https://github.com/Addy-1998/High\\_Order\\_Graphs",
    "link": "https://arxiv.org/abs/2409.11206",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Addy-1998/High\\_Order\\_Graphs"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "STCMOT: Spatio-Temporal Cohesion Learning for UAV-Based Multiple Object Tracking",
    "summary": "arXiv:2409.11234v1 Announce Type: new \nAbstract: Multiple object tracking (MOT) in Unmanned Aerial Vehicle (UAV) videos is important for diverse applications in computer vision. Current MOT trackers rely on accurate object detection results and precise matching of target reidentification (ReID). These methods focus on optimizing target spatial attributes while overlooking temporal cues in modelling object relationships, especially for challenging tracking conditions such as object deformation and blurring, etc. To address the above-mentioned issues, we propose a novel Spatio-Temporal Cohesion Multiple Object Tracking framework (STCMOT), which utilizes historical embedding features to model the representation of ReID and detection features in a sequential order. Concretely, a temporal embedding boosting module is introduced to enhance the discriminability of individual embedding based on adjacent frame cooperation. While the trajectory embedding is then propagated by a temporal detection refinement module to mine salient target locations in the temporal field. Extensive experiments on the VisDrone2019 and UAVDT datasets demonstrate our STCMOT sets a new state-of-the-art performance in MOTA and IDF1 metrics. The source codes are released at https://github.com/ydhcg-BoBo/STCMOT.",
    "link": "https://arxiv.org/abs/2409.11234",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ydhcg-BoBo/STCMOT."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking",
    "summary": "arXiv:2409.11235v1 Announce Type: new \nAbstract: Open-vocabulary Multiple Object Tracking (MOT) aims to generalize trackers to novel categories not in the training set. Currently, the best-performing methods are mainly based on pure appearance matching. Due to the complexity of motion patterns in the large-vocabulary scenarios and unstable classification of the novel objects, the motion and semantics cues are either ignored or applied based on heuristics in the final matching steps by existing methods. In this paper, we present a unified framework SLAck that jointly considers semantics, location, and appearance priors in the early steps of association and learns how to integrate all valuable information through a lightweight spatial and temporal object graph. Our method eliminates complex post-processing heuristics for fusing different cues and boosts the association performance significantly for large-scale open-vocabulary tracking. Without bells and whistles, we outperform previous state-of-the-art methods for novel classes tracking on the open-vocabulary MOT and TAO TETA benchmarks. Our code is available at \\href{https://github.com/siyuanliii/SLAck}{github.com/siyuanliii/SLAck}.",
    "link": "https://arxiv.org/abs/2409.11235",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/siyuanliii/SLAck}{github.com/siyuanliii/SLAck}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction",
    "summary": "arXiv:2409.11315v1 Announce Type: new \nAbstract: Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI) data, introduced as Recon3DMind in our conference work, is of significant interest to both cognitive neuroscience and computer vision. To advance this task, we present the fMRI-3D dataset, which includes data from 15 participants and showcases a total of 4768 3D objects. The dataset comprises two components: fMRI-Shape, previously introduced and accessible at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape, and fMRI-Objaverse, proposed in this paper and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse includes data from 5 subjects, 4 of whom are also part of the Core set in fMRI-Shape, with each subject viewing 3142 3D objects across 117 categories, all accompanied by text captions. This significantly enhances the diversity and potential applications of the dataset. Additionally, we propose MinD-3D, a novel framework designed to decode 3D visual information from fMRI signals. The framework first extracts and aggregates features from fMRI data using a neuro-fusion encoder, then employs a feature-bridge diffusion model to generate visual features, and finally reconstructs the 3D object using a generative transformer decoder. We establish new benchmarks by designing metrics at both semantic and structural levels to evaluate model performance. Furthermore, we assess our model's effectiveness in an Out-of-Distribution setting and analyze the attribution of the extracted features and the visual ROIs in fMRI signals. Our experiments demonstrate that MinD-3D not only reconstructs 3D objects with high semantic and spatial accuracy but also deepens our understanding of how human brain processes 3D visual information. Project page at: https://jianxgao.github.io/MinD-3D.",
    "link": "https://arxiv.org/abs/2409.11315",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [],
    "huggingface_urls": [
      "https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape,",
      "https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse."
    ],
    "source": "arXiv"
  },
  {
    "title": "MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping",
    "summary": "arXiv:2409.11316v1 Announce Type: new \nAbstract: Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi-scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve state-of-the-art results on benchmark datasets such as $PASCAL-5^i$ and $COCO-20^i$ in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. https://github.com/amirrezafateh/MSDNet",
    "link": "https://arxiv.org/abs/2409.11316",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/amirrezafateh/MSDNet"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "OmniGen: Unified Image Generation",
    "summary": "arXiv:2409.11340v1 Announce Type: new \nAbstract: In this work, we introduce OmniGen, a new diffusion model for unified image generation. Unlike popular diffusion models (e.g., Stable Diffusion), OmniGen no longer requires additional modules such as ControlNet or IP-Adapter to process diverse control conditions. OmniGenis characterized by the following features: 1) Unification: OmniGen not only demonstrates text-to-image generation capabilities but also inherently supports other downstream tasks, such as image editing, subject-driven generation, and visual-conditional generation. Additionally, OmniGen can handle classical computer vision tasks by transforming them into image generation tasks, such as edge detection and human pose recognition. 2) Simplicity: The architecture of OmniGen is highly simplified, eliminating the need for additional text encoders. Moreover, it is more user-friendly compared to existing diffusion models, enabling complex tasks to be accomplished through instructions without the need for extra preprocessing steps (e.g., human pose estimation), thereby significantly simplifying the workflow of image generation. 3) Knowledge Transfer: Through learning in a unified format, OmniGen effectively transfers knowledge across different tasks, manages unseen tasks and domains, and exhibits novel capabilities. We also explore the model's reasoning capabilities and potential applications of chain-of-thought mechanism. This work represents the first attempt at a general-purpose image generation model, and there remain several unresolved issues. We will open-source the related resources at https://github.com/VectorSpaceLab/OmniGen to foster advancements in this field.",
    "link": "https://arxiv.org/abs/2409.11340",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/VectorSpaceLab/OmniGen"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Ultrasound Image Enhancement with the Variance of Diffusion Models",
    "summary": "arXiv:2409.11380v1 Announce Type: new \nAbstract: Ultrasound imaging, despite its widespread use in medicine, often suffers from various sources of noise and artifacts that impact the signal-to-noise ratio and overall image quality. Enhancing ultrasound images requires a delicate balance between contrast, resolution, and speckle preservation. This paper introduces a novel approach that integrates adaptive beamforming with denoising diffusion-based variance imaging to address this challenge. By applying Eigenspace-Based Minimum Variance (EBMV) beamforming and employing a denoising diffusion model fine-tuned on ultrasound data, our method computes the variance across multiple diffusion-denoised samples to produce high-quality despeckled images. This approach leverages both the inherent multiplicative noise of ultrasound and the stochastic nature of diffusion models. Experimental results on a publicly available dataset demonstrate the effectiveness of our method in achieving superior image reconstructions from single plane-wave acquisitions. The code is available at: https://github.com/Yuxin-Zhang-Jasmine/IUS2024_Diffusion.",
    "link": "https://arxiv.org/abs/2409.11380",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Yuxin-Zhang-Jasmine/IUS2024_Diffusion."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "WaveMixSR-V2: Enhancing Super-resolution with Higher Efficiency",
    "summary": "arXiv:2409.10582v1 Announce Type: cross \nAbstract: Recent advancements in single image super-resolution have been predominantly driven by token mixers and transformer architectures. WaveMixSR utilized the WaveMix architecture, employing a two-dimensional discrete wavelet transform for spatial token mixing, achieving superior performance in super-resolution tasks with remarkable resource efficiency. In this work, we present an enhanced version of the WaveMixSR architecture by (1) replacing the traditional transpose convolution layer with a pixel shuffle operation and (2) implementing a multistage design for higher resolution tasks ($4\\times$). Our experiments demonstrate that our enhanced model -- WaveMixSR-V2 -- outperforms other architectures in multiple super-resolution tasks, achieving state-of-the-art for the BSD100 dataset, while also consuming fewer resources, exhibits higher parameter efficiency, lower latency and higher throughput. Our code is available at https://github.com/pranavphoenix/WaveMixSR.",
    "link": "https://arxiv.org/abs/2409.10582",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/pranavphoenix/WaveMixSR."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation",
    "summary": "arXiv:2409.10847v1 Announce Type: cross \nAbstract: Autoregressive models excel in modeling sequential dependencies by enforcing causal constraints, yet they struggle to capture complex bidirectional patterns due to their unidirectional nature. In contrast, mask-based models leverage bidirectional context, enabling richer dependency modeling. However, they often assume token independence during prediction, which undermines the modeling of sequential dependencies. Additionally, the corruption of sequences through masking or absorption can introduce unnatural distortions, complicating the learning process. To address these issues, we propose Bidirectional Autoregressive Diffusion (BAD), a novel approach that unifies the strengths of autoregressive and mask-based generative models. BAD utilizes a permutation-based corruption technique that preserves the natural sequence structure while enforcing causal dependencies through randomized ordering, enabling the effective capture of both sequential and bidirectional relationships. Comprehensive experiments show that BAD outperforms autoregressive and mask-based models in text-to-motion generation, suggesting a novel pre-training strategy for sequence modeling. The codebase for BAD is available on https://github.com/RohollahHS/BAD.",
    "link": "https://arxiv.org/abs/2409.10847",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/RohollahHS/BAD."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Neural Fields for Adaptive Photoacoustic Computed Tomography",
    "summary": "arXiv:2409.10876v1 Announce Type: cross \nAbstract: Photoacoustic computed tomography (PACT) is a non-invasive imaging modality with wide medical applications. Conventional PACT image reconstruction algorithms suffer from wavefront distortion caused by the heterogeneous speed of sound (SOS) in tissue, which leads to image degradation. Accounting for these effects improves image quality, but measuring the SOS distribution is experimentally expensive. An alternative approach is to perform joint reconstruction of the initial pressure image and SOS using only the PA signals. Existing joint reconstruction methods come with limitations: high computational cost, inability to directly recover SOS, and reliance on inaccurate simplifying assumptions. Implicit neural representation, or neural fields, is an emerging technique in computer vision to learn an efficient and continuous representation of physical fields with a coordinate-based neural network. In this work, we introduce NF-APACT, an efficient self-supervised framework utilizing neural fields to estimate the SOS in service of an accurate and robust multi-channel deconvolution. Our method removes SOS aberrations an order of magnitude faster and more accurately than existing methods. We demonstrate the success of our method on a novel numerical phantom as well as an experimentally collected phantom and in vivo data. Our code and numerical phantom are available at https://github.com/Lukeli0425/NF-APACT.",
    "link": "https://arxiv.org/abs/2409.10876",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Lukeli0425/NF-APACT."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "SkinMamba: A Precision Skin Lesion Segmentation Architecture with Cross-Scale Global State Modeling and Frequency Boundary Guidance",
    "summary": "arXiv:2409.10890v1 Announce Type: cross \nAbstract: Skin lesion segmentation is a crucial method for identifying early skin cancer. In recent years, both convolutional neural network (CNN) and Transformer-based methods have been widely applied. Moreover, combining CNN and Transformer effectively integrates global and local relationships, but remains limited by the quadratic complexity of Transformer. To address this, we propose a hybrid architecture based on Mamba and CNN, called SkinMamba. It maintains linear complexity while offering powerful long-range dependency modeling and local feature extraction capabilities. Specifically, we introduce the Scale Residual State Space Block (SRSSB), which captures global contextual relationships and cross-scale information exchange at a macro level, enabling expert communication in a global state. This effectively addresses challenges in skin lesion segmentation related to varying lesion sizes and inconspicuous target areas. Additionally, to mitigate boundary blurring and information loss during model downsampling, we introduce the Frequency Boundary Guided Module (FBGM), providing sufficient boundary priors to guide precise boundary segmentation, while also using the retained information to assist the decoder in the decoding process. Finally, we conducted comparative and ablation experiments on two public lesion segmentation datasets (ISIC2017 and ISIC2018), and the results demonstrate the strong competitiveness of SkinMamba in skin lesion segmentation tasks. The code is available at https://github.com/zs1314/SkinMamba.",
    "link": "https://arxiv.org/abs/2409.10890",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/zs1314/SkinMamba."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "CUNSB-RFIE: Context-aware Unpaired Neural Schr\"{o}dinger Bridge in Retinal Fundus Image Enhancement",
    "summary": "arXiv:2409.10966v1 Announce Type: cross \nAbstract: Retinal fundus photography is significant in diagnosing and monitoring retinal diseases. However, systemic imperfections and operator/patient-related factors can hinder the acquisition of high-quality retinal images. Previous efforts in retinal image enhancement primarily relied on GANs, which are limited by the trade-off between training stability and output diversity. In contrast, the Schr\\\"{o}dinger Bridge (SB), offers a more stable solution by utilizing Optimal Transport (OT) theory to model a stochastic differential equation (SDE) between two arbitrary distributions. This allows SB to effectively transform low-quality retinal images into their high-quality counterparts. In this work, we leverage the SB framework to propose an image-to-image translation pipeline for retinal image enhancement. Additionally, previous methods often fail to capture fine structural details, such as blood vessels. To address this, we enhance our pipeline by introducing Dynamic Snake Convolution, whose tortuous receptive field can better preserve tubular structures. We name the resulting retinal fundus image enhancement framework the Context-aware Unpaired Neural Schr\\\"{o}dinger Bridge (CUNSB-RFIE). To the best of our knowledge, this is the first endeavor to use the SB approach for retinal image enhancement. Experimental results on a large-scale dataset demonstrate the advantage of the proposed method compared to several state-of-the-art supervised and unsupervised methods in terms of image quality and performance on downstream tasks.The code is available at \\url{https://github.com/Retinal-Research/CUNSB-RFIE}.",
    "link": "https://arxiv.org/abs/2409.10966",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Retinal-Research/CUNSB-RFIE}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Annealed Winner-Takes-All for Motion Forecasting",
    "summary": "arXiv:2409.11172v1 Announce Type: cross \nAbstract: In autonomous driving, motion prediction aims at forecasting the future trajectories of nearby agents, helping the ego vehicle to anticipate behaviors and drive safely. A key challenge is generating a diverse set of future predictions, commonly addressed using data-driven models with Multiple Choice Learning (MCL) architectures and Winner-Takes-All (WTA) training objectives. However, these methods face initialization sensitivity and training instabilities. Additionally, to compensate for limited performance, some approaches rely on training with a large set of hypotheses, requiring a post-selection step during inference to significantly reduce the number of predictions. To tackle these issues, we take inspiration from annealed MCL, a recently introduced technique that improves the convergence properties of MCL methods through an annealed Winner-Takes-All loss (aWTA). In this paper, we demonstrate how the aWTA loss can be integrated with state-of-the-art motion forecasting models to enhance their performance using only a minimal set of hypotheses, eliminating the need for the cumbersome post-selection step. Our approach can be easily incorporated into any trajectory prediction model normally trained using WTA and yields significant improvements. To facilitate the application of our approach to future motion forecasting models, the code will be made publicly available upon acceptance: https://github.com/valeoai/MF_aWTA.",
    "link": "https://arxiv.org/abs/2409.11172",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/valeoai/MF_aWTA."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical image segmentation",
    "summary": "arXiv:2409.11299v1 Announce Type: cross \nAbstract: Biomedical image segmentation is crucial for accurately diagnosing and analyzing various diseases. However, Convolutional Neural Networks (CNNs) and Transformers, the most commonly used architectures for this task, struggle to effectively capture long-range dependencies due to the inherent locality of CNNs and the computational complexity of Transformers. To address this limitation, we introduce TTT-Unet, a novel framework that integrates Test-Time Training (TTT) layers into the traditional U-Net architecture for biomedical image segmentation. TTT-Unet dynamically adjusts model parameters during the testing time, enhancing the model's ability to capture both local and long-range features. We evaluate TTT-Unet on multiple medical imaging datasets, including 3D abdominal organ segmentation in CT and MR images, instrument segmentation in endoscopy images, and cell segmentation in microscopy images. The results demonstrate that TTT-Unet consistently outperforms state-of-the-art CNN-based and Transformer-based segmentation models across all tasks. The code is available at https://github.com/rongzhou7/TTT-Unet.",
    "link": "https://arxiv.org/abs/2409.11299",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/rongzhou7/TTT-Unet."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models",
    "summary": "arXiv:2308.16463v3 Announce Type: replace \nAbstract: Large language models exhibit enhanced zero-shot performance on various tasks when fine-tuned with instruction-following data. Multimodal instruction-following models extend these capabilities by integrating both text and images. However, existing models such as MiniGPT-4 and LLaVA face challenges in maintaining dialogue coherence in scenarios involving multiple images. A primary reason is the lack of a specialized dataset for this critical application. To bridge these gaps, we introduce SparklesDialogue, the first machine-generated dialogue dataset tailored for word-level interleaved multi-image and text interactions. Furthermore, we construct SparklesEval, a GPT-assisted benchmark for quantitatively assessing a model's conversational competence across multiple images and dialogue turns. We then present SparklesChat, a multimodal instruction-following model for open-ended dialogues across multiple images. Our experiments validate the effectiveness of training SparklesChat with SparklesDialogue based on MiniGPT-4 and LLaVA-v1.5, which enhances comprehension across multiple images and dialogue turns, and does not compromise single-image understanding capabilities. Qualitative evaluations further demonstrate SparklesChat's generality in handling real-world applications. All resources related to this study are publicly available at https://github.com/HYPJUDY/Sparkles.",
    "link": "https://arxiv.org/abs/2308.16463",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/HYPJUDY/Sparkles."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "GenQ: Quantization in Low Data Regimes with Generative Synthetic Data",
    "summary": "arXiv:2312.05272v3 Announce Type: replace \nAbstract: In the realm of deep neural network deployment, low-bit quantization presents a promising avenue for enhancing computational efficiency. However, it often hinges on the availability of training data to mitigate quantization errors, a significant challenge when data availability is scarce or restricted due to privacy or copyright concerns. Addressing this, we introduce GenQ, a novel approach employing an advanced Generative AI model to generate photorealistic, high-resolution synthetic data, overcoming the limitations of traditional methods that struggle to accurately mimic complex objects in extensive datasets like ImageNet. Our methodology is underscored by two robust filtering mechanisms designed to ensure the synthetic data closely aligns with the intrinsic characteristics of the actual training data. In case of limited data availability, the actual data is used to guide the synthetic data generation process, enhancing fidelity through the inversion of learnable token embeddings. Through rigorous experimentation, GenQ establishes new benchmarks in data-free and data-scarce quantization, significantly outperforming existing methods in accuracy and efficiency, thereby setting a new standard for quantization in low data regimes. Code is released at \\url{https://github.com/Intelligent-Computing-Lab-Yale/GenQ}.",
    "link": "https://arxiv.org/abs/2312.05272",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Intelligent-Computing-Lab-Yale/GenQ}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "AgileFormer: Spatially Agile Transformer UNet for Medical Image Segmentation",
    "summary": "arXiv:2404.00122v2 Announce Type: replace \nAbstract: In the past decades, deep neural networks, particularly convolutional neural networks, have achieved state-of-the-art performance in a variety of medical image segmentation tasks. Recently, the introduction of the vision transformer (ViT) has significantly altered the landscape of deep segmentation models. There has been a growing focus on ViTs, driven by their excellent performance and scalability. However, we argue that the current design of the vision transformer-based UNet (ViT-UNet) segmentation models may not effectively handle the heterogeneous appearance (e.g., varying shapes and sizes) of objects of interest in medical image segmentation tasks. To tackle this challenge, we present a structured approach to introduce spatially dynamic components to the ViT-UNet. This adaptation enables the model to effectively capture features of target objects with diverse appearances. This is achieved by three main components: \\textbf{(i)} deformable patch embedding; \\textbf{(ii)} spatially dynamic multi-head attention; \\textbf{(iii)} deformable positional encoding. These components were integrated into a novel architecture, termed AgileFormer. AgileFormer is a spatially agile ViT-UNet designed for medical image segmentation. Experiments in three segmentation tasks using publicly available datasets demonstrated the effectiveness of the proposed method. The code is available at \\href{https://github.com/sotiraslab/AgileFormer}{https://github.com/sotiraslab/AgileFormer}.",
    "link": "https://arxiv.org/abs/2404.00122",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/sotiraslab/AgileFormer}{https://github.com/sotiraslab/AgileFormer}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Transferable and Principled Efficiency for Open-Vocabulary Segmentation",
    "summary": "arXiv:2404.07448v3 Announce Type: replace \nAbstract: Recent success of pre-trained foundation vision-language models makes Open-Vocabulary Segmentation (OVS) possible. Despite the promising performance, this approach introduces heavy computational overheads for two challenges: 1) large model sizes of the backbone; 2) expensive costs during the fine-tuning. These challenges hinder this OVS strategy from being widely applicable and affordable in real-world scenarios. Although traditional methods such as model compression and efficient fine-tuning can address these challenges, they often rely on heuristics. This means that their solutions cannot be easily transferred and necessitate re-training on different models, which comes at a cost. In the context of efficient OVS, we target achieving performance that is comparable to or even better than prior OVS works based on large vision-language foundation models, by utilizing smaller models that incur lower training costs. The core strategy is to make our efficiency principled and thus seamlessly transferable from one OVS framework to others without further customization. Comprehensive experiments on diverse OVS benchmarks demonstrate our superior trade-off between segmentation accuracy and computation costs over previous works. Our code is available on https://github.com/Xujxyang/OpenTrans",
    "link": "https://arxiv.org/abs/2404.07448",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Xujxyang/OpenTrans"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation",
    "summary": "arXiv:2404.08181v2 Announce Type: replace \nAbstract: Despite the significant progress in deep learning for dense visual recognition problems, such as semantic segmentation, traditional methods are constrained by fixed class sets. Meanwhile, vision-language foundation models, such as CLIP, have showcased remarkable effectiveness in numerous zero-shot image-level tasks, owing to their robust generalizability. Recently, a body of work has investigated utilizing these models in open-vocabulary semantic segmentation (OVSS). However, existing approaches often rely on impractical supervised pre-training or access to additional pre-trained networks. In this work, we propose a strong baseline for training-free OVSS, termed Neighbour-Aware CLIP (NACLIP), representing a straightforward adaptation of CLIP tailored for this scenario. Our method enforces localization of patches in the self-attention of CLIP's vision transformer which, despite being crucial for dense prediction tasks, has been overlooked in the OVSS literature. By incorporating design choices favouring segmentation, our approach significantly improves performance without requiring additional data, auxiliary pre-trained networks, or extensive hyperparameter tuning, making it highly practical for real-world applications. Experiments are performed on 8 popular semantic segmentation benchmarks, yielding state-of-the-art performance on most scenarios. Our code is publicly available at https://github.com/sinahmr/NACLIP.",
    "link": "https://arxiv.org/abs/2404.08181",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/sinahmr/NACLIP."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Transferable-guided Attention Is All You Need for Video Domain Adaptation",
    "summary": "arXiv:2407.01375v2 Announce Type: replace \nAbstract: Unsupervised domain adaptation (UDA) in videos is a challenging task that remains not well explored compared to image-based UDA techniques. Although vision transformers (ViT) achieve state-of-the-art performance in many computer vision tasks, their use in video UDA has been little explored. Our key idea is to use transformer layers as a feature encoder and incorporate spatial and temporal transferability relationships into the attention mechanism. A Transferable-guided Attention (TransferAttn) framework is then developed to exploit the capacity of the transformer to adapt cross-domain knowledge across different backbones. To improve the transferability of ViT, we introduce a novel and effective module, named Domain Transferable-guided Attention Block (DTAB). DTAB compels ViT to focus on the spatio-temporal transferability relationship among video frames by changing the self-attention mechanism to a transferability attention mechanism. Extensive experiments were conducted on UCF-HMDB, Kinetics-Gameplay, and Kinetics-NEC Drone datasets, with different backbones, like ResNet101, I3D, and STAM, to verify the effectiveness of TransferAttn compared with state-of-the-art approaches. Also, we demonstrate that DTAB yields performance gains when applied to other state-of-the-art transformer-based UDA methods from both video and image domains. Our code is available at https://github.com/Andre-Sacilotti/transferattn-project-code.",
    "link": "https://arxiv.org/abs/2407.01375",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Andre-Sacilotti/transferattn-project-code."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "SeFlow: A Self-Supervised Scene Flow Method in Autonomous Driving",
    "summary": "arXiv:2407.01702v2 Announce Type: replace \nAbstract: Scene flow estimation predicts the 3D motion at each point in successive LiDAR scans. This detailed, point-level, information can help autonomous vehicles to accurately predict and understand dynamic changes in their surroundings. Current state-of-the-art methods require annotated data to train scene flow networks and the expense of labeling inherently limits their scalability. Self-supervised approaches can overcome the above limitations, yet face two principal challenges that hinder optimal performance: point distribution imbalance and disregard for object-level motion constraints. In this paper, we propose SeFlow, a self-supervised method that integrates efficient dynamic classification into a learning-based scene flow pipeline. We demonstrate that classifying static and dynamic points helps design targeted objective functions for different motion patterns. We also emphasize the importance of internal cluster consistency and correct object point association to refine the scene flow estimation, in particular on object details. Our real-time capable method achieves state-of-the-art performance on the self-supervised scene flow task on Argoverse 2 and Waymo datasets. The code is open-sourced at https://github.com/KTH-RPL/SeFlow along with trained model weights.",
    "link": "https://arxiv.org/abs/2407.01702",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/KTH-RPL/SeFlow"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Vision Mamba for Classification of Breast Ultrasound Images",
    "summary": "arXiv:2407.03552v2 Announce Type: replace \nAbstract: Mamba-based models, VMamba and Vim, are a recent family of vision encoders that offer promising performance improvements in many computer vision tasks. This paper compares Mamba-based models with traditional Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) using the breast ultrasound BUSI dataset and Breast Ultrasound B dataset. Our evaluation, which includes multiple runs of experiments and statistical significance analysis, demonstrates that some of the Mamba-based architectures often outperform CNN and ViT models with statistically significant results. For example, in the B dataset, the best Mamba-based models have a 1.98\\% average AUC and a 5.0\\% average Accuracy improvement compared to the best non-Mamba-based model in this study. These Mamba-based models effectively capture long-range dependencies while maintaining some inductive biases, making them suitable for applications with limited data. The code is available at \\url{https://github.com/anasiri/BU-Mamba}",
    "link": "https://arxiv.org/abs/2407.03552",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/anasiri/BU-Mamba}"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Fine-Grained Representation Learning via Multi-Level Contrastive Learning without Class Priors",
    "summary": "arXiv:2409.04867v2 Announce Type: replace \nAbstract: Recent advances in unsupervised representation learning frequently leverage class information to improve the extraction and clustering of features. However, this dependence on class priors limits the applicability of such methods in real-world scenarios where class information is unavailable or ambiguous. In this paper, we propose \\textit{Contrastive Disentangling (CD)}, a simple yet effective framework that learns representations without any relying on class priors. CD employs a multi-level contrastive learning strategy, integrating instance-level and feature-level losses with a normalized entropy loss to learn semantically rich and fine-grained representations. Specifically, (1) the instance-level contrastive loss encourages the separation of feature representations between different samples; (2) the feature-level contrastive loss promotes independence among feature prediction heads; and (3) the normalized entropy loss ensures that the feature heads capture meaningful and prevalent attributes from the data. These components together enable CD to outperform existing methods in scenarios lacking class priors, as demonstrated by extensive experiments on benchmark datasets including CIFAR-10, CIFAR-100, STL-10, and ImageNet-10. The code is available at https://github.com/Hoper-J/Contrastive-Disentangling.",
    "link": "https://arxiv.org/abs/2409.04867",
    "published": "Wed, 18 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Hoper-J/Contrastive-Disentangling."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  }
]