[
  {
    "title": "Low Latency Point Cloud Rendering with Learned Splatting",
    "summary": "arXiv:2409.16504v1 Announce Type: new \nAbstract: Point cloud is a critical 3D representation with many emerging applications. Because of the point sparsity and irregularity, high-quality rendering of point clouds is challenging and often requires complex computations to recover the continuous surface representation. On the other hand, to avoid visual discomfort, the motion-to-photon latency has to be very short, under 10 ms. Existing rendering solutions lack in either quality or speed. To tackle these challenges, we present a framework that unlocks interactive, free-viewing and high-fidelity point cloud rendering. We train a generic neural network to estimate 3D elliptical Gaussians from arbitrary point clouds and use differentiable surface splatting to render smooth texture and surface normal for arbitrary views. Our approach does not require per-scene optimization, and enable real-time rendering of dynamic point cloud. Experimental results demonstrate the proposed solution enjoys superior visual quality and speed, as well as generalizability to different scene content and robustness to compression artifacts. The code is available at https://github.com/huzi96/gaussian-pcloud-render .",
    "link": "https://arxiv.org/abs/2409.16504",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/huzi96/gaussian-pcloud-render"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "EventHallusion: Diagnosing Event Hallucinations in Video LLMs",
    "summary": "arXiv:2409.16597v1 Announce Type: new \nAbstract: Recently, Multimodal Large Language Models (MLLMs) have made significant progress in the video comprehension field. Despite remarkable content reasoning and instruction following capabilities they demonstrated, the hallucination problem of these VideoLLMs is less explored compared with its counterpart in the image domain. To mitigate this gap, we first propose EventHallusion, a novel benchmark that focuses on assessing the VideoLMMs' hallucination phenomenon on video event comprehension. Based on the observation that existing VideoLLMs are entangled with the priors stemming from their foundation models, our EventHallusion is curated by meticulously collecting videos and annotating questions to intentionally mislead the VideoLLMs into interpreting events based on these priors rather than accurately understanding the video content. On the other hand, we also propose a simple yet effective method, called Temporal Contrastive Decoding (TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD suppresses the model's preference toward their priors by comparing the original video with a constructed counterpart, whose temporal cues are disrupted, during the autoregressive decoding stage. Through comprehensive evaluation of eight open-source and two closed-source VideoLLMs on the proposed EventHallusion benchmark, we find that the open-source models suffer significantly from hallucination problems, whereas the closed-source models perform markedly better. By further equipping open-sourced VideoLLMs with the proposed TCD approach, evident performance improvements are achieved across most metrics in the EventHallusion benchmark. Our codes and benchmark data are available at https://github.com/Stevetich/EventHallusion.",
    "link": "https://arxiv.org/abs/2409.16597",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Stevetich/EventHallusion."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Enhancing Nighttime UAV Tracking with Light Distribution Suppression",
    "summary": "arXiv:2409.16631v1 Announce Type: new \nAbstract: Visual object tracking has boosted extensive intelligent applications for unmanned aerial vehicles (UAVs). However, the state-of-the-art (SOTA) enhancers for nighttime UAV tracking always neglect the uneven light distribution in low-light images, inevitably leading to excessive enhancement in scenarios with complex illumination. To address these issues, this work proposes a novel enhancer, i.e., LDEnhancer, enhancing nighttime UAV tracking with light distribution suppression. Specifically, a novel image content refinement module is developed to decompose the light distribution information and image content information in the feature space, allowing for the targeted enhancement of the image content information. Then this work designs a new light distribution generation module to capture light distribution effectively. The features with light distribution information and image content information are fed into the different parameter estimation modules, respectively, for the parameter map prediction. Finally, leveraging two parameter maps, an innovative interweave iteration adjustment is proposed for the collaborative pixel-wise adjustment of low-light images. Additionally, a challenging nighttime UAV tracking dataset with uneven light distribution, namely NAT2024-2, is constructed to provide a comprehensive evaluation, which contains 40 challenging sequences with over 74K frames in total. Experimental results on the authoritative UAV benchmarks and the proposed NAT2024-2 demonstrate that LDEnhancer outperforms other SOTA low-light enhancers for nighttime UAV tracking. Furthermore, real-world tests on a typical UAV platform with an NVIDIA Orin NX confirm the practicality and efficiency of LDEnhancer. The code is available at https://github.com/vision4robotics/LDEnhancer.",
    "link": "https://arxiv.org/abs/2409.16631",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/vision4robotics/LDEnhancer."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Progressive Representation Learning for Real-Time UAV Tracking",
    "summary": "arXiv:2409.16652v1 Announce Type: new \nAbstract: Visual object tracking has significantly promoted autonomous applications for unmanned aerial vehicles (UAVs). However, learning robust object representations for UAV tracking is especially challenging in complex dynamic environments, when confronted with aspect ratio change and occlusion. These challenges severely alter the original information of the object. To handle the above issues, this work proposes a novel progressive representation learning framework for UAV tracking, i.e., PRL-Track. Specifically, PRL-Track is divided into coarse representation learning and fine representation learning. For coarse representation learning, two innovative regulators, which rely on appearance and semantic information, are designed to mitigate appearance interference and capture semantic information. Furthermore, for fine representation learning, a new hierarchical modeling generator is developed to intertwine coarse object representations. Exhaustive experiments demonstrate that the proposed PRL-Track delivers exceptional performance on three authoritative UAV tracking benchmarks. Real-world tests indicate that the proposed PRL-Track realizes superior tracking performance with 42.6 frames per second on the typical UAV platform equipped with an edge smart camera. The code, model, and demo videos are available at \\url{https://github.com/vision4robotics/PRL-Track}.",
    "link": "https://arxiv.org/abs/2409.16652",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/vision4robotics/PRL-Track}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification",
    "summary": "arXiv:2409.16718v1 Announce Type: new \nAbstract: Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed the success of prompt tuning and adapter tuning, while the classic model fine-tuning on inherent parameters seems to be overlooked. It is believed that fine-tuning the parameters of VLMs with few-shot samples corrupts the pre-trained knowledge since fine-tuning the CLIP model even degrades performance. In this paper, we revisit this viewpoint, and propose a new perspective: fine-tuning the specific parameters instead of all will uncover the power of classic model fine-tuning on VLMs. Through our meticulous study, we propose ClipFit, a simple yet effective method to fine-tune CLIP without introducing any overhead of extra parameters. We demonstrate that by only fine-tuning the specific bias terms and normalization layers, ClipFit can improve the performance of zero-shot CLIP by 7.27\\% average harmonic mean accuracy. Lastly, to understand how fine-tuning in CLIPFit affects the pre-trained models, we conducted extensive experimental analyses w.r.t. changes in internal parameters and representations. We found that low-level text bias layers and the first layer normalization layer change much more than other layers. The code is available at \\url{https://github.com/minglllli/CLIPFit}.",
    "link": "https://arxiv.org/abs/2409.16718",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/minglllli/CLIPFit}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Statewide Visual Geolocalization in the Wild",
    "summary": "arXiv:2409.16763v1 Announce Type: new \nAbstract: This work presents a method that is able to predict the geolocation of a street-view photo taken in the wild within a state-sized search region by matching against a database of aerial reference imagery. We partition the search region into geographical cells and train a model to map cells and corresponding photos into a joint embedding space that is used to perform retrieval at test time. The model utilizes aerial images for each cell at multiple levels-of-detail to provide sufficient information about the surrounding scene. We propose a novel layout of the search region with consistent cell resolutions that allows scaling to large geographical regions. Experiments demonstrate that the method successfully localizes 60.6% of all non-panoramic street-view photos uploaded to the crowd-sourcing platform Mapillary in the state of Massachusetts to within 50m of their ground-truth location. Source code is available at https://github.com/fferflo/statewide-visual-geolocalization.",
    "link": "https://arxiv.org/abs/2409.16763",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/fferflo/statewide-visual-geolocalization."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Spacewalker: Traversing Representation Spaces for Fast Interactive Exploration and Annotation of Unstructured Data",
    "summary": "arXiv:2409.16793v1 Announce Type: new \nAbstract: Unstructured data in industries such as healthcare, finance, and manufacturing presents significant challenges for efficient analysis and decision making. Detecting patterns within this data and understanding their impact is critical but complex without the right tools. Traditionally, these tasks relied on the expertise of data analysts or labor-intensive manual reviews. In response, we introduce Spacewalker, an interactive tool designed to explore and annotate data across multiple modalities. Spacewalker allows users to extract data representations and visualize them in low-dimensional spaces, enabling the detection of semantic similarities. Through extensive user studies, we assess Spacewalker's effectiveness in data annotation and integrity verification. Results show that the tool's ability to traverse latent spaces and perform multi-modal queries significantly enhances the user's capacity to quickly identify relevant data. Moreover, Spacewalker allows for annotation speed-ups far superior to conventional methods, making it a promising tool for efficiently navigating unstructured data and improving decision making processes. The code of this work is open-source and can be found at: https://github.com/code-lukas/Spacewalker",
    "link": "https://arxiv.org/abs/2409.16793",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/code-lukas/Spacewalker"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Topological SLAM in colonoscopies leveraging deep features and topological priors",
    "summary": "arXiv:2409.16806v1 Announce Type: new \nAbstract: We introduce ColonSLAM, a system that combines classical multiple-map metric SLAM with deep features and topological priors to create topological maps of the whole colon. The SLAM pipeline by itself is able to create disconnected individual metric submaps representing locations from short video subsections of the colon, but is not able to merge covisible submaps due to deformations and the limited performance of the SIFT descriptor in the medical domain. ColonSLAM is guided by topological priors and combines a deep localization network trained to distinguish if two images come from the same place or not and the soft verification of a transformer-based matching network, being able to relate far-in-time submaps during an exploration, grouping them in nodes imaging the same colon place, building more complex maps than any other approach in the literature. We demonstrate our approach in the Endomapper dataset, showing its potential for producing maps of the whole colon in real human explorations. Code and models are available at: https://github.com/endomapper/ColonSLAM.",
    "link": "https://arxiv.org/abs/2409.16806",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/endomapper/ColonSLAM."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Linking in Style: Understanding learned features in deep learning models",
    "summary": "arXiv:2409.16865v1 Announce Type: new \nAbstract: Convolutional neural networks (CNNs) learn abstract features to perform object classification, but understanding these features remains challenging due to difficult-to-interpret results or high computational costs. We propose an automatic method to visualize and systematically analyze learned features in CNNs. Specifically, we introduce a linking network that maps the penultimate layer of a pre-trained classifier to the latent space of a generative model (StyleGAN-XL), thereby enabling an interpretable, human-friendly visualization of the classifier's representations. Our findings indicate a congruent semantic order in both spaces, enabling a direct linear mapping between them. Training the linking network is computationally inexpensive and decoupled from training both the GAN and the classifier. We introduce an automatic pipeline that utilizes such GAN-based visualizations to quantify learned representations by analyzing activation changes in the classifier in the image domain. This quantification allows us to systematically study the learned representations in several thousand units simultaneously and to extract and visualize units selective for specific semantic concepts. Further, we illustrate how our method can be used to quantify and interpret the classifier's decision boundary using counterfactual examples. Overall, our method offers systematic and objective perspectives on learned abstract representations in CNNs. https://github.com/kaschube-lab/LinkingInStyle.git",
    "link": "https://arxiv.org/abs/2409.16865",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/kaschube-lab/LinkingInStyle.git"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2",
    "summary": "arXiv:2409.16902v1 Announce Type: new \nAbstract: Over the past decade, significant progress has been made in visual object tracking, largely due to the availability of large-scale training datasets. However, existing tracking datasets are primarily focused on open-air scenarios, which greatly limits the development of object tracking in underwater environments. To address this issue, we take a step forward by proposing the first large-scale underwater camouflaged object tracking dataset, namely UW-COT. Based on the proposed dataset, this paper presents an experimental evaluation of several advanced visual object tracking methods and the latest advancements in image and video segmentation. Specifically, we compare the performance of the Segment Anything Model (SAM) and its updated version, SAM 2, in challenging underwater environments. Our findings highlight the improvements in SAM 2 over SAM, demonstrating its enhanced capability to handle the complexities of underwater camouflaged objects. Compared to current advanced visual object tracking methods, the latest video segmentation foundation model SAM 2 also exhibits significant advantages, providing valuable insights into the development of more effective tracking technologies for underwater scenarios. The dataset will be accessible at \\color{magenta}{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.",
    "link": "https://arxiv.org/abs/2409.16902",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/983632847/Awesome-Multimodal-Object-Tracking}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Face Forgery Detection with Elaborate Backbone",
    "summary": "arXiv:2409.16945v1 Announce Type: new \nAbstract: Face Forgery Detection (FFD), or Deepfake detection, aims to determine whether a digital face is real or fake. Due to different face synthesis algorithms with diverse forgery patterns, FFD models often overfit specific patterns in training datasets, resulting in poor generalization to other unseen forgeries. This severe challenge requires FFD models to possess strong capabilities in representing complex facial features and extracting subtle forgery cues. Although previous FFD models directly employ existing backbones to represent and extract facial forgery cues, the critical role of backbones is often overlooked, particularly as their knowledge and capabilities are insufficient to address FFD challenges, inevitably limiting generalization. Therefore, it is essential to integrate the backbone pre-training configurations and seek practical solutions by revisiting the complete FFD workflow, from backbone pre-training and fine-tuning to inference of discriminant results. Specifically, we analyze the crucial contributions of backbones with different configurations in FFD task and propose leveraging the ViT network with self-supervised learning on real-face datasets to pre-train a backbone, equipping it with superior facial representation capabilities. We then build a competitive backbone fine-tuning framework that strengthens the backbone's ability to extract diverse forgery cues within a competitive learning mechanism. Moreover, we devise a threshold optimization mechanism that utilizes prediction confidence to improve the inference reliability. Comprehensive experiments demonstrate that our FFD model with the elaborate backbone achieves excellent performance in FFD and extra face-related tasks, i.e., presentation attack detection. Code and models are available at https://github.com/zhenglab/FFDBackbone.",
    "link": "https://arxiv.org/abs/2409.16945",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/zhenglab/FFDBackbone."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling",
    "summary": "arXiv:2409.16949v1 Announce Type: new \nAbstract: In this paper, we present an effective data augmentation framework leveraging the Large Language Model (LLM) and Diffusion Model (DM) to tackle the challenges inherent in data-scarce scenarios. Recently, DMs have opened up the possibility of generating synthetic images to complement a few training images. However, increasing the diversity of synthetic images also raises the risk of generating samples outside the target distribution. Our approach addresses this issue by embedding novel semantic information into text prompts via LLM and utilizing real images as visual prompts, thus generating semantically rich images. To ensure that the generated images remain within the target distribution, we dynamically adjust the guidance weight based on each image's CLIPScore to control the diversity. Experimental results show that our method produces synthetic images with enhanced diversity while maintaining adherence to the target distribution. Consequently, our approach proves to be more efficient in the few-shot setting on several benchmarks. Our code is available at https://github.com/kkyuhun94/dalda .",
    "link": "https://arxiv.org/abs/2409.16949",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/kkyuhun94/dalda"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "PTQ4RIS: Post-Training Quantization for Referring Image Segmentation",
    "summary": "arXiv:2409.17020v1 Announce Type: new \nAbstract: Referring Image Segmentation (RIS), aims to segment the object referred by a given sentence in an image by understanding both visual and linguistic information. However, existing RIS methods tend to explore top-performance models, disregarding considerations for practical applications on resources-limited edge devices. This oversight poses a significant challenge for on-device RIS inference. To this end, we propose an effective and efficient post-training quantization framework termed PTQ4RIS. Specifically, we first conduct an in-depth analysis of the root causes of performance degradation in RIS model quantization and propose dual-region quantization (DRQ) and reorder-based outlier-retained quantization (RORQ) to address the quantization difficulties in visual and text encoders. Extensive experiments on three benchmarks with different bits settings (from 8 to 4 bits) demonstrates its superior performance. Importantly, we are the first PTQ method specifically designed for the RIS task, highlighting the feasibility of PTQ in RIS applications. Code will be available at {https://github.com/gugu511yy/PTQ4RIS}.",
    "link": "https://arxiv.org/abs/2409.17020",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/gugu511yy/PTQ4RIS}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Enhanced Wavelet Scattering Network for image inpainting detection",
    "summary": "arXiv:2409.17023v1 Announce Type: new \nAbstract: The rapid advancement of image inpainting tools, especially those aimed at removing artifacts, has made digital image manipulation alarmingly accessible. This paper proposes several innovative ideas for detecting inpainting forgeries based on low level noise analysis by combining Dual-Tree Complex Wavelet Transform (DT-CWT) for feature extraction with convolutional neural networks (CNN) for forged area detection and localization, and lastly by employing an innovative combination of texture segmentation with noise variance estimations. The DT-CWT offers significant advantages due to its shift-invariance, enhancing its robustness against subtle manipulations during the inpainting process. Furthermore, its directional selectivity allows for the detection of subtle artifacts introduced by inpainting within specific frequency bands and orientations. Various neural network architectures were evaluated and proposed. Lastly, we propose a fusion detection module that combines texture analysis with noise variance estimation to give the forged area. Our approach was benchmarked against state-of-the-art methods and demonstrated superior performance over all cited alternatives. The training code (with pretrained model weights) as long as the dataset will be available at https://github.com/jmaba/Deep-dual-tree-complex-neural-network-for-image-inpainting-detection",
    "link": "https://arxiv.org/abs/2409.17023",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/jmaba/Deep-dual-tree-complex-neural-network-for-image-inpainting-detection"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "BitQ: Tailoring Block Floating Point Precision for Improved DNN Efficiency on Resource-Constrained Devices",
    "summary": "arXiv:2409.17093v1 Announce Type: new \nAbstract: Deep neural networks (DNNs) are powerful for cognitive tasks such as image classification, object detection, and scene segmentation. One drawback however is the significant high computational complexity and memory consumption, which makes them unfeasible to run real-time on embedded platforms because of the limited hardware resources. Block floating point (BFP) quantization is one of the representative compression approaches for reducing the memory and computational burden owing to their capability to effectively capture the broad data distribution of DNN models. Unfortunately, prior works on BFP-based quantization empirically choose the block size and the precision that preserve accuracy. In this paper, we develop a BFP-based bitwidth-aware analytical modeling framework (called ``BitQ'') for the best BFP implementation of DNN inference on embedded platforms. We formulate and resolve an optimization problem to identify the optimal BFP block size and bitwidth distribution by the trade-off of both accuracy and performance loss. Experimental results show that compared with an equal bitwidth setting, the BFP DNNs with optimized bitwidth allocation provide efficient computation, preserving accuracy on famous benchmarks. The source code and data are available at https://github.com/Cheliosoops/BitQ.",
    "link": "https://arxiv.org/abs/2409.17093",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Cheliosoops/BitQ."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "General Detection-based Text Line Recognition",
    "summary": "arXiv:2409.17095v1 Announce Type: new \nAbstract: We introduce a general detection-based approach to text line recognition, be it printed (OCR) or handwritten (HTR), with Latin, Chinese, or ciphered characters. Detection-based approaches have until now been largely discarded for HTR because reading characters separately is often challenging, and character-level annotation is difficult and expensive. We overcome these challenges thanks to three main insights: (i) synthetic pre-training with sufficiently diverse data enables learning reasonable character localization for any script; (ii) modern transformer-based detectors can jointly detect a large number of instances, and, if trained with an adequate masking strategy, leverage consistency between the different detections; (iii) once a pre-trained detection model with approximate character localization is available, it is possible to fine-tune it with line-level annotation on real data, even with a different alphabet. Our approach, dubbed DTLR, builds on a completely different paradigm than state-of-the-art HTR methods, which rely on autoregressive decoding, predicting character values one by one, while we treat a complete line in parallel. Remarkably, we demonstrate good performance on a large range of scripts, usually tackled with specialized approaches. In particular, we improve state-of-the-art performances for Chinese script recognition on the CASIA v2 dataset, and for cipher recognition on the Borg and Copiale datasets. Our code and models are available at https://github.com/raphael-baena/DTLR.",
    "link": "https://arxiv.org/abs/2409.17095",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/raphael-baena/DTLR."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "MorphoSeg: An Uncertainty-Aware Deep Learning Method for Biomedical Segmentation of Complex Cellular Morphologies",
    "summary": "arXiv:2409.17110v1 Announce Type: new \nAbstract: Deep learning has revolutionized medical and biological imaging, particularly in segmentation tasks. However, segmenting biological cells remains challenging due to the high variability and complexity of cell shapes. Addressing this challenge requires high-quality datasets that accurately represent the diverse morphologies found in biological cells. Existing cell segmentation datasets are often limited by their focus on regular and uniform shapes. In this paper, we introduce a novel benchmark dataset of Ntera-2 (NT2) cells, a pluripotent carcinoma cell line, exhibiting diverse morphologies across multiple stages of differentiation, capturing the intricate and heterogeneous cellular structures that complicate segmentation tasks. To address these challenges, we propose an uncertainty-aware deep learning framework for complex cellular morphology segmentation (MorphoSeg) by incorporating sampling of virtual outliers from low-likelihood regions during training. Our comprehensive experimental evaluations against state-of-the-art baselines demonstrate that MorphoSeg significantly enhances segmentation accuracy, achieving up to a 7.74% increase in the Dice Similarity Coefficient (DSC) and a 28.36% reduction in the Hausdorff Distance. These findings highlight the effectiveness of our dataset and methodology in advancing cell segmentation capabilities, especially for complex and variable cell morphologies. The dataset and source code is publicly available at https://github.com/RanchoGoose/MorphoSeg.",
    "link": "https://arxiv.org/abs/2409.17110",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/RanchoGoose/MorphoSeg."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "TSBP: Improving Object Detection in Histology Images via Test-time Self-guided Bounding-box Propagation",
    "summary": "arXiv:2409.16678v1 Announce Type: cross \nAbstract: A global threshold (e.g., 0.5) is often applied to determine which bounding boxes should be included in the final results for an object detection task. A higher threshold reduces false positives but may result in missing a significant portion of true positives. A lower threshold can increase detection recall but may also result in more false positives. Because of this, using a preset global threshold (e.g., 0.5) applied to all the bounding box candidates may lead to suboptimal solutions. In this paper, we propose a Test-time Self-guided Bounding-box Propagation (TSBP) method, leveraging Earth Mover's Distance (EMD) to enhance object detection in histology images. TSBP utilizes bounding boxes with high confidence to influence those with low confidence, leveraging visual similarities between them. This propagation mechanism enables bounding boxes to be selected in a controllable, explainable, and robust manner, which surpasses the effectiveness of using simple thresholds and uncertainty calibration methods. Importantly, TSBP does not necessitate additional labeled samples for model training or parameter estimation, unlike calibration methods. We conduct experiments on gland detection and cell detection tasks in histology images. The results show that our proposed TSBP significantly improves detection outcomes when working in conjunction with state-of-the-art deep learning-based detection networks. Compared to other methods such as uncertainty calibration, TSBP yields more robust and accurate object detection predictions while using no additional labeled samples. The code is available at https://github.com/jwhgdeu/TSBP.",
    "link": "https://arxiv.org/abs/2409.16678",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/jwhgdeu/TSBP."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "SDCL: Students Discrepancy-Informed Correction Learning for Semi-supervised Medical Image Segmentation",
    "summary": "arXiv:2409.16728v1 Announce Type: cross \nAbstract: Semi-supervised medical image segmentation (SSMIS) has been demonstrated the potential to mitigate the issue of limited medical labeled data. However, confirmation and cognitive biases may affect the prevalent teacher-student based SSMIS methods due to erroneous pseudo-labels. To tackle this challenge, we improve the mean teacher approach and propose the Students Discrepancy-Informed Correction Learning (SDCL) framework that includes two students and one non-trainable teacher, which utilizes the segmentation difference between the two students to guide the self-correcting learning. The essence of SDCL is to identify the areas of segmentation discrepancy as the potential bias areas, and then encourage the model to review the correct cognition and rectify their own biases in these areas. To facilitate the bias correction learning with continuous review and rectification, two correction loss functions are employed to minimize the correct segmentation voxel distance and maximize the erroneous segmentation voxel entropy. We conducted experiments on three public medical image datasets: two 3D datasets (CT and MRI) and one 2D dataset (MRI). The results show that our SDCL surpasses the current State-of-the-Art (SOTA) methods by 2.57\\%, 3.04\\%, and 2.34\\% in the Dice score on the Pancreas, LA, and ACDC datasets, respectively. In addition, the accuracy of our method is very close to the fully supervised method on the ACDC dataset, and even exceeds the fully supervised method on the Pancreas and LA dataset. (Code available at \\url{https://github.com/pascalcpp/SDCL}).",
    "link": "https://arxiv.org/abs/2409.16728",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/pascalcpp/SDCL})."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Scalable Ensemble Diversification for OOD Generalization and Detection",
    "summary": "arXiv:2409.16797v1 Announce Type: cross \nAbstract: Training a diverse ensemble of models has several practical applications such as providing candidates for model selection with better out-of-distribution (OOD) generalization, and enabling the detection of OOD samples via Bayesian principles. An existing approach to diverse ensemble training encourages the models to disagree on provided OOD samples. However, the approach is computationally expensive and it requires well-separated ID and OOD examples, such that it has only been demonstrated in small-scale settings.\n  $\\textbf{Method.}$ This work presents a method for Scalable Ensemble Diversification (SED) applicable to large-scale settings (e.g. ImageNet) that does not require OOD samples. Instead, SED identifies hard training samples on the fly and encourages the ensemble members to disagree on these. To improve scaling, we show how to avoid the expensive computations in existing methods of exhaustive pairwise disagreements across models.\n  $\\textbf{Results.}$ We evaluate the benefits of diversification with experiments on ImageNet. First, for OOD generalization, we observe large benefits from the diversification in multiple settings including output-space (classical) ensembles and weight-space ensembles (model soups). Second, for OOD detection, we turn the diversity of ensemble hypotheses into a novel uncertainty score estimator that surpasses a large number of OOD detection baselines.\n  Code is available here: https://github.com/AlexanderRubinstein/diverse-universe-public.",
    "link": "https://arxiv.org/abs/2409.16797",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/AlexanderRubinstein/diverse-universe-public."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Multi-Robot Informative Path Planning for Efficient Target Mapping using Deep Reinforcement Learning",
    "summary": "arXiv:2409.16967v1 Announce Type: cross \nAbstract: Autonomous robots are being employed in several mapping and data collection tasks due to their efficiency and low labor costs. In these tasks, the robots are required to map targets-of-interest in an unknown environment while constrained to a given resource budget such as path length or mission time. This is a challenging problem as each robot has to not only detect and avoid collisions from static obstacles in the environment but also has to model other robots' trajectories to avoid inter-robot collisions. We propose a novel deep reinforcement learning approach for multi-robot informative path planning to map targets-of-interest in an unknown 3D environment. A key aspect of our approach is an augmented graph that models other robots' trajectories to enable planning for communication and inter-robot collision avoidance. We train our decentralized reinforcement learning policy via the centralized training and decentralized execution paradigm. Once trained, our policy is also scalable to varying number of robots and does not require re-training. Our approach outperforms other state-of-the-art multi-robot target mapping approaches by 33.75% in terms of the number of discovered targets-of-interest. We open-source our code and model at: https://github.com/AccGen99/marl_ipp",
    "link": "https://arxiv.org/abs/2409.16967",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/AccGen99/marl_ipp"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "WasteGAN: Data Augmentation for Robotic Waste Sorting through Generative Adversarial Networks",
    "summary": "arXiv:2409.16999v1 Announce Type: cross \nAbstract: Robotic waste sorting poses significant challenges in both perception and manipulation, given the extreme variability of objects that should be recognized on a cluttered conveyor belt. While deep learning has proven effective in solving complex tasks, the necessity for extensive data collection and labeling limits its applicability in real-world scenarios like waste sorting. To tackle this issue, we introduce a data augmentation method based on a novel GAN architecture called wasteGAN. The proposed method allows to increase the performance of semantic segmentation models, starting from a very limited bunch of labeled examples, such as few as 100. The key innovations of wasteGAN include a novel loss function, a novel activation function, and a larger generator block. Overall, such innovations helps the network to learn from limited number of examples and synthesize data that better mirrors real-world distributions. We then leverage the higher-quality segmentation masks predicted from models trained on the wasteGAN synthetic data to compute semantic-aware grasp poses, enabling a robotic arm to effectively recognizing contaminants and separating waste in a real-world scenario. Through comprehensive evaluation encompassing dataset-based assessments and real-world experiments, our methodology demonstrated promising potential for robotic waste sorting, yielding performance gains of up to 5.8\\% in picking contaminants. The project page is available at https://github.com/bach05/wasteGAN.git",
    "link": "https://arxiv.org/abs/2409.16999",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/bach05/wasteGAN.git"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "PACE: marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization",
    "summary": "arXiv:2409.17137v1 Announce Type: cross \nAbstract: Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained vision transformers to downstream tasks. However, the optimization for tasks performance often comes at the cost of generalizability in fine-tuned models. To address this issue, we theoretically connect smaller weight gradient norms during training and larger datasets to the improved model generalization. Motivated by this connection, we propose reducing gradient norms for enhanced generalization and aligning fine-tuned model with the pre-trained counterpart to retain knowledge from large-scale pre-training data. Yet, naive alignment does not guarantee gradient reduction and can potentially cause gradient explosion, complicating efforts to manage gradients. To address such issues, we propose PACE, marrying generalization of PArameter-efficient fine-tuning with Consistency rEgularization. We perturb features learned from the adapter with the multiplicative noise and ensure the fine-tuned model remains consistent for same sample under different perturbations. Theoretical analysis shows that PACE not only implicitly regularizes gradients for enhanced generalization, but also implicitly aligns the fine-tuned and pre-trained models to retain knowledge. Experimental evidence supports our theories. PACE outperforms existing PEFT methods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning and domain adaptation. Code will be available at https://github.com/MaxwellYaoNi/PACE",
    "link": "https://arxiv.org/abs/2409.17137",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/MaxwellYaoNi/PACE"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Event-Free Moving Object Segmentation from Moving Ego Vehicle",
    "summary": "arXiv:2305.00126v3 Announce Type: replace \nAbstract: Moving object segmentation (MOS) in dynamic scenes is an important, challenging, but under-explored research topic for autonomous driving, especially for sequences obtained from moving ego vehicles. Most segmentation methods leverage motion cues obtained from optical flow maps. However, since these methods are often based on optical flows that are pre-computed from successive RGB frames, this neglects the temporal consideration of events occurring within the inter-frame, consequently constraining its ability to discern objects exhibiting relative staticity but genuinely in motion. To address these limitations, we propose to exploit event cameras for better video understanding, which provide rich motion cues without relying on optical flow. To foster research in this area, we first introduce a novel large-scale dataset called DSEC-MOS for moving object segmentation from moving ego vehicles, which is the first of its kind. For benchmarking, we select various mainstream methods and rigorously evaluate them on our dataset. Subsequently, we devise EmoFormer, a novel network able to exploit the event data. For this purpose, we fuse the event temporal prior with spatial semantic maps to distinguish genuinely moving objects from the static background, adding another level of dense supervision around our object of interest. Our proposed network relies only on event data for training but does not require event input during inference, making it directly comparable to frame-only methods in terms of efficiency and more widely usable in many application cases. The exhaustive comparison highlights a significant performance improvement of our method over all other methods. The source code and dataset are publicly available at: https://github.com/ZZY-Zhou/DSEC-MOS.",
    "link": "https://arxiv.org/abs/2305.00126",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ZZY-Zhou/DSEC-MOS."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "YCB-Ev 1.1: Event-vision dataset for 6DoF object pose estimation",
    "summary": "arXiv:2309.08482v2 Announce Type: replace \nAbstract: Our work introduces the YCB-Ev dataset, which contains synchronized RGB-D frames and event data that enables evaluating 6DoF object pose estimation algorithms using these modalities.\n  This dataset provides ground truth 6DoF object poses for the same 21 YCB objects that were used in the YCB-Video (YCB-V) dataset, allowing for cross-dataset algorithm performance evaluation.\n  The dataset consists of 21 synchronized event and RGB-D sequences, totalling 13,851 frames (7 minutes and 43 seconds of event data). Notably, 12 of these sequences feature the same object arrangement as the YCB-V subset used in the BOP challenge.\n  Ground truth poses are generated by detecting objects in the RGB-D frames, interpolating the poses to align with the event timestamps, and then transferring them to the event coordinate frame using extrinsic calibration.\n  Our dataset is the first to provide ground truth 6DoF pose data for event streams. Furthermore, we evaluate the generalization capabilities of two state-of-the-art algorithms, which were pre-trained for the BOP challenge, using our novel YCB-V sequences.\n  The dataset is publicly available at https://github.com/paroj/ycbev.",
    "link": "https://arxiv.org/abs/2309.08482",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/paroj/ycbev."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Improving Pseudo-labelling and Enhancing Robustness for Semi-Supervised Domain Generalization",
    "summary": "arXiv:2401.13965v2 Announce Type: replace \nAbstract: Beyond attaining domain generalization (DG), visual recognition models should also be data-efficient during learning by leveraging limited labels. We study the problem of Semi-Supervised Domain Generalization (SSDG) which is crucial for real-world applications like automated healthcare. SSDG requires learning a cross-domain generalizable model when the given training data is only partially labelled. Empirical investigations reveal that the DG methods tend to underperform in SSDG settings, likely because they are unable to exploit the unlabelled data. Semi-supervised learning (SSL) shows improved but still inferior results compared to fully-supervised learning. A key challenge, faced by the best-performing SSL-based SSDG methods, is selecting accurate pseudo-labels under multiple domain shifts and reducing overfitting to source domains under limited labels. In this work, we propose new SSDG approach, which utilizes a novel uncertainty-guided pseudo-labelling with model averaging (UPLM). Our uncertainty-guided pseudo-labelling (UPL) uses model uncertainty to improve pseudo-labelling selection, addressing poor model calibration under multi-source unlabelled data. The UPL technique, enhanced by our novel model averaging (MA) strategy, mitigates overfitting to source domains with limited labels. Extensive experiments on key representative DG datasets suggest that our method demonstrates effectiveness against existing methods. Our code and chosen labelled data seeds are available on GitHub: https://github.com/Adnan-Khan7/UPLM",
    "link": "https://arxiv.org/abs/2401.13965",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Adnan-Khan7/UPLM"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "ScanTalk: 3D Talking Heads from Unregistered Scans",
    "summary": "arXiv:2403.10942v3 Announce Type: replace \nAbstract: Speech-driven 3D talking heads generation has emerged as a significant area of interest among researchers, presenting numerous challenges. Existing methods are constrained by animating faces with fixed topologies, wherein point-wise correspondence is established, and the number and order of points remains consistent across all identities the model can animate. In this work, we present \\textbf{ScanTalk}, a novel framework capable of animating 3D faces in arbitrary topologies including scanned data. Our approach relies on the DiffusionNet architecture to overcome the fixed topology constraint, offering promising avenues for more flexible and realistic 3D animations. By leveraging the power of DiffusionNet, ScanTalk not only adapts to diverse facial structures but also maintains fidelity when dealing with scanned data, thereby enhancing the authenticity and versatility of generated 3D talking heads. Through comprehensive comparisons with state-of-the-art methods, we validate the efficacy of our approach, demonstrating its capacity to generate realistic talking heads comparable to existing techniques. While our primary objective is to develop a generic method free from topological constraints, all state-of-the-art methodologies are bound by such limitations. Code for reproducing our results, and the pre-trained model are available at https://github.com/miccunifi/ScanTalk .",
    "link": "https://arxiv.org/abs/2403.10942",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/miccunifi/ScanTalk"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text Cues",
    "summary": "arXiv:2406.19364v3 Announce Type: replace \nAbstract: Weakly-supervised medical image segmentation is a challenging task that aims to reduce the annotation cost while keep the segmentation performance. In this paper, we present a novel framework, SimTxtSeg, that leverages simple text cues to generate high-quality pseudo-labels and study the cross-modal fusion in training segmentation models, simultaneously. Our contribution consists of two key components: an effective Textual-to-Visual Cue Converter that produces visual prompts from text prompts on medical images, and a text-guided segmentation model with Text-Vision Hybrid Attention that fuses text and image features. We evaluate our framework on two medical image segmentation tasks: colonic polyp segmentation and MRI brain tumor segmentation, and achieve consistent state-of-the-art performance. Source code is available at: https://github.com/xyx1024/SimTxtSeg.",
    "link": "https://arxiv.org/abs/2406.19364",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/xyx1024/SimTxtSeg."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval",
    "summary": "arXiv:2407.12346v2 Announce Type: replace \nAbstract: The pre-trained vision and language (V\\&amp;L) models have substantially improved the performance of cross-modal image-text retrieval. In general, however, V\\&amp;L models have limited retrieval performance for small objects because of the rough alignment between words and the small objects in the image. In contrast, it is known that human cognition is object-centric, and we pay more attention to important objects, even if they are small. To bridge this gap between the human cognition and the V\\&amp;L model's capability, we propose a cross-modal image-text retrieval framework based on ``object-aware query perturbation.'' The proposed method generates a key feature subspace of the detected objects and perturbs the corresponding queries using this subspace to improve the object awareness in the image. In our proposed method, object-aware cross-modal image-text retrieval is possible while keeping the rich expressive power and retrieval performance of existing V\\&amp;L models without additional fine-tuning. Comprehensive experiments on four public datasets show that our method outperforms conventional algorithms. Our code is publicly available at \\url{https://github.com/NEC-N-SOGI/query-perturbation}.",
    "link": "https://arxiv.org/abs/2407.12346",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/NEC-N-SOGI/query-perturbation}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "RoboSense: Large-scale Dataset and Benchmark for Multi-sensor Low-speed Autonomous Driving",
    "summary": "arXiv:2408.15503v3 Announce Type: replace \nAbstract: Robust object detection and tracking under arbitrary sight of view is challenging yet essential for the development of Autonomous Vehicle technology. With the growing demand of unmanned function vehicles, near-field scene understanding becomes an important research topic in the areas of low-speed autonomous driving. Due to the complexity of driving conditions and diversity of near obstacles such as blind spots and high occlusion, the perception capability of near-field environment is still inferior than its farther counterpart. To further enhance the intelligent ability of unmanned vehicles, in this paper, we construct a multimodal data collection platform based on 3 main types of sensors (Camera, LiDAR and Fisheye), which supports flexible sensor configurations to enable dynamic sight of view for ego vehicle, either global view or local view. Meanwhile, a large-scale multi-sensor dataset is built, named RoboSense, to facilitate near-field scene understanding. RoboSense contains more than 133K synchronized data with 1.4M 3D bounding box and IDs annotated in the full $360^{\\circ}$ view, forming 216K trajectories across 7.6K temporal sequences. It has $270\\times$ and $18\\times$ as many annotations of near-field obstacles within 5$m$ as the previous single-vehicle datasets such as KITTI and nuScenes. Moreover, we define a novel matching criterion for near-field 3D perception and prediction metrics. Based on RoboSense, we formulate 6 popular tasks to facilitate the future development of related research, where the detailed data analysis as well as benchmarks are also provided accordingly. Code and dataset will be available at https://github.com/suhaisheng/RoboSense.",
    "link": "https://arxiv.org/abs/2408.15503",
    "published": "Thu, 26 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/suhaisheng/RoboSense."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  }
]