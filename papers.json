[
  {
    "title": "Evaluation of Security of ML-based Watermarking: Copy and Removal Attacks",
    "summary": "arXiv:2409.18211v1 Announce Type: new \nAbstract: The vast amounts of digital content captured from the real world or AI-generated media necessitate methods for copyright protection, traceability, or data provenance verification. Digital watermarking serves as a crucial approach to address these challenges. Its evolution spans three generations: handcrafted, autoencoder-based, and foundation model based methods. %Its evolution spans three generations: handcrafted methods, autoencoder-based schemes, and methods based on foundation models. While the robustness of these systems is well-documented, the security against adversarial attacks remains underexplored. This paper evaluates the security of foundation models' latent space digital watermarking systems that utilize adversarial embedding techniques. A series of experiments investigate the security dimensions under copy and removal attacks, providing empirical insights into these systems' vulnerabilities. All experimental codes and results are available at https://github.com/vkinakh/ssl-watermarking-attacks}{repository",
    "link": "https://arxiv.org/abs/2409.18211",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/vkinakh/ssl-watermarking-attacks}{repository"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Harnessing Wavelet Transformations for Generalizable Deepfake Forgery Detection",
    "summary": "arXiv:2409.18301v1 Announce Type: new \nAbstract: The evolution of digital image manipulation, particularly with the advancement of deep generative models, significantly challenges existing deepfake detection methods, especially when the origin of the deepfake is obscure. To tackle the increasing complexity of these forgeries, we propose \\textbf{Wavelet-CLIP}, a deepfake detection framework that integrates wavelet transforms with features derived from the ViT-L/14 architecture, pre-trained in the CLIP fashion. Wavelet-CLIP utilizes Wavelet Transforms to deeply analyze both spatial and frequency features from images, thus enhancing the model's capability to detect sophisticated deepfakes. To verify the effectiveness of our approach, we conducted extensive evaluations against existing state-of-the-art methods for cross-dataset generalization and detection of unseen images generated by standard diffusion models. Our method showcases outstanding performance, achieving an average AUC of 0.749 for cross-data generalization and 0.893 for robustness against unseen deepfakes, outperforming all compared methods. The code can be reproduced from the repo: \\url{https://github.com/lalithbharadwajbaru/Wavelet-CLIP}",
    "link": "https://arxiv.org/abs/2409.18301",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/lalithbharadwajbaru/Wavelet-CLIP}"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Does End-to-End Autonomous Driving Really Need Perception Tasks?",
    "summary": "arXiv:2409.18341v1 Announce Type: new \nAbstract: End-to-End Autonomous Driving (E2EAD) methods typically rely on supervised perception tasks to extract explicit scene information (e.g., objects, maps). This reliance necessitates expensive annotations and constrains deployment and data scalability in real-time applications. In this paper, we introduce SSR, a novel framework that utilizes only 16 navigation-guided tokens as Sparse Scene Representation, efficiently extracting crucial scene information for E2EAD. Our method eliminates the need for supervised sub-tasks, allowing computational resources to concentrate on essential elements directly related to navigation intent. We further introduce a temporal enhancement module that employs a Bird's-Eye View (BEV) world model, aligning predicted future scenes with actual future scenes through self-supervision. SSR achieves state-of-the-art planning performance on the nuScenes dataset, demonstrating a 27.2\\% relative reduction in L2 error and a 51.6\\% decrease in collision rate to the leading E2EAD method, UniAD. Moreover, SSR offers a 10.9$\\times$ faster inference speed and 13$\\times$ faster training time. This framework represents a significant leap in real-time autonomous driving systems and paves the way for future scalable deployment. Code will be released at \\url{https://github.com/PeidongLi/SSR}.",
    "link": "https://arxiv.org/abs/2409.18341",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/PeidongLi/SSR}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Underwater Image Enhancement with Physical-based Denoising Diffusion Implicit Models",
    "summary": "arXiv:2409.18476v1 Announce Type: new \nAbstract: Underwater vision is crucial for autonomous underwater vehicles (AUVs), and enhancing degraded underwater images in real-time on a resource-constrained AUV is a key challenge due to factors like light absorption and scattering, or the sufficient model computational complexity to resolve such factors. Traditional image enhancement techniques lack adaptability to varying underwater conditions, while learning-based methods, particularly those using convolutional neural networks (CNNs) and generative adversarial networks (GANs), offer more robust solutions but face limitations such as inadequate enhancement, unstable training, or mode collapse. Denoising diffusion probabilistic models (DDPMs) have emerged as a state-of-the-art approach in image-to-image tasks but require intensive computational complexity to achieve the desired underwater image enhancement (UIE) using the recent UW-DDPM solution. To address these challenges, this paper introduces UW-DiffPhys, a novel physical-based and diffusion-based UIE approach. UW-DiffPhys combines light-computation physical-based UIE network components with a denoising U-Net to replace the computationally intensive distribution transformation U-Net in the existing UW-DDPM framework, reducing complexity while maintaining performance. Additionally, the Denoising Diffusion Implicit Model (DDIM) is employed to accelerate the inference process through non-Markovian sampling. Experimental results demonstrate that UW-DiffPhys achieved a substantial reduction in computational complexity and inference time compared to UW-DDPM, with competitive performance in key metrics such as PSNR, SSIM, UCIQE, and an improvement in the overall underwater image quality UIQM metric. The implementation code can be found at the following repository: https://github.com/bachzz/UW-DiffPhys",
    "link": "https://arxiv.org/abs/2409.18476",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/bachzz/UW-DiffPhys"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Prompt-Driven Temporal Domain Adaptation for Nighttime UAV Tracking",
    "summary": "arXiv:2409.18533v1 Announce Type: new \nAbstract: Nighttime UAV tracking under low-illuminated scenarios has achieved great progress by domain adaptation (DA). However, previous DA training-based works are deficient in narrowing the discrepancy of temporal contexts for UAV trackers. To address the issue, this work proposes a prompt-driven temporal domain adaptation training framework to fully utilize temporal contexts for challenging nighttime UAV tracking, i.e., TDA. Specifically, the proposed framework aligns the distribution of temporal contexts from daytime and nighttime domains by training the temporal feature generator against the discriminator. The temporal-consistent discriminator progressively extracts shared domain-specific features to generate coherent domain discrimination results in the time series. Additionally, to obtain high-quality training samples, a prompt-driven object miner is employed to precisely locate objects in unannotated nighttime videos. Moreover, a new benchmark for long-term nighttime UAV tracking is constructed. Exhaustive evaluations on both public and self-constructed nighttime benchmarks demonstrate the remarkable performance of the tracker trained in TDA framework, i.e., TDA-Track. Real-world tests at nighttime also show its practicality. The code and demo videos are available at https://github.com/vision4robotics/TDA-Track.",
    "link": "https://arxiv.org/abs/2409.18533",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/vision4robotics/TDA-Track."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Reducing Semantic Ambiguity In Domain Adaptive Semantic Segmentation Via Probabilistic Prototypical Pixel Contrast",
    "summary": "arXiv:2409.18543v1 Announce Type: new \nAbstract: Domain adaptation aims to reduce the model degradation on the target domain caused by the domain shift between the source and target domains. Although encouraging performance has been achieved by combining cognitive learning with the self-training paradigm, they suffer from ambiguous scenarios caused by scale, illumination, or overlapping when deploying deterministic embedding. To address these issues, we propose probabilistic proto-typical pixel contrast (PPPC), a universal adaptation framework that models each pixel embedding as a probability via multivariate Gaussian distribution to fully exploit the uncertainty within them, eventually improving the representation quality of the model. In addition, we derive prototypes from probability estimation posterior probability estimation which helps to push the decision boundary away from the ambiguity points. Moreover, we employ an efficient method to compute similarity between distributions, eliminating the need for sampling and reparameterization, thereby significantly reducing computational overhead. Further, we dynamically select the ambiguous crops at the image level to enlarge the number of boundary points involved in contrastive learning, which benefits the establishment of precise distributions for each category. Extensive experimentation demonstrates that PPPC not only helps to address ambiguity at the pixel level, yielding discriminative representations but also achieves significant improvements in both synthetic-to-real and day-to-night adaptation tasks. It surpasses the previous state-of-the-art (SOTA) by +5.2% mIoU in the most challenging daytime-to-nighttime adaptation scenario, exhibiting stronger generalization on other unseen datasets. The code and models are available at https://github.com/DarlingInTheSV/Probabilistic-Prototypical-Pixel-Contrast.",
    "link": "https://arxiv.org/abs/2409.18543",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/DarlingInTheSV/Probabilistic-Prototypical-Pixel-Contrast."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "AL-GTD: Deep Active Learning for Gaze Target Detection",
    "summary": "arXiv:2409.18561v1 Announce Type: new \nAbstract: Gaze target detection aims at determining the image location where a person is looking. While existing studies have made significant progress in this area by regressing accurate gaze heatmaps, these achievements have largely relied on access to extensive labeled datasets, which demands substantial human labor. In this paper, our goal is to reduce the reliance on the size of labeled training data for gaze target detection. To achieve this, we propose AL-GTD, an innovative approach that integrates supervised and self-supervised losses within a novel sample acquisition function to perform active learning (AL). Additionally, it utilizes pseudo-labeling to mitigate distribution shifts during the training phase. AL-GTD achieves the best of all AUC results by utilizing only 40-50% of the training data, in contrast to state-of-the-art (SOTA) gaze target detectors requiring the entire training dataset to achieve the same performance. Importantly, AL-GTD quickly reaches satisfactory performance with 10-20% of the training data, showing the effectiveness of our acquisition function, which is able to acquire the most informative samples. We provide a comprehensive experimental analysis by adapting several AL methods for the task. AL-GTD outperforms AL competitors, simultaneously exhibiting superior performance compared to SOTA gaze target detectors when all are trained within a low-data regime. Code is available at https://github.com/francescotonini/al-gtd.",
    "link": "https://arxiv.org/abs/2409.18561",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/francescotonini/al-gtd."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Cross-video Identity Correlating for Person Re-identification Pre-training",
    "summary": "arXiv:2409.18569v1 Announce Type: new \nAbstract: Recent researches have proven that pre-training on large-scale person images extracted from internet videos is an effective way in learning better representations for person re-identification. However, these researches are mostly confined to pre-training at the instance-level or single-video tracklet-level. They ignore the identity-invariance in images of the same person across different videos, which is a key focus in person re-identification. To address this issue, we propose a Cross-video Identity-cOrrelating pre-traiNing (CION) framework. Defining a noise concept that comprehensively considers both intra-identity consistency and inter-identity discrimination, CION seeks the identity correlation from cross-video images by modeling it as a progressive multi-level denoising problem. Furthermore, an identity-guided self-distillation loss is proposed to implement better large-scale pre-training by mining the identity-invariance within person images. We conduct extensive experiments to verify the superiority of our CION in terms of efficiency and performance. CION achieves significantly leading performance with even fewer training samples. For example, compared with the previous state-of-the-art~\\cite{ISR}, CION with the same ResNet50-IBN achieves higher mAP of 93.3\\% and 74.3\\% on Market1501 and MSMT17, while only utilizing 8\\% training samples. Finally, with CION demonstrating superior model-agnostic ability, we contribute a model zoo named ReIDZoo to meet diverse research and application needs in this field. It contains a series of CION pre-trained models with spanning structures and parameters, totaling 32 models with 10 different structures, including GhostNet, ConvNext, RepViT, FastViT and so on. The code and models will be made publicly available at https://github.com/Zplusdragon/CION_ReIDZoo.",
    "link": "https://arxiv.org/abs/2409.18569",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Zplusdragon/CION_ReIDZoo."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Off to new Shores: A Dataset & Benchmark for (near-)coastal Flood Inundation Forecasting",
    "summary": "arXiv:2409.18591v1 Announce Type: new \nAbstract: Floods are among the most common and devastating natural hazards, imposing immense costs on our society and economy due to their disastrous consequences. Recent progress in weather prediction and spaceborne flood mapping demonstrated the feasibility of anticipating extreme events and reliably detecting their catastrophic effects afterwards. However, these efforts are rarely linked to one another and there is a critical lack of datasets and benchmarks to enable the direct forecasting of flood extent. To resolve this issue, we curate a novel dataset enabling a timely prediction of flood extent. Furthermore, we provide a representative evaluation of state-of-the-art methods, structured into two benchmark tracks for forecasting flood inundation maps i) in general and ii) focused on coastal regions. Altogether, our dataset and benchmark provide a comprehensive platform for evaluating flood forecasts, enabling future solutions for this critical challenge. Data, code & models are shared at https://github.com/Multihuntr/GFF under a CC0 license.",
    "link": "https://arxiv.org/abs/2409.18591",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Multihuntr/GFF"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "When SAM2 Meets Video Camouflaged Object Segmentation: A Comprehensive Evaluation and Adaptation",
    "summary": "arXiv:2409.18653v1 Announce Type: new \nAbstract: This study investigates the application and performance of the Segment Anything Model 2 (SAM2) in the challenging task of video camouflaged object segmentation (VCOS). VCOS involves detecting objects that blend seamlessly in the surroundings for videos, due to similar colors and textures, poor light conditions, etc. Compared to the objects in normal scenes, camouflaged objects are much more difficult to detect. SAM2, a video foundation model, has shown potential in various tasks. But its effectiveness in dynamic camouflaged scenarios remains under-explored. This study presents a comprehensive study on SAM2's ability in VCOS. First, we assess SAM2's performance on camouflaged video datasets using different models and prompts (click, box, and mask). Second, we explore the integration of SAM2 with existing multimodal large language models (MLLMs) and VCOS methods. Third, we specifically adapt SAM2 by fine-tuning it on the video camouflaged dataset. Our comprehensive experiments demonstrate that SAM2 has excellent zero-shot ability of detecting camouflaged objects in videos. We also show that this ability could be further improved by specifically adjusting SAM2's parameters for VCOS. The code will be available at https://github.com/zhoustan/SAM2-VCOS",
    "link": "https://arxiv.org/abs/2409.18653",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/zhoustan/SAM2-VCOS"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "YOLOv8-ResCBAM: YOLOv8 Based on An Effective Attention Module for Pediatric Wrist Fracture Detection",
    "summary": "arXiv:2409.18826v1 Announce Type: new \nAbstract: Wrist trauma and even fractures occur frequently in daily life, particularly among children who account for a significant proportion of fracture cases. Before performing surgery, surgeons often request patients to undergo X-ray imaging first, and prepare for the surgery based on the analysis of the X-ray images. With the development of neural networks, You Only Look Once (YOLO) series models have been widely used in fracture detection for Computer-Assisted Diagnosis, where the YOLOv8 model has obtained the satisfactory results. Applying the attention modules to neural networks is one of the effective methods to improve the model performance. This paper proposes YOLOv8-ResCBAM, which incorporates Convolutional Block Attention Module integrated with resblock (ResCBAM) into the original YOLOv8 network architecture. The experimental results on the GRAZPEDWRI-DX dataset demonstrate that the mean Average Precision calculated at Intersection over Union threshold of 0.5 (mAP 50) of the proposed model increased from 63.6% of the original YOLOv8 model to 65.8%, which achieves the state-of-the-art performance. The implementation code is available at https://github.com/RuiyangJu/Fracture_Detection_Improved_YOLOv8.",
    "link": "https://arxiv.org/abs/2409.18826",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/RuiyangJu/Fracture_Detection_Improved_YOLOv8."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "MinerU: An Open-Source Solution for Precise Document Content Extraction",
    "summary": "arXiv:2409.18839v1 Announce Type: new \nAbstract: Document content analysis has been a crucial research area in computer vision. Despite significant advancements in methods such as OCR, layout detection, and formula recognition, existing open-source solutions struggle to consistently deliver high-quality content extraction due to the diversity in document types and content. To address these challenges, we present MinerU, an open-source solution for high-precision document content extraction. MinerU leverages the sophisticated PDF-Extract-Kit models to extract content from diverse documents effectively and employs finely-tuned preprocessing and postprocessing rules to ensure the accuracy of the final results. Experimental results demonstrate that MinerU consistently achieves high performance across various document types, significantly enhancing the quality and consistency of content extraction. The MinerU open-source project is available at https://github.com/opendatalab/MinerU.",
    "link": "https://arxiv.org/abs/2409.18839",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/opendatalab/MinerU."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "LW2G: Learning Whether to Grow for Prompt-based Continual Learning",
    "summary": "arXiv:2409.18860v1 Announce Type: new \nAbstract: Continual Learning (CL) aims to learn in non-stationary scenarios, progressively acquiring and maintaining knowledge from sequential tasks. Recent Prompt-based Continual Learning (PCL) has achieved remarkable performance with Pre-Trained Models (PTMs). These approaches grow a prompt sets pool by adding a new set of prompts when learning each new task (\\emph{prompt learning}) and adopt a matching mechanism to select the correct set for each testing sample (\\emph{prompt retrieval}). Previous studies focus on the latter stage by improving the matching mechanism to enhance Prompt Retrieval Accuracy (PRA). To promote cross-task knowledge facilitation and form an effective and efficient prompt sets pool, we propose a plug-in module in the former stage to \\textbf{Learn Whether to Grow (LW2G)} based on the disparities between tasks. Specifically, a shared set of prompts is utilized when several tasks share certain commonalities, and a new set is added when there are significant differences between the new task and previous tasks. Inspired by Gradient Projection Continual Learning, our LW2G develops a metric called Hinder Forward Capability (HFC) to measure the hindrance imposed on learning new tasks by surgically modifying the original gradient onto the orthogonal complement of the old feature space. With HFC, an automated scheme Dynamic Growing Approach adaptively learns whether to grow with a dynamic threshold. Furthermore, we design a gradient-based constraint to ensure the consistency between the updating prompts and pre-trained knowledge, and a prompts weights reusing strategy to enhance forward transfer. Extensive experiments show the effectiveness of our method. The source codes are available at \\url{https://github.com/RAIAN08/LW2G}.",
    "link": "https://arxiv.org/abs/2409.18860",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/RAIAN08/LW2G}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Toward Efficient Deep Blind RAW Image Restoration",
    "summary": "arXiv:2409.18204v1 Announce Type: cross \nAbstract: Multiple low-vision tasks such as denoising, deblurring and super-resolution depart from RGB images and further reduce the degradations, improving the quality. However, modeling the degradations in the sRGB domain is complicated because of the Image Signal Processor (ISP) transformations. Despite of this known issue, very few methods in the literature work directly with sensor RAW images. In this work we tackle image restoration directly in the RAW domain. We design a new realistic degradation pipeline for training deep blind RAW restoration models. Our pipeline considers realistic sensor noise, motion blur, camera shake, and other common degradations. The models trained with our pipeline and data from multiple sensors, can successfully reduce noise and blur, and recover details in RAW images captured from different cameras. To the best of our knowledge, this is the most exhaustive analysis on RAW image restoration. Code available at https://github.com/mv-lab/AISP",
    "link": "https://arxiv.org/abs/2409.18204",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/mv-lab/AISP"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Task-recency bias strikes back: Adapting covariances in Exemplar-Free Class Incremental Learning",
    "summary": "arXiv:2409.18265v1 Announce Type: cross \nAbstract: Exemplar-Free Class Incremental Learning (EFCIL) tackles the problem of training a model on a sequence of tasks without access to past data. Existing state-of-the-art methods represent classes as Gaussian distributions in the feature extractor's latent space, enabling Bayes classification or training the classifier by replaying pseudo features. However, we identify two critical issues that compromise their efficacy when the feature extractor is updated on incremental tasks. First, they do not consider that classes' covariance matrices change and must be adapted after each task. Second, they are susceptible to a task-recency bias caused by dimensionality collapse occurring during training. In this work, we propose AdaGauss -- a novel method that adapts covariance matrices from task to task and mitigates the task-recency bias owing to the additional anti-collapse loss function. AdaGauss yields state-of-the-art results on popular EFCIL benchmarks and datasets when training from scratch or starting from a pre-trained backbone. The code is available at: https://github.com/grypesc/AdaGauss.",
    "link": "https://arxiv.org/abs/2409.18265",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/grypesc/AdaGauss."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "MultiClimate: Multimodal Stance Detection on Climate Change Videos",
    "summary": "arXiv:2409.18346v1 Announce Type: cross \nAbstract: Climate change (CC) has attracted increasing attention in NLP in recent years. However, detecting the stance on CC in multimodal data is understudied and remains challenging due to a lack of reliable datasets. To improve the understanding of public opinions and communication strategies, this paper presents MultiClimate, the first open-source manually-annotated stance detection dataset with $100$ CC-related YouTube videos and $4,209$ frame-transcript pairs. We deploy state-of-the-art vision and language models, as well as multimodal models for MultiClimate stance detection. Results show that text-only BERT significantly outperforms image-only ResNet50 and ViT. Combining both modalities achieves state-of-the-art, $0.747$/$0.749$ in accuracy/F1. Our 100M-sized fusion models also beat CLIP and BLIP, as well as the much larger 9B-sized multimodal IDEFICS and text-only Llama3 and Gemma2, indicating that multimodal stance detection remains challenging for large language models. Our code, dataset, as well as supplementary materials, are available at https://github.com/werywjw/MultiClimate.",
    "link": "https://arxiv.org/abs/2409.18346",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/werywjw/MultiClimate."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration",
    "summary": "arXiv:2409.18461v1 Announce Type: cross \nAbstract: Federated Learning has emerged as a promising paradigm for collaborative machine learning, while preserving user data privacy. Despite its potential, standard FL lacks support for diverse heterogeneous device prototypes, which vary significantly in model and dataset sizes -- from small IoT devices to large workstations. This limitation is only partially addressed by existing knowledge distillation techniques, which often fail to transfer knowledge effectively across a broad spectrum of device prototypes with varied capabilities. This failure primarily stems from two issues: the dilution of informative logits from more capable devices by those from less capable ones, and the use of a single integrated logits as the distillation target across all devices, which neglects their individual learning capacities and and the unique contributions of each. To address these challenges, we introduce TAKFL, a novel KD-based framework that treats the knowledge transfer from each device prototype's ensemble as a separate task, independently distilling each to preserve its unique contributions and avoid dilution. TAKFL also incorporates a KD-based self-regularization technique to mitigate the issues related to the noisy and unsupervised ensemble distillation process. To integrate the separately distilled knowledge, we introduce an adaptive task arithmetic knowledge integration process, allowing each student model to customize the knowledge integration for optimal performance. Additionally, we present theoretical results demonstrating the effectiveness of task arithmetic in transferring knowledge across heterogeneous devices with varying capacities. Comprehensive evaluations of our method across both CV and NLP tasks demonstrate that TAKFL achieves SOTA results in a variety of datasets and settings, significantly outperforming existing KD-based methods. Code is released at https://github.com/MMorafah/TAKFL",
    "link": "https://arxiv.org/abs/2409.18461",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/MMorafah/TAKFL"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "UniEmoX: Cross-modal Semantic-Guided Large-Scale Pretraining for Universal Scene Emotion Perception",
    "summary": "arXiv:2409.18877v1 Announce Type: cross \nAbstract: Visual emotion analysis holds significant research value in both computer vision and psychology. However, existing methods for visual emotion analysis suffer from limited generalizability due to the ambiguity of emotion perception and the diversity of data scenarios. To tackle this issue, we introduce UniEmoX, a cross-modal semantic-guided large-scale pretraining framework. Inspired by psychological research emphasizing the inseparability of the emotional exploration process from the interaction between individuals and their environment, UniEmoX integrates scene-centric and person-centric low-level image spatial structural information, aiming to derive more nuanced and discriminative emotional representations. By exploiting the similarity between paired and unpaired image-text samples, UniEmoX distills rich semantic knowledge from the CLIP model to enhance emotional embedding representations more effectively. To the best of our knowledge, this is the first large-scale pretraining framework that integrates psychological theories with contemporary contrastive learning and masked image modeling techniques for emotion analysis across diverse scenarios. Additionally, we develop a visual emotional dataset titled Emo8. Emo8 samples cover a range of domains, including cartoon, natural, realistic, science fiction and advertising cover styles, covering nearly all common emotional scenes. Comprehensive experiments conducted on six benchmark datasets across two downstream tasks validate the effectiveness of UniEmoX. The source code is available at https://github.com/chincharles/u-emo.",
    "link": "https://arxiv.org/abs/2409.18877",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/chincharles/u-emo."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "RoCOCO: Robustness Benchmark of MS-COCO to Stress-test Image-Text Matching Models",
    "summary": "arXiv:2304.10727v4 Announce Type: replace \nAbstract: With the extensive use of vision-language models in various downstream tasks, evaluating their robustness is crucial. In this paper, we propose a benchmark for assessing the robustness of vision-language models. We believe that a robust model should properly understand both linguistic and visual semantics and be resilient to explicit variations. In pursuit of this goal, we create new variants of texts and images in the MS-COCO test set and re-evaluate the state-of-the-art (SOTA) models with the new data. Specifically, we alter the meaning of text by replacing a word, and generate visually altered images that maintain some visual context while introducing noticeable pixel changes through image mixing techniques.Our evaluations on the proposed benchmark reveal substantial performance degradation in many SOTA models (e.g., Image-to-Text Recall@1: 81.9\\% $\\rightarrow$ 48.4\\% in BLIP, 66.1\\% $\\rightarrow$ 37.6\\% in VSE$\\infty$), with the models often favoring the altered texts/images over the original ones. This indicates the current vision-language models struggle with subtle changes and often fail to understand the overall context of texts and images. Based on these findings, we propose semantic contrastive loss and visual contrastive loss to learn more robust embedding. Datasets and code are available at {\\url{https://github.com/pseulki/rococo}}.",
    "link": "https://arxiv.org/abs/2304.10727",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/pseulki/rococo}}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "$\\texttt{NePhi}$: Neural Deformation Fields for Approximately Diffeomorphic Medical Image Registration",
    "summary": "arXiv:2309.07322v3 Announce Type: replace \nAbstract: This work proposes NePhi, a generalizable neural deformation model which results in approximately diffeomorphic transformations. In contrast to the predominant voxel-based transformation fields used in learning-based registration approaches, NePhi represents deformations functionally, leading to great flexibility within the design space of memory consumption during training and inference, inference time, registration accuracy, as well as transformation regularity. Specifically, NePhi 1) requires less memory compared to voxel-based learning approaches, 2) improves inference speed by predicting latent codes, compared to current existing neural deformation based registration approaches that \\emph{only} rely on optimization, 3) improves accuracy via instance optimization, and 4) shows excellent deformation regularity which is highly desirable for medical image registration. We demonstrate the performance of NePhi on a 2D synthetic dataset as well as for real 3D medical image datasets (e.g., lungs and brains). Our results show that NePhi can match the accuracy of voxel-based representations in a single-resolution registration setting. For multi-resolution registration, our method matches the accuracy of current SOTA learning-based registration approaches with instance optimization while reducing memory requirements by a factor of five. Our code is available at https://github.com/uncbiag/NePhi.",
    "link": "https://arxiv.org/abs/2309.07322",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/uncbiag/NePhi."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Fast ODE-based Sampling for Diffusion Models in Around 5 Steps",
    "summary": "arXiv:2312.00094v3 Announce Type: replace \nAbstract: Sampling from diffusion models can be treated as solving the corresponding ordinary differential equations (ODEs), with the aim of obtaining an accurate solution with as few number of function evaluations (NFE) as possible. Recently, various fast samplers utilizing higher-order ODE solvers have emerged and achieved better performance than the initial first-order one. However, these numerical methods inherently result in certain approximation errors, which significantly degrades sample quality with extremely small NFE (e.g., around 5). In contrast, based on the geometric observation that each sampling trajectory almost lies in a two-dimensional subspace embedded in the ambient space, we propose Approximate MEan-Direction Solver (AMED-Solver) that eliminates truncation errors by directly learning the mean direction for fast diffusion sampling. Besides, our method can be easily used as a plugin to further improve existing ODE-based samplers. Extensive experiments on image synthesis with the resolution ranging from 32 to 512 demonstrate the effectiveness of our method. With only 5 NFE, we achieve 6.61 FID on CIFAR-10, 10.74 FID on ImageNet 64$\\times$64, and 13.20 FID on LSUN Bedroom. Our code is available at https://github.com/zju-pi/diff-sampler.",
    "link": "https://arxiv.org/abs/2312.00094",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/zju-pi/diff-sampler."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "GenFace: A Large-Scale Fine-Grained Face Forgery Benchmark and Cross Appearance-Edge Learning",
    "summary": "arXiv:2402.02003v4 Announce Type: replace \nAbstract: The rapid advancement of photorealistic generators has reached a critical juncture where the discrepancy between authentic and manipulated images is increasingly indistinguishable. Thus, benchmarking and advancing techniques detecting digital manipulation become an urgent issue. Although there have been a number of publicly available face forgery datasets, the forgery faces are mostly generated using GAN-based synthesis technology, which does not involve the most recent technologies like diffusion. The diversity and quality of images generated by diffusion models have been significantly improved and thus a much more challenging face forgery dataset shall be used to evaluate SOTA forgery detection literature. In this paper, we propose a large-scale, diverse, and fine-grained high-fidelity dataset, namely GenFace, to facilitate the advancement of deepfake detection, which contains a large number of forgery faces generated by advanced generators such as the diffusion-based model and more detailed labels about the manipulation approaches and adopted generators. In addition to evaluating SOTA approaches on our benchmark, we design an innovative cross appearance-edge learning (CAEL) detector to capture multi-grained appearance and edge global representations, and detect discriminative and general forgery traces. Moreover, we devise an appearance-edge cross-attention (AECA) module to explore the various integrations across two domains. Extensive experiment results and visualizations show that our detection model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations. Code and datasets will be available at \\url{https://github.com/Jenine-321/GenFace",
    "link": "https://arxiv.org/abs/2402.02003",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Jenine-321/GenFace"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free Efficient Vision Transformer",
    "summary": "arXiv:2405.03882v2 Announce Type: replace \nAbstract: Motivated by the huge success of Transformers in the field of natural language processing (NLP), Vision Transformers (ViTs) have been rapidly developed and achieved remarkable performance in various computer vision tasks. However, their huge model sizes and intensive computations hinder ViTs' deployment on embedded devices, calling for effective model compression methods, such as quantization. Unfortunately, due to the existence of hardware-unfriendly and quantization-sensitive non-linear operations, particularly {Softmax}, it is non-trivial to completely quantize all operations in ViTs, yielding either significant accuracy drops or non-negligible hardware costs. In response to challenges associated with \\textit{standard ViTs}, we focus our attention towards the quantization and acceleration for \\textit{efficient ViTs}, which not only eliminate the troublesome Softmax but also integrate linear attention with low computational complexity, and propose Trio-ViT accordingly. Specifically, at the algorithm level, we develop a {tailored post-training quantization engine} taking the unique activation distributions of Softmax-free efficient ViTs into full consideration, aiming to boost quantization accuracy. Furthermore, at the hardware level, we build an accelerator dedicated to the specific Convolution-Transformer hybrid architecture of efficient ViTs, thereby enhancing hardware efficiency. Extensive experimental results consistently prove the effectiveness of our Trio-ViT framework. {Particularly, we can gain up to $\\uparrow$$\\mathbf{3.6}\\times$, $\\uparrow$$\\mathbf{5.0}\\times$, and $\\uparrow$$\\mathbf{7.3}\\times$ FPS under comparable accuracy over state-of-the-art ViT accelerators, as well as $\\uparrow$$\\mathbf{6.0}\\times$, $\\uparrow$$\\mathbf{1.5}\\times$, and $\\uparrow$$\\mathbf{2.1}\\times$ DSP efficiency.} Codes are available at \\url{https://github.com/shihuihong214/Trio-ViT}.",
    "link": "https://arxiv.org/abs/2405.03882",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/shihuihong214/Trio-ViT}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Beyond Raw Videos: Understanding Edited Videos with Large Multimodal Model",
    "summary": "arXiv:2406.10484v2 Announce Type: replace \nAbstract: The emerging video LMMs (Large Multimodal Models) have achieved significant improvements on generic video understanding in the form of VQA (Visual Question Answering), where the raw videos are captured by cameras. However, a large portion of videos in real-world applications are edited videos, \\textit{e.g.}, users usually cut and add effects/modifications to the raw video before publishing it on social media platforms. The edited videos usually have high view counts but they are not covered in existing benchmarks of video LMMs, \\textit{i.e.}, ActivityNet-QA, or VideoChatGPT benchmark. In this paper, we leverage the edited videos on a popular short video platform, \\textit{i.e.}, TikTok, and build a video VQA benchmark (named EditVid-QA) covering four typical editing categories, i.e., effect, funny, meme, and game. Funny and meme videos benchmark nuanced understanding and high-level reasoning, while effect and game evaluate the understanding capability of artificial design. Most of the open-source video LMMs perform poorly on the EditVid-QA benchmark, indicating a huge domain gap between edited short videos on social media and regular raw videos. To improve the generalization ability of LMMs, we collect a training set for the proposed benchmark based on both Panda-70M/WebVid raw videos and small-scale TikTok/CapCut edited videos, which boosts the performance on the proposed EditVid-QA benchmark, indicating the effectiveness of high-quality training data. We also identified a serious issue in the existing evaluation protocol using the GPT-3.5 judge, namely a \"sorry\" attack, where a sorry-style naive answer can achieve an extremely high rating from the GPT judge, e.g., over 4.3 for correctness score on VideoChatGPT evaluation protocol. To avoid the \"sorry\" attacks, we evaluate results with GPT-4 judge and keyword filtering. The dataset is released at https://github.com/XenonLamb/EditVid-QA.",
    "link": "https://arxiv.org/abs/2406.10484",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/XenonLamb/EditVid-QA."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Ultra-High-Definition Image Restoration: New Benchmarks and A Dual Interaction Prior-Driven Solution",
    "summary": "arXiv:2406.13607v3 Announce Type: replace \nAbstract: Ultra-High-Definition (UHD) image restoration has acquired remarkable attention due to its practical demand. In this paper, we construct UHD snow and rain benchmarks, named UHD-Snow and UHD-Rain, to remedy the deficiency in this field. The UHD-Snow/UHD-Rain is established by simulating the physics process of rain/snow into consideration and each benchmark contains 3200 degraded/clear image pairs of 4K resolution. Furthermore, we propose an effective UHD image restoration solution by considering gradient and normal priors in model design thanks to these priors' spatial and detail contributions. Specifically, our method contains two branches: (a) feature fusion and reconstruction branch in high-resolution space and (b) prior feature interaction branch in low-resolution space. The former learns high-resolution features and fuses prior-guided low-resolution features to reconstruct clear images, while the latter utilizes normal and gradient priors to mine useful spatial features and detail features to guide high-resolution recovery better. To better utilize these priors, we introduce single prior feature interaction and dual prior feature interaction, where the former respectively fuses normal and gradient priors with high-resolution features to enhance prior ones, while the latter calculates the similarity between enhanced prior ones and further exploits dual guided filtering to boost the feature interaction of dual priors. We conduct experiments on both new and existing public datasets and demonstrate the state-of-the-art performance of our method on UHD image low-light enhancement, dehazing, deblurring, desonwing, and deraining. The source codes and benchmarks are available at \\url{https://github.com/wlydlut/UHDDIP}.",
    "link": "https://arxiv.org/abs/2406.13607",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/wlydlut/UHDDIP}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Perception-Guided Quality Metric of 3D Point Clouds Using Hybrid Strategy",
    "summary": "arXiv:2407.03885v2 Announce Type: replace \nAbstract: Full-reference point cloud quality assessment (FR-PCQA) aims to infer the quality of distorted point clouds with available references. Most of the existing FR-PCQA metrics ignore the fact that the human visual system (HVS) dynamically tackles visual information according to different distortion levels (i.e., distortion detection for high-quality samples and appearance perception for low-quality samples) and measure point cloud quality using unified features. To bridge the gap, in this paper, we propose a perception-guided hybrid metric (PHM) that adaptively leverages two visual strategies with respect to distortion degree to predict point cloud quality: to measure visible difference in high-quality samples, PHM takes into account the masking effect and employs texture complexity as an effective compensatory factor for absolute difference; on the other hand, PHM leverages spectral graph theory to evaluate appearance degradation in low-quality samples. Variations in geometric signals on graphs and changes in the spectral graph wavelet coefficients are utilized to characterize geometry and texture appearance degradation, respectively. Finally, the results obtained from the two components are combined in a non-linear method to produce an overall quality score of the tested point cloud. The results of the experiment on five independent databases show that PHM achieves state-of-the-art (SOTA) performance and offers significant performance improvement in multiple distortion environments. The code is publicly available at https://github.com/zhangyujie-1998/PHM.",
    "link": "https://arxiv.org/abs/2407.03885",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/zhangyujie-1998/PHM."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models",
    "summary": "arXiv:2408.10571v3 Announce Type: replace \nAbstract: Diffusion models have revolutionized customized text-to-image generation, allowing for efficient synthesis of photos from personal data with textual descriptions. However, these advancements bring forth risks including privacy breaches and unauthorized replication of artworks. Previous researches primarily center around using prompt-specific methods to generate adversarial examples to protect personal images, yet the effectiveness of existing methods is hindered by constrained adaptability to different prompts. In this paper, we introduce a Prompt-Agnostic Adversarial Perturbation (PAP) method for customized diffusion models. PAP first models the prompt distribution using a Laplace Approximation, and then produces prompt-agnostic perturbations by maximizing a disturbance expectation based on the modeled distribution. This approach effectively tackles the prompt-agnostic attacks, leading to improved defense stability. Extensive experiments in face privacy and artistic style protection, demonstrate the superior generalization of PAP in comparison to existing techniques. Our project page is available at https://github.com/vancyland/Prompt-Agnostic-Adversarial-Perturbation-for-Customized-Diffusion-Models.github.io.",
    "link": "https://arxiv.org/abs/2408.10571",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/vancyland/Prompt-Agnostic-Adversarial-Perturbation-for-Customized-Diffusion-Models.github.io."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing",
    "summary": "arXiv:2408.14180v2 Announce Type: replace \nAbstract: Significant progress has been made in the field of Instruction-based Image Editing (IIE). However, evaluating these models poses a significant challenge. A crucial requirement in this field is the establishment of a comprehensive evaluation benchmark for accurately assessing editing results and providing valuable insights for its further development. In response to this need, we propose I2EBench, a comprehensive benchmark designed to automatically evaluate the quality of edited images produced by IIE models from multiple dimensions. I2EBench consists of 2,000+ images for editing, along with 4,000+ corresponding original and diverse instructions. It offers three distinctive characteristics: 1) Comprehensive Evaluation Dimensions: I2EBench comprises 16 evaluation dimensions that cover both high-level and low-level aspects, providing a comprehensive assessment of each IIE model. 2) Human Perception Alignment: To ensure the alignment of our benchmark with human perception, we conducted an extensive user study for each evaluation dimension. 3) Valuable Research Insights: By analyzing the advantages and disadvantages of existing IIE models across the 16 dimensions, we offer valuable research insights to guide future development in the field. We will open-source I2EBench, including all instructions, input images, human annotations, edited images from all evaluated methods, and a simple script for evaluating the results from new IIE models. The code, dataset and generated images from all IIE models are provided in github: https://github.com/cocoshe/I2EBench.",
    "link": "https://arxiv.org/abs/2408.14180",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/cocoshe/I2EBench."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "A preliminary study on continual learning in computer vision using Kolmogorov-Arnold Networks",
    "summary": "arXiv:2409.13550v2 Announce Type: replace \nAbstract: Deep learning has long been dominated by multi-layer perceptrons (MLPs), which have demonstrated superiority over other optimizable models in various domains. Recently, a new alternative to MLPs has emerged - Kolmogorov-Arnold Networks (KAN)- which are based on a fundamentally different mathematical framework. According to their authors, KANs address several major issues in MLPs, such as catastrophic forgetting in continual learning scenarios. However, this claim has only been supported by results from a regression task on a toy 1D dataset. In this paper, we extend the investigation by evaluating the performance of KANs in continual learning tasks within computer vision, specifically using the MNIST datasets. To this end, we conduct a structured analysis of the behavior of MLPs and two KAN-based models in a class-incremental learning scenario, ensuring that the architectures involved have the same number of trainable parameters. Our results demonstrate that an efficient version of KAN outperforms both traditional MLPs and the original KAN implementation. We further analyze the influence of hyperparameters in MLPs and KANs, as well as the impact of certain trainable parameters in KANs, such as bias and scale weights. Additionally, we provide a preliminary investigation of recent KAN-based convolutional networks and compare their performance with that of traditional convolutional neural networks. Our codes can be found at https://github.com/MrPio/KAN-Continual_Learning_tests.",
    "link": "https://arxiv.org/abs/2409.13550",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/MrPio/KAN-Continual_Learning_tests."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "SpikeGS: Learning 3D Gaussian Fields from Continuous Spike Stream",
    "summary": "arXiv:2409.15176v2 Announce Type: replace \nAbstract: A spike camera is a specialized high-speed visual sensor that offers advantages such as high temporal resolution and high dynamic range compared to conventional frame cameras. These features provide the camera with significant advantages in many computer vision tasks. However, the tasks of 3D reconstruction and novel view synthesis based on spike cameras remain underdeveloped. Although there are existing methods for learning neural radiance fields from spike stream, they either lack robustness in extremely noisy, low-quality lighting conditions or suffer from high computational complexity due to the deep fully connected neural networks and ray marching rendering strategies used in neural radiance fields, making it difficult to recover fine texture details. In contrast, the latest advancements in 3DGS have achieved high-quality real-time rendering by optimizing the point cloud representation into Gaussian ellipsoids. Building on this, we introduce SpikeGS, the first method to learn 3D Gaussian fields solely from spike stream. We designed a differentiable spike stream rendering framework based on 3DGS, incorporating noise embedding and spiking neurons. By leveraging the multi-view consistency of 3DGS and the tile-based multi-threaded parallel rendering mechanism, we achieved high-quality real-time rendering results. Additionally, we introduced a spike rendering loss function that generalizes under varying illumination conditions. Our method can reconstruct view synthesis results with fine texture details from a continuous spike stream captured by a moving spike camera, while demonstrating high robustness in extremely noisy low-light scenarios. Experimental results on both real and synthetic datasets demonstrate that our method surpasses existing approaches in terms of rendering quality and speed. Our code will be available at https://github.com/520jz/SpikeGS.",
    "link": "https://arxiv.org/abs/2409.15176",
    "published": "Mon, 30 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/520jz/SpikeGS."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  }
]