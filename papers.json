[
  {
    "title": "QUB-PHEO: A Visual-Based Dyadic Multi-View Dataset for Intention Inference in Collaborative Assembly",
    "summary": "arXiv:2409.15560v1 Announce Type: new \nAbstract: QUB-PHEO introduces a visual-based, dyadic dataset with the potential of advancing human-robot interaction (HRI) research in assembly operations and intention inference. This dataset captures rich multimodal interactions between two participants, one acting as a 'robot surrogate,' across a variety of assembly tasks that are further broken down into 36 distinct subtasks. With rich visual annotations, such as facial landmarks, gaze, hand movements, object localization, and more for 70 participants, QUB-PHEO offers two versions: full video data for 50 participants and visual cues for all 70. Designed to improve machine learning models for HRI, QUB-PHEO enables deeper analysis of subtle interaction cues and intentions, promising contributions to the field. The dataset will be available at https://github.com/exponentialR/QUB-PHEO subject to an End-User License Agreement (EULA).",
    "link": "https://arxiv.org/abs/2409.15560",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/exponentialR/QUB-PHEO"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "FACET: Fast and Accurate Event-Based Eye Tracking Using Ellipse Modeling for Extended Reality",
    "summary": "arXiv:2409.15584v1 Announce Type: new \nAbstract: Eye tracking is a key technology for gaze-based interactions in Extended Reality (XR), but traditional frame-based systems struggle to meet XR's demands for high accuracy, low latency, and power efficiency. Event cameras offer a promising alternative due to their high temporal resolution and low power consumption. In this paper, we present FACET (Fast and Accurate Event-based Eye Tracking), an end-to-end neural network that directly outputs pupil ellipse parameters from event data, optimized for real-time XR applications. The ellipse output can be directly used in subsequent ellipse-based pupil trackers. We enhance the EV-Eye dataset by expanding annotated data and converting original mask labels to ellipse-based annotations to train the model. Besides, a novel trigonometric loss is adopted to address angle discontinuities and a fast causal event volume event representation method is put forward. On the enhanced EV-Eye test set, FACET achieves an average pupil center error of 0.20 pixels and an inference time of 0.53 ms, reducing pixel error and inference time by 1.6$\\times$ and 1.8$\\times$ compared to the prior art, EV-Eye, with 4.4$\\times$ and 11.7$\\times$ less parameters and arithmetic operations. The code is available at https://github.com/DeanJY/FACET.",
    "link": "https://arxiv.org/abs/2409.15584",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/DeanJY/FACET."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "KISS-Matcher: Fast and Robust Point Cloud Registration Revisited",
    "summary": "arXiv:2409.15615v1 Announce Type: new \nAbstract: While global point cloud registration systems have advanced significantly in all aspects, many studies have focused on specific components, such as feature extraction, graph-theoretic pruning, or pose solvers. In this paper, we take a holistic view on the registration problem and develop an open-source and versatile C++ library for point cloud registration, called \\textit{KISS-Matcher}. KISS-Matcher combines a novel feature detector, \\textit{Faster-PFH}, that improves over the classical fast point feature histogram (FPFH). Moreover, it adopts a $k$-core-based graph-theoretic pruning to reduce the time complexity of rejecting outlier correspondences. Finally, it combines these modules in a complete, user-friendly, and ready-to-use pipeline. As verified by extensive experiments, KISS-Matcher has superior scalability and broad applicability, achieving a substantial speed-up compared to state-of-the-art outlier-robust registration pipelines while preserving accuracy. Our code will be available at \\href{https://github.com/MIT-SPARK/KISS-Matcher}{\\texttt{https://github.com/MIT-SPARK/KISS-Matcher}}.",
    "link": "https://arxiv.org/abs/2409.15615",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/MIT-SPARK/KISS-Matcher}{\\texttt{https://github.com/MIT-SPARK/KISS-Matcher}}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "ImPoster: Text and Frequency Guidance for Subject Driven Action Personalization using Diffusion Models",
    "summary": "arXiv:2409.15650v1 Announce Type: new \nAbstract: We present ImPoster, a novel algorithm for generating a target image of a 'source' subject performing a 'driving' action. The inputs to our algorithm are a single pair of a source image with the subject that we wish to edit and a driving image with a subject of an arbitrary class performing the driving action, along with the text descriptions of the two images. Our approach is completely unsupervised and does not require any access to additional annotations like keypoints or pose. Our approach builds on a pretrained text-to-image latent diffusion model and learns the characteristics of the source and the driving image by finetuning the diffusion model for a small number of iterations. At inference time, ImPoster performs step-wise text prompting i.e. it denoises by first moving in the direction of the image manifold corresponding to the driving image followed by the direction of the image manifold corresponding to the text description of the desired target image. We propose a novel diffusion guidance formulation, image frequency guidance, to steer the generation towards the manifold of the source subject and the driving action at every step of the inference denoising. Our frequency guidance formulations are derived from the frequency domain properties of images. We extensively evaluate ImPoster on a diverse set of source-driving image pairs to demonstrate improvements over baselines. To the best of our knowledge, ImPoster is the first approach towards achieving both subject-driven as well as action-driven image personalization. Code and data is available at https://github.com/divyakraman/ImPosterDiffusion2024.",
    "link": "https://arxiv.org/abs/2409.15650",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/divyakraman/ImPosterDiffusion2024."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "PDT: Uav Target Detection Dataset for Pests and Diseases Tree",
    "summary": "arXiv:2409.15679v1 Announce Type: new \nAbstract: UAVs emerge as the optimal carriers for visual weed iden?tification and integrated pest and disease management in crops. How?ever, the absence of specialized datasets impedes the advancement of model development in this domain. To address this, we have developed the Pests and Diseases Tree dataset (PDT dataset). PDT dataset repre?sents the first high-precision UAV-based dataset for targeted detection of tree pests and diseases, which is collected in real-world operational environments and aims to fill the gap in available datasets for this field. Moreover, by aggregating public datasets and network data, we further introduced the Common Weed and Crop dataset (CWC dataset) to ad?dress the challenge of inadequate classification capabilities of test models within datasets for this field. Finally, we propose the YOLO-Dense Pest (YOLO-DP) model for high-precision object detection of weed, pest, and disease crop images. We re-evaluate the state-of-the-art detection models with our proposed PDT dataset and CWC dataset, showing the completeness of the dataset and the effectiveness of the YOLO-DP. The proposed PDT dataset, CWC dataset, and YOLO-DP model are pre?sented at https://github.com/RuiXing123/PDT_CWC_YOLO-DP.",
    "link": "https://arxiv.org/abs/2409.15679",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/RuiXing123/PDT_CWC_YOLO-DP."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "LaPose: Laplacian Mixture Shape Modeling for RGB-Based Category-Level Object Pose Estimation",
    "summary": "arXiv:2409.15727v1 Announce Type: new \nAbstract: While RGBD-based methods for category-level object pose estimation hold promise, their reliance on depth data limits their applicability in diverse scenarios. In response, recent efforts have turned to RGB-based methods; however, they face significant challenges stemming from the absence of depth information. On one hand, the lack of depth exacerbates the difficulty in handling intra-class shape variation, resulting in increased uncertainty in shape predictions. On the other hand, RGB-only inputs introduce inherent scale ambiguity, rendering the estimation of object size and translation an ill-posed problem. To tackle these challenges, we propose LaPose, a novel framework that models the object shape as the Laplacian mixture model for Pose estimation. By representing each point as a probabilistic distribution, we explicitly quantify the shape uncertainty. LaPose leverages both a generalized 3D information stream and a specialized feature stream to independently predict the Laplacian distribution for each point, capturing different aspects of object geometry. These two distributions are then integrated as a Laplacian mixture model to establish the 2D-3D correspondences, which are utilized to solve the pose via the PnP module. In order to mitigate scale ambiguity, we introduce a scale-agnostic representation for object size and translation, enhancing training efficiency and overall robustness. Extensive experiments on the NOCS datasets validate the effectiveness of LaPose, yielding state-of-the-art performance in RGB-based category-level object pose estimation. Codes are released at https://github.com/lolrudy/LaPose",
    "link": "https://arxiv.org/abs/2409.15727",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/lolrudy/LaPose"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Mind the Prompt: A Novel Benchmark for Prompt-based Class-Agnostic Counting",
    "summary": "arXiv:2409.15953v1 Announce Type: new \nAbstract: Class-agnostic counting (CAC) is a recent task in computer vision that aims to estimate the number of instances of arbitrary object classes never seen during model training. With the recent advancement of robust vision-and-language foundation models, there is a growing interest in prompt-based CAC, where object categories to be counted can be specified using natural language. However, we identify significant limitations in current benchmarks for evaluating this task, which hinder both accurate assessment and the development of more effective solutions. Specifically, we argue that the current evaluation protocols do not measure the ability of the model to understand which object has to be counted. This is due to two main factors: (i) the shortcomings of CAC datasets, which primarily consist of images containing objects from a single class, and (ii) the limitations of current counting performance evaluators, which are based on traditional class-specific counting and focus solely on counting errors. To fill this gap, we introduce the Prompt-Aware Counting (PrACo) benchmark, which comprises two targeted tests, each accompanied by appropriate evaluation metrics. We evaluate state-of-the-art methods and demonstrate that, although some achieve impressive results on standard class-specific counting metrics, they exhibit a significant deficiency in understanding the input prompt, indicating the need for more careful training procedures or revised designs. The code for reproducing our results is available at https://github.com/ciampluca/PrACo.",
    "link": "https://arxiv.org/abs/2409.15953",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ciampluca/PrACo."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Benchmarking Robustness of Endoscopic Depth Estimation with Synthetically Corrupted Data",
    "summary": "arXiv:2409.16063v1 Announce Type: new \nAbstract: Accurate depth perception is crucial for patient outcomes in endoscopic surgery, yet it is compromised by image distortions common in surgical settings. To tackle this issue, our study presents a benchmark for assessing the robustness of endoscopic depth estimation models. We have compiled a comprehensive dataset that reflects real-world conditions, incorporating a range of synthetically induced corruptions at varying severity levels. To further this effort, we introduce the Depth Estimation Robustness Score (DERS), a novel metric that combines measures of error, accuracy, and robustness to meet the multifaceted requirements of surgical applications. This metric acts as a foundational element for evaluating performance, establishing a new paradigm for the comparative analysis of depth estimation technologies. Additionally, we set forth a benchmark focused on robustness for the evaluation of depth estimation in endoscopic surgery, with the aim of driving progress in model refinement. A thorough analysis of two monocular depth estimation models using our framework reveals crucial information about their reliability under adverse conditions. Our results emphasize the essential need for algorithms that can tolerate data corruption, thereby advancing discussions on improving model robustness. The impact of this research transcends theoretical frameworks, providing concrete gains in surgical precision and patient safety. This study establishes a benchmark for the robustness of depth estimation and serves as a foundation for developing more resilient surgical support technologies. Code is available at https://github.com/lofrienger/EndoDepthBenchmark.",
    "link": "https://arxiv.org/abs/2409.16063",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/lofrienger/EndoDepthBenchmark."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "MM-CamObj: A Comprehensive Multimodal Dataset for Camouflaged Object Scenarios",
    "summary": "arXiv:2409.16084v1 Announce Type: new \nAbstract: Large visual-language models (LVLMs) have achieved great success in multiple applications. However, they still encounter challenges in complex scenes, especially those involving camouflaged objects. This is primarily due to the lack of samples related to camouflaged scenes in the training dataset. To mitigate this issue, we construct the MM-CamObj dataset for the first time, comprising two subsets: CamObj-Align and CamObj-Instruct. Specifically, CamObj-Align contains 11,363 image-text pairs, and it is designed for VL alignment and injecting rich knowledge of camouflaged scenes into LVLMs. CamObj-Instruct is collected for fine-tuning the LVLMs with improved instruction-following capabilities, and it includes 11,363 images and 68,849 conversations with diverse instructions. Based on the MM-CamObj dataset, we propose the CamObj-Llava, an LVLM specifically designed for addressing tasks in camouflaged scenes. To facilitate our model's effective acquisition of knowledge about camouflaged objects and scenes, we introduce a curriculum learning strategy with six distinct modes. Additionally, we construct the CamObj-Bench to evaluate the existing LVLMs' capabilities of understanding, recognition, localization and count in camouflage scenes. This benchmark includes 600 images and 7 tasks, with a total of 9,449 questions. Extensive experiments are conducted on the CamObj-Bench with CamObj-Llava, 8 existing open-source and 3 closed-source LVLMs. Surprisingly, the results indicate that our model achieves a 25.84% improvement in 4 out of 7 tasks compared to GPT-4o. Code and datasets will be available at https://github.com/JCruan519/MM-CamObj.",
    "link": "https://arxiv.org/abs/2409.16084",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/JCruan519/MM-CamObj."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "VisioPhysioENet: Multimodal Engagement Detection using Visual and Physiological Signals",
    "summary": "arXiv:2409.16126v1 Announce Type: new \nAbstract: This paper presents VisioPhysioENet, a novel multimodal system that leverages visual cues and physiological signals to detect learner engagement. It employs a two-level approach for visual feature extraction using the Dlib library for facial landmark extraction and the OpenCV library for further estimations. This is complemented by extracting physiological signals using the plane-orthogonal-to-skin method to assess cardiovascular activity. These features are integrated using advanced machine learning classifiers, enhancing the detection of various engagement levels. We rigorously evaluate VisioPhysioENet on the DAiSEE dataset, where it achieves an accuracy of 63.09%, demonstrating a superior ability to discern various levels of engagement compared to existing methodologies. The proposed system's code can be accessed at https://github.com/MIntelligence-Group/VisioPhysioENet.",
    "link": "https://arxiv.org/abs/2409.16126",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/MIntelligence-Group/VisioPhysioENet."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "MCTrack: A Unified 3D Multi-Object Tracking Framework for Autonomous Driving",
    "summary": "arXiv:2409.16149v1 Announce Type: new \nAbstract: This paper introduces MCTrack, a new 3D multi-object tracking method that achieves state-of-the-art (SOTA) performance across KITTI, nuScenes, and Waymo datasets. Addressing the gap in existing tracking paradigms, which often perform well on specific datasets but lack generalizability, MCTrack offers a unified solution. Additionally, we have standardized the format of perceptual results across various datasets, termed BaseVersion, facilitating researchers in the field of multi-object tracking (MOT) to concentrate on the core algorithmic development without the undue burden of data preprocessing. Finally, recognizing the limitations of current evaluation metrics, we propose a novel set that assesses motion information output, such as velocity and acceleration, crucial for downstream tasks. The source codes of the proposed method are available at this link: https://github.com/megvii-research/MCTrack}{https://github.com/megvii-research/MCTrack",
    "link": "https://arxiv.org/abs/2409.16149",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/megvii-research/MCTrack}{https://github.com/megvii-research/MCTrack"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "ComiCap: A VLMs pipeline for dense captioning of Comic Panels",
    "summary": "arXiv:2409.16159v1 Announce Type: new \nAbstract: The comic domain is rapidly advancing with the development of single- and multi-page analysis and synthesis models. Recent benchmarks and datasets have been introduced to support and assess models' capabilities in tasks such as detection (panels, characters, text), linking (character re-identification and speaker identification), and analysis of comic elements (e.g., dialog transcription). However, to provide a comprehensive understanding of the storyline, a model must not only extract elements but also understand their relationships and generate highly informative captions. In this work, we propose a pipeline that leverages Vision-Language Models (VLMs) to obtain dense, grounded captions. To construct our pipeline, we introduce an attribute-retaining metric that assesses whether all important attributes are identified in the caption. Additionally, we created a densely annotated test set to fairly evaluate open-source VLMs and select the best captioning model according to our metric. Our pipeline generates dense captions with bounding boxes that are quantitatively and qualitatively superior to those produced by specifically trained models, without requiring any additional training. Using this pipeline, we annotated over 2 million panels across 13,000 books, which will be available on the project page https://github.com/emanuelevivoli/ComiCap.",
    "link": "https://arxiv.org/abs/2409.16159",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/emanuelevivoli/ComiCap."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO",
    "summary": "arXiv:2409.16205v1 Announce Type: new \nAbstract: Accurate segmentation of prostate cancer histopathology images is crucial for diagnosis and treatment planning. This study presents a comparative analysis of three deep learning-based methods, Mamba, SAM, and YOLO, for segmenting prostate cancer histopathology images. We evaluated the performance of these models on two comprehensive datasets, Gleason 2019 and SICAPv2, using Dice score, precision, and recall metrics. Our results show that the High-order Vision Mamba UNet (H-vmunet) model outperforms the other two models, achieving the highest scores across all metrics on both datasets. The H-vmunet model's advanced architecture, which integrates high-order visual state spaces and 2D-selective-scan operations, enables efficient and sensitive lesion detection across different scales. Our study demonstrates the potential of the H-vmunet model for clinical applications and highlights the importance of robust validation and comparison of deep learning-based methods for medical image analysis. The findings of this study contribute to the development of accurate and reliable computer-aided diagnosis systems for prostate cancer. The code is available at http://github.com/alibdz/prostate-segmentation.",
    "link": "https://arxiv.org/abs/2409.16205",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "http://github.com/alibdz/prostate-segmentation."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Deep Learning for Precision Agriculture: Post-Spraying Evaluation and Deposition Estimation",
    "summary": "arXiv:2409.16213v1 Announce Type: new \nAbstract: Precision spraying evaluation requires automation primarily in post-spraying imagery. In this paper we propose an eXplainable Artificial Intelligence (XAI) computer vision pipeline to evaluate a precision spraying system post-spraying without the need for traditional agricultural methods. The developed system can semantically segment potential targets such as lettuce, chickweed, and meadowgrass and correctly identify if targets have been sprayed. Furthermore, this pipeline evaluates using a domain-specific Weakly Supervised Deposition Estimation task, allowing for class-specific quantification of spray deposit weights in {\\mu}L. Estimation of coverage rates of spray deposition in a class-wise manner allows for further understanding of effectiveness of precision spraying systems. Our study evaluates different Class Activation Mapping techniques, namely AblationCAM and ScoreCAM, to determine which is more effective and interpretable for these tasks. In the pipeline, inference-only feature fusion is used to allow for further interpretability and to enable the automation of precision spraying evaluation post-spray. Our findings indicate that a Fully Convolutional Network with an EfficientNet-B0 backbone and inference-only feature fusion achieves an average absolute difference in deposition values of 156.8 {\\mu}L across three classes in our test set. The dataset curated in this paper is publicly available at https://github.com/Harry-Rogers/PSIE",
    "link": "https://arxiv.org/abs/2409.16213",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Harry-Rogers/PSIE"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "BurstM: Deep Burst Multi-scale SR using Fourier Space with Optical Flow",
    "summary": "arXiv:2409.15384v1 Announce Type: cross \nAbstract: Multi frame super-resolution(MFSR) achieves higher performance than single image super-resolution (SISR), because MFSR leverages abundant information from multiple frames. Recent MFSR approaches adapt the deformable convolution network (DCN) to align the frames. However, the existing MFSR suffers from misalignments between the reference and source frames due to the limitations of DCN, such as small receptive fields and the predefined number of kernels. From these problems, existing MFSR approaches struggle to represent high-frequency information. To this end, we propose Deep Burst Multi-scale SR using Fourier Space with Optical Flow (BurstM). The proposed method estimates the optical flow offset for accurate alignment and predicts the continuous Fourier coefficient of each frame for representing high-frequency textures. In addition, we have enhanced the network flexibility by supporting various super-resolution (SR) scale factors with the unimodel. We demonstrate that our method has the highest performance and flexibility than the existing MFSR methods. Our source code is available at https://github.com/Egkang-Luis/burstm",
    "link": "https://arxiv.org/abs/2409.15384",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Egkang-Luis/burstm"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "ViKL: A Mammography Interpretation Framework via Multimodal Aggregation of Visual-knowledge-linguistic Features",
    "summary": "arXiv:2409.15744v1 Announce Type: cross \nAbstract: Mammography is the primary imaging tool for breast cancer diagnosis. Despite significant strides in applying deep learning to interpret mammography images, efforts that focus predominantly on visual features often struggle with generalization across datasets. We hypothesize that integrating additional modalities in the radiology practice, notably the linguistic features of reports and manifestation features embodying radiological insights, offers a more powerful, interpretable and generalizable representation. In this paper, we announce MVKL, the first multimodal mammography dataset encompassing multi-view images, detailed manifestations and reports. Based on this dataset, we focus on the challanging task of unsupervised pretraining and propose ViKL, a innovative framework that synergizes Visual, Knowledge, and Linguistic features. This framework relies solely on pairing information without the necessity for pathology labels, which are often challanging to acquire. ViKL employs a triple contrastive learning approach to merge linguistic and knowledge-based insights with visual data, enabling both inter-modality and intra-modality feature enhancement. Our research yields significant findings: 1) Integrating reports and manifestations with unsupervised visual pretraining, ViKL substantially enhances the pathological classification and fosters multimodal interactions. 2) Manifestations can introduce a novel hard negative sample selection mechanism. 3) The multimodal features demonstrate transferability across different datasets. 4) The multimodal pretraining approach curbs miscalibrations and crafts a high-quality representation space. The MVKL dataset and ViKL code are publicly available at https://github.com/wxwxwwxxx/ViKL to support a broad spectrum of future research.",
    "link": "https://arxiv.org/abs/2409.15744",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/wxwxwwxxx/ViKL"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "ManiNeg: Manifestation-guided Multimodal Pretraining for Mammography Classification",
    "summary": "arXiv:2409.15745v1 Announce Type: cross \nAbstract: Breast cancer is a significant threat to human health. Contrastive learning has emerged as an effective method to extract critical lesion features from mammograms, thereby offering a potent tool for breast cancer screening and analysis. A crucial aspect of contrastive learning involves negative sampling, where the selection of appropriate hard negative samples is essential for driving representations to retain detailed information about lesions. In contrastive learning, it is often assumed that features can sufficiently capture semantic content, and that each minibatch inherently includes ideal hard negative samples. However, the characteristics of breast lumps challenge these assumptions. In response, we introduce ManiNeg, a novel approach that leverages manifestations as proxies to mine hard negative samples. Manifestations, which refer to the observable symptoms or signs of a disease, provide a knowledge-driven and robust basis for choosing hard negative samples. This approach benefits from its invariance to model optimization, facilitating efficient sampling. To support ManiNeg and future research endeavors, we developed the MVKL dataset, which includes multi-view mammograms, corresponding reports, meticulously annotated manifestations, and pathologically confirmed benign-malignant outcomes. We evaluate ManiNeg on the benign and malignant classification task. Our results demonstrate that ManiNeg not only improves representation in both unimodal and multimodal contexts but also shows generalization across datasets. The MVKL dataset and our codes are publicly available at https://github.com/wxwxwwxxx/ManiNeg.",
    "link": "https://arxiv.org/abs/2409.15745",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/wxwxwwxxx/ManiNeg."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "DepMamba: Progressive Fusion Mamba for Multimodal Depression Detection",
    "summary": "arXiv:2409.15936v1 Announce Type: cross \nAbstract: Depression is a common mental disorder that affects millions of people worldwide. Although promising, current multimodal methods hinge on aligned or aggregated multimodal fusion, suffering two significant limitations: (i) inefficient long-range temporal modeling, and (ii) sub-optimal multimodal fusion between intermodal fusion and intramodal processing. In this paper, we propose an audio-visual progressive fusion Mamba for multimodal depression detection, termed DepMamba. DepMamba features two core designs: hierarchical contextual modeling and progressive multimodal fusion. On the one hand, hierarchical modeling introduces convolution neural networks and Mamba to extract the local-to-global features within long-range sequences. On the other hand, the progressive fusion first presents a multimodal collaborative State Space Model (SSM) extracting intermodal and intramodal information for each modality, and then utilizes a multimodal enhanced SSM for modality cohesion. Extensive experimental results on two large-scale depression datasets demonstrate the superior performance of our DepMamba over existing state-of-the-art methods. Code is available at https://github.com/Jiaxin-Ye/DepMamba.",
    "link": "https://arxiv.org/abs/2409.15936",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/Jiaxin-Ye/DepMamba."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Fine-Tuning is Fine, if Calibrated",
    "summary": "arXiv:2409.16223v1 Announce Type: cross \nAbstract: Fine-tuning is arguably the most straightforward way to tailor a pre-trained model (e.g., a foundation model) to downstream applications, but it also comes with the risk of losing valuable knowledge the model had learned in pre-training. For example, fine-tuning a pre-trained classifier capable of recognizing a large number of classes to master a subset of classes at hand is shown to drastically degrade the model's accuracy in the other classes it had previously learned. As such, it is hard to further use the fine-tuned model when it encounters classes beyond the fine-tuning data. In this paper, we systematically dissect the issue, aiming to answer the fundamental question, ''What has been damaged in the fine-tuned model?'' To our surprise, we find that the fine-tuned model neither forgets the relationship among the other classes nor degrades the features to recognize these classes. Instead, the fine-tuned model often produces more discriminative features for these other classes, even if they were missing during fine-tuning! {What really hurts the accuracy is the discrepant logit scales between the fine-tuning classes and the other classes}, implying that a simple post-processing calibration would bring back the pre-trained model's capability and at the same time unveil the feature improvement over all classes. We conduct an extensive empirical study to demonstrate the robustness of our findings and provide preliminary explanations underlying them, suggesting new directions for future theoretical analysis. Our code is available at https://github.com/OSU-MLB/Fine-Tuning-Is-Fine-If-Calibrated.",
    "link": "https://arxiv.org/abs/2409.16223",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/OSU-MLB/Fine-Tuning-Is-Fine-If-Calibrated."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Do text-free diffusion models learn discriminative visual representations?",
    "summary": "arXiv:2311.17921v3 Announce Type: replace \nAbstract: While many unsupervised learning models focus on one family of tasks, either generative or discriminative, we explore the possibility of a unified representation learner: a model which addresses both families of tasks simultaneously. We identify diffusion models, a state-of-the-art method for generative tasks, as a prime candidate. Such models involve training a U-Net to iteratively predict and remove noise, and the resulting model can synthesize high-fidelity, diverse, novel images. We find that the intermediate feature maps of the U-Net are diverse, discriminative feature representations. We propose a novel attention mechanism for pooling feature maps and further leverage this mechanism as DifFormer, a transformer feature fusion of features from different diffusion U-Net blocks and noise steps. We also develop DifFeed, a novel feedback mechanism tailored to diffusion. We find that diffusion models are better than GANs, and, with our fusion and feedback mechanisms, can compete with state-of-the-art unsupervised image representation learning methods for discriminative tasks - image classification with full and semi-supervision, transfer for fine-grained classification, object detection and segmentation, and semantic segmentation. Our project website (https://mgwillia.github.io/diffssl/) and code (https://github.com/soumik-kanad/diffssl) are available publicly.",
    "link": "https://arxiv.org/abs/2311.17921",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/soumik-kanad/diffssl)"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Region-Adaptive Transform with Segmentation Prior for Image Compression",
    "summary": "arXiv:2403.00628v4 Announce Type: replace \nAbstract: Learned Image Compression (LIC) has shown remarkable progress in recent years. Existing works commonly employ CNN-based or self-attention-based modules as transform methods for compression. However, there is no prior research on neural transform that focuses on specific regions. In response, we introduce the class-agnostic segmentation masks (i.e. semantic masks without category labels) for extracting region-adaptive contextual information. Our proposed module, Region-Adaptive Transform, applies adaptive convolutions on different regions guided by the masks. Additionally, we introduce a plug-and-play module named Scale Affine Layer to incorporate rich contexts from various regions. While there have been prior image compression efforts that involve segmentation masks as additional intermediate inputs, our approach differs significantly from them. Our advantages lie in that, to avoid extra bitrate overhead, we treat these masks as privilege information, which is accessible during the model training stage but not required during the inference phase. To the best of our knowledge, we are the first to employ class-agnostic masks as privilege information and achieve superior performance in pixel-fidelity metrics, such as Peak Signal to Noise Ratio (PSNR). The experimental results demonstrate our improvement compared to previously well-performing methods, with about 8.2% bitrate saving compared to VTM-17.0. The source code is available at https://github.com/GityuxiLiu/SegPIC-for-Image-Compression.",
    "link": "https://arxiv.org/abs/2403.00628",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/GityuxiLiu/SegPIC-for-Image-Compression."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "WaveDH: Wavelet Sub-bands Guided ConvNet for Efficient Image Dehazing",
    "summary": "arXiv:2404.01604v2 Announce Type: replace \nAbstract: The surge in interest regarding image dehazing has led to notable advancements in deep learning-based single image dehazing approaches, exhibiting impressive performance in recent studies. Despite these strides, many existing methods fall short in meeting the efficiency demands of practical applications. In this paper, we introduce WaveDH, a novel and compact ConvNet designed to address this efficiency gap in image dehazing. Our WaveDH leverages wavelet sub-bands for guided up-and-downsampling and frequency-aware feature refinement. The key idea lies in utilizing wavelet decomposition to extract low-and-high frequency components from feature levels, allowing for faster processing while upholding high-quality reconstruction. The downsampling block employs a novel squeeze-and-attention scheme to optimize the feature downsampling process in a structurally compact manner through wavelet domain learning, preserving discriminative features while discarding noise components. In our upsampling block, we introduce a dual-upsample and fusion mechanism to enhance high-frequency component awareness, aiding in the reconstruction of high-frequency details. Departing from conventional dehazing methods that treat low-and-high frequency components equally, our feature refinement block strategically processes features with a frequency-aware approach. By employing a coarse-to-fine methodology, it not only refines the details at frequency levels but also significantly optimizes computational costs. The refinement is performed in a maximum 8x downsampled feature space, striking a favorable efficiency-vs-accuracy trade-off. Extensive experiments demonstrate that our method, WaveDH, outperforms many state-of-the-art methods on several image dehazing benchmarks with significantly reduced computational costs. Our code is available at https://github.com/AwesomeHwang/WaveDH.",
    "link": "https://arxiv.org/abs/2404.01604",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/AwesomeHwang/WaveDH."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Multi-Label Guided Soft Contrastive Learning for Efficient Earth Observation Pretraining",
    "summary": "arXiv:2405.20462v2 Announce Type: replace \nAbstract: Self-supervised pretraining on large-scale satellite data has raised great interest in building Earth observation (EO) foundation models. However, many important resources beyond pure satellite imagery, such as land-cover-land-use products that provide free global semantic information, as well as vision foundation models that hold strong knowledge of the natural world, are not widely studied. In this work, we show these free additional resources not only help resolve common contrastive learning bottlenecks, but also significantly boost the efficiency and effectiveness of EO pretraining. Specifically, we first propose soft contrastive learning that optimizes cross-scene soft similarity based on land-cover-generated multi-label supervision, naturally solving the issue of multiple positive samples and too strict positive matching in complex scenes. Second, we revisit and explore cross-domain continual pretraining for both multispectral and SAR imagery, building efficient EO foundation models from strongest vision models such as DINOv2. Adapting simple weight-initialization and Siamese masking strategies into our soft contrastive learning framework, we demonstrate impressive continual pretraining performance even when the input modalities are not aligned. Without prohibitive training, we produce multispectral and SAR foundation models that achieve significantly better results in 10 out of 11 downstream tasks than most existing SOTA models. For example, our ResNet50/ViT-S achieve 84.8/85.0 linear probing mAP scores on BigEarthNet-10\\% which are better than most existing ViT-L models; under the same setting, our ViT-B sets a new record of 86.8 in multispectral, and 82.5 in SAR, the latter even better than many multispectral models. Dataset and models are available at \\url{https://github.com/zhu-xlab/softcon}.",
    "link": "https://arxiv.org/abs/2405.20462",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/zhu-xlab/softcon}."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Too Many Frames, Not All Useful: Efficient Strategies for Long-Form Video QA",
    "summary": "arXiv:2406.09396v3 Announce Type: replace \nAbstract: Long-form videos that span across wide temporal intervals are highly information redundant and contain multiple distinct events or entities that are often loosely related. Therefore, when performing long-form video question answering (LVQA), all information necessary to generate a correct response can often be contained within a small subset of frames. Recent literature explore the use of large language models (LLMs) in LVQA benchmarks, achieving exceptional performance, while relying on vision language models (VLMs) to convert all visual content within videos into natural language. Such VLMs often independently caption a large number of frames uniformly sampled from long videos, which is not efficient and can mostly be redundant. Questioning these decision choices, we explore optimal strategies for key-frame selection that can significantly reduce these redundancies, namely Hierarchical Keyframe Selector. Our proposed framework, LVNet, achieves state-of-the-art performance at a comparable caption scale across three benchmark LVQA datasets: EgoSchema, IntentQA, NExT-QA. The code can be found at https://github.com/jongwoopark7978/LVNet",
    "link": "https://arxiv.org/abs/2406.09396",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/jongwoopark7978/LVNet"
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Tarsier: Recipes for Training and Evaluating Large Video Description Models",
    "summary": "arXiv:2407.00634v2 Announce Type: replace \nAbstract: Generating fine-grained video descriptions is a fundamental challenge in video understanding. In this work, we introduce Tarsier, a family of large-scale video-language models designed to generate high-quality video descriptions. Tarsier employs CLIP-ViT to encode frames separately and then uses an LLM to model temporal relationships. Despite its simple architecture, we demonstrate that with a meticulously designed two-stage training procedure, the Tarsier models exhibit substantially stronger video description capabilities than any existing open-source model, showing a $+51.4\\%$ advantage in human side-by-side evaluation over the strongest model. Additionally, they are comparable to state-of-the-art proprietary models, with a $+12.3\\%$ advantage against GPT-4V and a $-6.7\\%$ disadvantage against Gemini 1.5 Pro. When upgraded to Tarsier2 by building upon SigLIP and Qwen2-7B, it further improves significantly with a $+4.8\\%$ advantage against GPT-4o. Besides video description, Tarsier proves to be a versatile generalist model, achieving new state-of-the-art results across nine public benchmarks, including multi-choice VQA, open-ended VQA, and zero-shot video captioning. Our second contribution is the introduction of a new benchmark -- DREAM-1K (https://tarsier-vlm.github.io/) for evaluating video description models, consisting of a new challenging dataset featuring videos from diverse sources and varying complexity, along with an automatic method specifically designed to assess the quality of fine-grained video descriptions. We make our models and evaluation benchmark publicly available at https://github.com/bytedance/tarsier.",
    "link": "https://arxiv.org/abs/2407.00634",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/bytedance/tarsier."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Deep Multimodal Collaborative Learning for Polyp Re-Identification",
    "summary": "arXiv:2408.05914v2 Announce Type: replace \nAbstract: Colonoscopic Polyp Re-Identification aims to match the same polyp from a large gallery with images from different views taken using different cameras, which plays an important role in the prevention and treatment of colorectal cancer in computer-aided diagnosis. However, traditional methods for object ReID directly adopting CNN models trained on the ImageNet dataset usually produce unsatisfactory retrieval performance on colonoscopic datasets due to the large domain gap. Worsely, these solutions typically learn unimodal modal representations on the basis of visual samples, which fails to explore complementary information from other different modalities. To address this challenge, we propose a novel Deep Multimodal Collaborative Learning framework named DMCL for polyp re-identification, which can effectively encourage modality collaboration and reinforce generalization capability in medical scenarios. On the basis of it, a dynamic multimodal feature fusion strategy is introduced to leverage the optimized multimodal representations for multimodal fusion via end-to-end training. Experiments on the standard benchmarks show the benefits of the multimodal setting over state-of-the-art unimodal ReID models, especially when combined with the specialized multimodal fusion strategy, from which we have proved that learning representation with multiple-modality can be competitive to methods based on unimodal representation learning. We also hope that our method will shed light on some related researches to move forward, especially for multimodal collaborative learning. The code is publicly available at https://github.com/JeremyXSC/DMCL.",
    "link": "https://arxiv.org/abs/2408.05914",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/JeremyXSC/DMCL."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Perception-Distortion Balanced Super-Resolution: A Multi-Objective Optimization Perspective",
    "summary": "arXiv:2312.15408v2 Announce Type: replace-cross \nAbstract: High perceptual quality and low distortion degree are two important goals in image restoration tasks such as super-resolution (SR). Most of the existing SR methods aim to achieve these goals by minimizing the corresponding yet conflicting losses, such as the $\\ell_1$ loss and the adversarial loss. Unfortunately, the commonly used gradient-based optimizers, such as Adam, are hard to balance these objectives due to the opposite gradient decent directions of the contradictory losses. In this paper, we formulate the perception-distortion trade-off in SR as a multi-objective optimization problem and develop a new optimizer by integrating the gradient-free evolutionary algorithm (EA) with gradient-based Adam, where EA and Adam focus on the divergence and convergence of the optimization directions respectively. As a result, a population of optimal models with different perception-distortion preferences is obtained. We then design a fusion network to merge these models into a single stronger one for an effective perception-distortion trade-off. Experiments demonstrate that with the same backbone network, the perception-distortion balanced SR model trained by our method can achieve better perceptual quality than its competitors while attaining better reconstruction fidelity. Codes and models can be found at https://github.com/csslc/EA-Adam}{https://github.com/csslc/EA-Adam.",
    "link": "https://arxiv.org/abs/2312.15408",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/csslc/EA-Adam}{https://github.com/csslc/EA-Adam."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "RSTAR4D: Rotational Streak Artifact Reduction in 4D CBCT using a Separable 4D CNN",
    "summary": "arXiv:2403.16361v3 Announce Type: replace-cross \nAbstract: Four-dimensional cone-beam computed tomography (4D CBCT) provides respiration-resolved images and can be used for image-guided radiation therapy. However, the ability to reveal respiratory motion comes at the cost of image artifacts. As raw projection data are sorted into multiple respiratory phases, the cone-beam projections become much sparser and the reconstructed 4D CBCT images will be covered by severe streak artifacts. Although several deep learning-based methods have been proposed to address this issue, most algorithms employ 2D network models as backbones, neglecting the intrinsic structural priors within 4D CBCT images. In this paper, we first explore the origin and appearance of streak artifacts in 4D CBCT images. We find that streak artifacts exhibit a unique rotational motion along with the patient's respiration, distinguishable from diaphragm-driven respiratory motion in the spatiotemporal domain. Therefore, we propose a novel 4D neural network model, RSTAR4D-Net, designed to address Rotational STreak Artifact Reduction by integrating the spatial and temporal information within 4D CBCT images. Specifically, we overcome the computational and training difficulties of a 4D neural network. The specially designed model adopts an efficient implementation of 4D convolutions to reduce computational costs and thus can process the whole 4D image in one pass. Additionally, a Tetris training strategy pertinent to the separable 4D convolutions is proposed to effectively train the model using limited 4D training samples. Extensive experiments substantiate the effectiveness of our proposed method, and the RSTAR4D-Net shows superior performance compared to other methods. The source code and dynamic demos are available at https://github.com/ivy9092111111/RSTAR.",
    "link": "https://arxiv.org/abs/2403.16361",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ivy9092111111/RSTAR."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Toward Unified Practices in Trajectory Prediction Research on Drone Datasets",
    "summary": "arXiv:2405.00604v2 Announce Type: replace-cross \nAbstract: The availability of high-quality datasets is crucial for the development of behavior prediction algorithms in autonomous vehicles. This paper highlights the need to standardize the use of certain datasets for motion forecasting research to simplify comparative analysis and proposes a set of tools and practices to achieve this. Drawing on extensive experience and a comprehensive review of current literature, we summarize our proposals for preprocessing, visualization, and evaluation in the form of an open-sourced toolbox designed for researchers working on trajectory prediction problems. The clear specification of necessary preprocessing steps and evaluation metrics is intended to alleviate development efforts and facilitate the comparison of results across different studies. The toolbox is available at: https://github.com/westny/dronalize.",
    "link": "https://arxiv.org/abs/2405.00604",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/westny/dronalize."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  },
  {
    "title": "Towards Ground-truth-free Evaluation of Any Segmentation in Medical Images",
    "summary": "arXiv:2409.14874v2 Announce Type: replace-cross \nAbstract: We explore the feasibility and potential of building a ground-truth-free evaluation model to assess the quality of segmentations generated by the Segment Anything Model (SAM) and its variants in medical imaging. This evaluation model estimates segmentation quality scores by analyzing the coherence and consistency between the input images and their corresponding segmentation predictions. Based on prior research, we frame the task of training this model as a regression problem within a supervised learning framework, using Dice scores (and optionally other metrics) along with mean squared error to compute the training loss. The model is trained utilizing a large collection of public datasets of medical images with segmentation predictions from SAM and its variants. We name this model EvanySeg (Evaluation of Any Segmentation in Medical Images). Our exploration of convolution-based models (e.g., ResNet) and transformer-based models (e.g., ViT) suggested that ViT yields better performance for this task. EvanySeg can be employed for various tasks, including: (1) identifying poorly segmented samples by detecting low-percentile segmentation quality scores; (2) benchmarking segmentation models without ground truth by averaging quality scores across test samples; (3) alerting human experts to poor-quality segmentation predictions during human-AI collaboration by applying a threshold within the score space; and (4) selecting the best segmentation prediction for each test sample at test time when multiple segmentation models are available, by choosing the prediction with the highest quality score. Models and code will be made available at https://github.com/ahjolsenbics/EvanySeg.",
    "link": "https://arxiv.org/abs/2409.14874",
    "published": "Wed, 25 Sep 2024 00:00:00 -0400",
    "github_urls": [
      "https://github.com/ahjolsenbics/EvanySeg."
    ],
    "huggingface_urls": [],
    "source": "arXiv"
  }
]